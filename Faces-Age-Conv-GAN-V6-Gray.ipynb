{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAQBOBYyi9sc"
   },
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2813,
     "status": "ok",
     "timestamp": 1554977904593,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "BIEdHLeqDLJH",
    "outputId": "90567bca-f98c-4b16-c01f-adc92d510da4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "root_path = './'\n",
    "dataset_path = os.path.join(root_path, 'tf_dataset_gray')\n",
    "\n",
    "models_path = os.path.join(root_path, 'saved_models_gray')\n",
    "if not os.path.exists(models_path):\n",
    "  os.mkdir(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2751,
     "status": "ok",
     "timestamp": 1554977904597,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "MxSDNtvqj0Xs",
    "outputId": "876f60f6-e4ef-4454-bfa5-f3ff6b5a8f17"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "image_feature_description = {\n",
    "    'enc': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'age': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "    'img': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "\n",
    "raw_train_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path,'train.tfrecords'))\n",
    "parsed_train_dataset = raw_train_dataset.map(_parse_image_function)\n",
    "\n",
    "# raw_val_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path, 'val.tfrecords'))\n",
    "# parsed_val_dataset = raw_val_dataset.map(_parse_image_function)\n",
    "\n",
    "# raw_test_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path, 'test.tfrecords'))\n",
    "# parsed_test_dataset = raw_test_dataset.map(_parse_image_function)\n",
    "\n",
    "\n",
    "parsed_train_dataset = parsed_train_dataset.repeat()\n",
    "parsed_train_dataset = parsed_train_dataset.shuffle(6000)\n",
    "parsed_train_dataset = parsed_train_dataset.batch(BATCH_SIZE)\n",
    "dataset_iterator = parsed_train_dataset.make_one_shot_iterator()\n",
    "\n",
    "variable_dataset = dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMouydu3i9sp"
   },
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3123,
     "status": "ok",
     "timestamp": 1554977905340,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "HAuHCfjlFBOy",
    "outputId": "abf88c5b-9df8-41dc-9017-9d8a2007a0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 15, 15, 32)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 64)          32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 128)         131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 1)           128       \n",
      "=================================================================\n",
      "Total params: 165,248\n",
      "Trainable params: 164,864\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose_1 (Conv2DTr (None, 22, 22, 512)       8192      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 22, 22, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 22, 22, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 25, 25, 256)       2097152   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 128)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 31, 31, 128)       262144    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 31, 31, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 32, 32, 1)         512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 32, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,896,384\n",
      "Trainable params: 2,894,336\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 10, 10, 1)         0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_len = 0\n",
    "age_len = 0\n",
    "img_shape = (32, 32, 1)\n",
    "width, height, depth = (32, 32, 1)\n",
    "img_len = np.prod(img_shape)\n",
    "latent_dim = enc_len + age_len + img_len\n",
    "noise_len = 100\n",
    "input_dim = enc_len + age_len + noise_len\n",
    "cond_len = enc_len + age_len\n",
    "ngf = 64\n",
    "ndf = 32\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    conv = keras.Sequential([\n",
    "        # conv block 1\n",
    "        keras.layers.Conv2D(\n",
    "            filters=ndf,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=2,\n",
    "            input_shape=img_shape,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        \n",
    "        # conv block 2\n",
    "        keras.layers.Conv2D(\n",
    "            filters=ndf * 2,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=2,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        \n",
    "        # conv block 3\n",
    "        keras.layers.Conv2D(\n",
    "            filters=ndf * 4,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=2,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        \n",
    "        # conv block 5\n",
    "        keras.layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=2,\n",
    "            use_bias=False\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    conv.summary()\n",
    "    \n",
    "    model = keras.Sequential([        \n",
    "        # output\n",
    "        keras.layers.Activation(tf.nn.sigmoid, input_shape=(1,)),\n",
    "    ])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # image\n",
    "    z = keras.layers.Input(shape=img_shape)\n",
    "    \n",
    "    # convolution\n",
    "    zout = conv(z)\n",
    "    \n",
    "    # flatten image\n",
    "    z_flat = keras.layers.Flatten()(zout)\n",
    "    \n",
    "    # concatenation\n",
    "#     inputs = keras.layers.concatenate([z_flat])\n",
    "    \n",
    "    # real or fake\n",
    "    outputs = model(z_flat)\n",
    "    \n",
    "    # age label\n",
    "#     classes = clf(inputs)\n",
    "    \n",
    "    \n",
    "    return keras.models.Model(z, outputs)\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    \n",
    "    conv = keras.Sequential([\n",
    "        # transpose conv block 1\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=2,\n",
    "            input_shape=(10, 10, 1),\n",
    "            use_bias=False\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 2\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 4,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=1,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 3\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 2,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=1,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 4\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 2,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=1,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 5\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=(2, 2),\n",
    "            strides=1,\n",
    "            use_bias=False\n",
    "        ),\n",
    "        \n",
    "        # output\n",
    "        keras.layers.Activation(tf.nn.tanh)\n",
    "    ])\n",
    "    \n",
    "    conv.summary()\n",
    "    \n",
    "    model = keras.Sequential([        \n",
    "        # reshape 1d to 3d\n",
    "        keras.layers.Reshape((10, 10, 1), input_shape=(input_dim,))\n",
    "    ])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # noise\n",
    "    x = keras.layers.Input(shape=(noise_len,))\n",
    "    \n",
    "    # flat dense output\n",
    "    out_1 = model(x)\n",
    "    \n",
    "    # transpose conv output\n",
    "    outputs = conv(out_1)\n",
    "    \n",
    "    return keras.models.Model(x, outputs)\n",
    "\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3046,
     "status": "ok",
     "timestamp": 1554977905345,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "tfo8J8jQ4-FH",
    "outputId": "022c24ae-1c6e-40ad-e167-964953449961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 10, 10, 1)         0         \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 32, 32, 1)         2896384   \n",
      "=================================================================\n",
      "Total params: 2,896,384\n",
      "Trainable params: 2,894,336\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3013,
     "status": "ok",
     "timestamp": 1554977905348,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "ObTb7HIf5CqA",
    "outputId": "1bdb05c0-cd96-46c6-a124-68ac9ee9ddb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1, 1, 1)           165248    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 165,248\n",
      "Trainable params: 164,864\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctWfkxy5i9tR"
   },
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHTmYpPeImn5"
   },
   "outputs": [],
   "source": [
    "GLR = 0.0002  # generator\n",
    "DLR = 0.0002  # discriminator\n",
    "\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=keras.optimizers.Adam(DLR, 0.5),\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# noise\n",
    "x = keras.layers.Input(shape=(noise_len,))\n",
    "\n",
    "# freeze discriminator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# output\n",
    "z = generator(x)\n",
    "out = discriminator(z)\n",
    "\n",
    "# GAN\n",
    "gan = keras.models.Model(inputs=x, outputs=out)\n",
    "\n",
    "gan.compile(\n",
    "    optimizer=keras.optimizers.Adam(GLR , 0.5),\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3156,
     "status": "ok",
     "timestamp": 1554977905707,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "o_76rZ0ti9tc",
    "outputId": "8eb9fa73-4621-40e3-fab1-fd2d52d7bbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 32, 32, 1)         2896384   \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 165248    \n",
      "=================================================================\n",
      "Total params: 3,061,632\n",
      "Trainable params: 2,894,336\n",
      "Non-trainable params: 167,296\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REaxJyLqi9tp"
   },
   "source": [
    "## Visualization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25210,
     "status": "ok",
     "timestamp": 1554977927807,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "4kA4g6_lt3D8",
    "outputId": "c5b38cce-ad98-49ac-9b13-950a8e2356ca"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "import os\n",
    "\n",
    "\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "root_path = './'\n",
    "tgt_pth = os.path.join(root_path, 'visualize_age-cgan-v15')\n",
    "\n",
    "if not os.path.exists(tgt_pth):\n",
    "  os.mkdir(tgt_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3nB78iWi9ts"
   },
   "outputs": [],
   "source": [
    "def visualizeGAN(e, z_real, z_fake):\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(20, 18))\n",
    "\n",
    "    r_real = 0\n",
    "    r_fake = 0\n",
    "    for row, axe in enumerate(axes):\n",
    "        for col, cell in enumerate(axe):\n",
    "            if row % 2 == 0:\n",
    "                cell.imshow(\n",
    "                    np.squeeze(\n",
    "                        0.5 * z_real[r_real * 4 + col] + 0.5,\n",
    "                        axis=-1\n",
    "                    ),\n",
    "                    cmap='gray'\n",
    "                )\n",
    "            else:\n",
    "                cell.imshow(\n",
    "                    np.squeeze(\n",
    "                        0.5 * z_fake[r_fake * 4 + col] + 0.5,\n",
    "                        axis=-1\n",
    "                    ),\n",
    "                    cmap='gray'\n",
    "                )\n",
    "\n",
    "            cell.axis(\"off\")\n",
    "\n",
    "        if row % 2 == 0:\n",
    "            r_real += 1\n",
    "        else:\n",
    "            r_fake += 1\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(os.path.join(tgt_pth, '{}.jpg'.format(str(e).zfill(3))))\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noise():\n",
    "    \n",
    "    y_real = tf.ones((BATCH_SIZE,))\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.40\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        # run once\n",
    "        y_real = y_real.eval()\n",
    "\n",
    "        while True:\n",
    "            values = sess.run([variable_dataset])\n",
    "            row = values[0]\n",
    "\n",
    "            sz = row['img'].shape[0]\n",
    "\n",
    "            if sz != BATCH_SIZE:\n",
    "                continue\n",
    "            \n",
    "            # fake data\n",
    "            # concatenate face + age + noise\n",
    "#             c1 = row['enc']\n",
    "#             c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "            x = tf.random.normal((sz, noise_len,)).eval()\n",
    "            \n",
    "            yield x, y_real\n",
    "\n",
    "\n",
    "def load_batch():\n",
    "    \n",
    "    y_fake = tf.zeros((BATCH_SIZE,))\n",
    "    y_real = tf.ones((BATCH_SIZE,))\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.40\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        # run once\n",
    "        y_fake = y_fake.eval()\n",
    "        y_real = y_real.eval()\n",
    "\n",
    "        while True:\n",
    "            values = sess.run([variable_dataset])\n",
    "            row = values[0]\n",
    "\n",
    "            sz = row['img'].shape[0]\n",
    "\n",
    "            if sz != BATCH_SIZE:\n",
    "                continue\n",
    "            \n",
    "            # fake data\n",
    "#             c1 = row['enc']\n",
    "#             c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "            x = tf.random.normal((sz, noise_len,)).eval()\n",
    "            z_fake = generator.predict(x)\n",
    "\n",
    "            # real data\n",
    "#             c1 = row['enc']\n",
    "#             c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "            z_real = tf.reshape(tf.io.decode_raw(row['img'], tf.int64), (-1, width, height, depth))\n",
    "    \n",
    "            z_real = tf.cast(z_real, tf.float32)\n",
    "    \n",
    "            # scale to [-1, +1]\n",
    "            z_real = tf.math.subtract(tf.math.divide(z_real, 127.5), 1)\n",
    "        \n",
    "            z_real = z_real.eval()\n",
    "                        \n",
    "            yield x, z_fake, y_fake, z_real, y_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GNNmDUZi9t3"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "(16, 100)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 0, [D ACC: %57.81], [D LOSS: 68.30], [G ACC: %81.25], [G LOSS: 50.54]\n",
      "E: 1, [D ACC: %51.56], [D LOSS: 122.98], [G ACC: %93.75], [G LOSS: 24.87]\n",
      "E: 2, [D ACC: %51.56], [D LOSS: 116.95], [G ACC: %84.38], [G LOSS: 39.97]\n",
      "E: 3, [D ACC: %53.12], [D LOSS: 94.99], [G ACC: %81.25], [G LOSS: 39.81]\n",
      "E: 4, [D ACC: %54.69], [D LOSS: 96.19], [G ACC: %81.25], [G LOSS: 48.04]\n",
      "E: 5, [D ACC: %53.12], [D LOSS: 97.34], [G ACC: %90.62], [G LOSS: 43.49]\n",
      "E: 6, [D ACC: %53.12], [D LOSS: 90.02], [G ACC: %84.38], [G LOSS: 47.00]\n",
      "E: 7, [D ACC: %60.94], [D LOSS: 66.30], [G ACC: %90.62], [G LOSS: 38.16]\n",
      "E: 8, [D ACC: %65.62], [D LOSS: 74.03], [G ACC: %93.75], [G LOSS: 33.47]\n",
      "E: 9, [D ACC: %90.62], [D LOSS: 43.53], [G ACC: %84.38], [G LOSS: 39.15]\n",
      "E: 10, [D ACC: %89.06], [D LOSS: 41.85], [G ACC: %62.50], [G LOSS: 70.37]\n",
      "E: 11, [D ACC: %98.44], [D LOSS: 37.27], [G ACC: %28.12], [G LOSS: 99.55]\n",
      "E: 12, [D ACC: %95.31], [D LOSS: 41.55], [G ACC: %21.88], [G LOSS: 112.82]\n",
      "E: 13, [D ACC: %92.19], [D LOSS: 44.72], [G ACC: %40.62], [G LOSS: 112.23]\n",
      "E: 14, [D ACC: %71.88], [D LOSS: 61.10], [G ACC: %75.00], [G LOSS: 53.65]\n",
      "E: 15, [D ACC: %50.00], [D LOSS: 170.83], [G ACC: %100.00], [G LOSS: 7.18]\n",
      "E: 16, [D ACC: %51.56], [D LOSS: 79.50], [G ACC: %84.38], [G LOSS: 18.43]\n",
      "E: 17, [D ACC: %92.19], [D LOSS: 43.21], [G ACC: %93.75], [G LOSS: 17.23]\n",
      "E: 18, [D ACC: %95.31], [D LOSS: 34.36], [G ACC: %100.00], [G LOSS: 16.48]\n",
      "E: 19, [D ACC: %95.31], [D LOSS: 32.34], [G ACC: %93.75], [G LOSS: 23.10]\n",
      "E: 20, [D ACC: %95.31], [D LOSS: 31.49], [G ACC: %87.50], [G LOSS: 30.40]\n",
      "E: 21, [D ACC: %84.38], [D LOSS: 39.52], [G ACC: %90.62], [G LOSS: 28.96]\n",
      "E: 22, [D ACC: %85.94], [D LOSS: 50.22], [G ACC: %93.75], [G LOSS: 26.25]\n",
      "E: 23, [D ACC: %79.69], [D LOSS: 48.43], [G ACC: %87.50], [G LOSS: 39.80]\n",
      "E: 24, [D ACC: %76.56], [D LOSS: 56.15], [G ACC: %59.38], [G LOSS: 65.43]\n",
      "E: 25, [D ACC: %54.69], [D LOSS: 100.14], [G ACC: %71.88], [G LOSS: 57.06]\n",
      "E: 26, [D ACC: %60.94], [D LOSS: 71.77], [G ACC: %40.62], [G LOSS: 91.09]\n",
      "E: 27, [D ACC: %82.81], [D LOSS: 67.16], [G ACC: %34.38], [G LOSS: 99.33]\n",
      "E: 28, [D ACC: %98.44], [D LOSS: 28.57], [G ACC: %75.00], [G LOSS: 52.71]\n",
      "E: 29, [D ACC: %89.06], [D LOSS: 38.62], [G ACC: %87.50], [G LOSS: 38.95]\n",
      "E: 30, [D ACC: %96.88], [D LOSS: 30.40], [G ACC: %75.00], [G LOSS: 51.63]\n",
      "E: 31, [D ACC: %98.44], [D LOSS: 34.75], [G ACC: %53.12], [G LOSS: 69.53]\n",
      "E: 32, [D ACC: %95.31], [D LOSS: 36.08], [G ACC: %46.88], [G LOSS: 82.19]\n",
      "E: 33, [D ACC: %82.81], [D LOSS: 54.28], [G ACC: %59.38], [G LOSS: 88.10]\n",
      "E: 34, [D ACC: %64.06], [D LOSS: 72.81], [G ACC: %56.25], [G LOSS: 71.92]\n",
      "E: 35, [D ACC: %60.94], [D LOSS: 86.06], [G ACC: %100.00], [G LOSS: 19.07]\n",
      "E: 36, [D ACC: %82.81], [D LOSS: 51.97], [G ACC: %93.75], [G LOSS: 24.47]\n",
      "E: 37, [D ACC: %98.44], [D LOSS: 26.09], [G ACC: %93.75], [G LOSS: 22.55]\n",
      "E: 38, [D ACC: %98.44], [D LOSS: 47.19], [G ACC: %100.00], [G LOSS: 18.66]\n",
      "E: 39, [D ACC: %98.44], [D LOSS: 26.50], [G ACC: %100.00], [G LOSS: 15.05]\n",
      "E: 40, [D ACC: %100.00], [D LOSS: 30.46], [G ACC: %100.00], [G LOSS: 12.31]\n",
      "E: 41, [D ACC: %95.31], [D LOSS: 56.77], [G ACC: %96.88], [G LOSS: 22.38]\n",
      "E: 42, [D ACC: %92.19], [D LOSS: 35.61], [G ACC: %84.38], [G LOSS: 41.57]\n",
      "E: 43, [D ACC: %90.62], [D LOSS: 45.29], [G ACC: %71.88], [G LOSS: 49.71]\n",
      "E: 44, [D ACC: %82.81], [D LOSS: 48.00], [G ACC: %46.88], [G LOSS: 86.60]\n",
      "E: 45, [D ACC: %78.12], [D LOSS: 56.70], [G ACC: %21.88], [G LOSS: 106.96]\n",
      "E: 46, [D ACC: %57.81], [D LOSS: 89.00], [G ACC: %87.50], [G LOSS: 26.85]\n",
      "E: 47, [D ACC: %50.00], [D LOSS: 147.23], [G ACC: %100.00], [G LOSS: 5.36]\n",
      "E: 48, [D ACC: %53.12], [D LOSS: 132.82], [G ACC: %100.00], [G LOSS: 12.00]\n",
      "E: 49, [D ACC: %75.00], [D LOSS: 60.70], [G ACC: %87.50], [G LOSS: 28.29]\n",
      "E: 50, [D ACC: %92.19], [D LOSS: 38.63], [G ACC: %96.88], [G LOSS: 29.40]\n",
      "E: 51, [D ACC: %87.50], [D LOSS: 39.15], [G ACC: %93.75], [G LOSS: 24.80]\n",
      "E: 52, [D ACC: %82.81], [D LOSS: 42.49], [G ACC: %100.00], [G LOSS: 16.02]\n",
      "E: 53, [D ACC: %87.50], [D LOSS: 40.11], [G ACC: %96.88], [G LOSS: 12.87]\n",
      "E: 54, [D ACC: %87.50], [D LOSS: 36.97], [G ACC: %100.00], [G LOSS: 16.16]\n",
      "E: 55, [D ACC: %82.81], [D LOSS: 46.28], [G ACC: %100.00], [G LOSS: 19.94]\n",
      "E: 56, [D ACC: %98.44], [D LOSS: 27.02], [G ACC: %100.00], [G LOSS: 19.37]\n",
      "E: 57, [D ACC: %98.44], [D LOSS: 31.52], [G ACC: %93.75], [G LOSS: 26.56]\n",
      "E: 58, [D ACC: %98.44], [D LOSS: 37.41], [G ACC: %96.88], [G LOSS: 24.46]\n",
      "E: 59, [D ACC: %98.44], [D LOSS: 40.12], [G ACC: %78.12], [G LOSS: 43.77]\n",
      "E: 60, [D ACC: %96.88], [D LOSS: 32.65], [G ACC: %75.00], [G LOSS: 50.36]\n",
      "E: 61, [D ACC: %93.75], [D LOSS: 42.59], [G ACC: %59.38], [G LOSS: 67.18]\n",
      "E: 62, [D ACC: %85.94], [D LOSS: 44.76], [G ACC: %37.50], [G LOSS: 81.78]\n",
      "E: 63, [D ACC: %92.19], [D LOSS: 37.53], [G ACC: %59.38], [G LOSS: 77.56]\n",
      "E: 64, [D ACC: %78.12], [D LOSS: 52.02], [G ACC: %68.75], [G LOSS: 52.74]\n",
      "E: 65, [D ACC: %75.00], [D LOSS: 65.23], [G ACC: %87.50], [G LOSS: 43.11]\n",
      "E: 66, [D ACC: %89.06], [D LOSS: 42.13], [G ACC: %71.88], [G LOSS: 48.44]\n",
      "E: 67, [D ACC: %73.44], [D LOSS: 64.82], [G ACC: %56.25], [G LOSS: 76.95]\n",
      "E: 68, [D ACC: %78.12], [D LOSS: 53.34], [G ACC: %34.38], [G LOSS: 108.77]\n",
      "E: 69, [D ACC: %73.44], [D LOSS: 61.65], [G ACC: %78.12], [G LOSS: 57.08]\n",
      "E: 70, [D ACC: %84.38], [D LOSS: 44.79], [G ACC: %18.75], [G LOSS: 126.58]\n",
      "E: 71, [D ACC: %75.00], [D LOSS: 68.95], [G ACC: %6.25], [G LOSS: 198.81]\n",
      "E: 72, [D ACC: %93.75], [D LOSS: 38.49], [G ACC: %18.75], [G LOSS: 153.17]\n",
      "E: 73, [D ACC: %79.69], [D LOSS: 51.39], [G ACC: %90.62], [G LOSS: 20.88]\n",
      "E: 74, [D ACC: %92.19], [D LOSS: 38.10], [G ACC: %96.88], [G LOSS: 14.48]\n",
      "E: 75, [D ACC: %90.62], [D LOSS: 26.20], [G ACC: %90.62], [G LOSS: 22.04]\n",
      "E: 76, [D ACC: %100.00], [D LOSS: 25.84], [G ACC: %93.75], [G LOSS: 18.43]\n",
      "E: 77, [D ACC: %93.75], [D LOSS: 30.72], [G ACC: %87.50], [G LOSS: 35.46]\n",
      "E: 78, [D ACC: %78.12], [D LOSS: 71.78], [G ACC: %78.12], [G LOSS: 40.55]\n",
      "E: 79, [D ACC: %70.31], [D LOSS: 69.42], [G ACC: %84.38], [G LOSS: 48.66]\n",
      "E: 80, [D ACC: %73.44], [D LOSS: 66.77], [G ACC: %78.12], [G LOSS: 43.21]\n",
      "E: 81, [D ACC: %54.69], [D LOSS: 102.87], [G ACC: %90.62], [G LOSS: 37.70]\n",
      "E: 82, [D ACC: %93.75], [D LOSS: 37.09], [G ACC: %75.00], [G LOSS: 52.81]\n",
      "E: 83, [D ACC: %95.31], [D LOSS: 40.43], [G ACC: %81.25], [G LOSS: 39.45]\n",
      "E: 84, [D ACC: %96.88], [D LOSS: 33.26], [G ACC: %90.62], [G LOSS: 24.78]\n",
      "E: 85, [D ACC: %89.06], [D LOSS: 24.68], [G ACC: %90.62], [G LOSS: 29.09]\n",
      "E: 86, [D ACC: %98.44], [D LOSS: 42.64], [G ACC: %93.75], [G LOSS: 17.44]\n",
      "E: 87, [D ACC: %95.31], [D LOSS: 23.02], [G ACC: %93.75], [G LOSS: 19.86]\n",
      "E: 88, [D ACC: %98.44], [D LOSS: 45.79], [G ACC: %93.75], [G LOSS: 18.53]\n",
      "E: 89, [D ACC: %98.44], [D LOSS: 44.10], [G ACC: %90.62], [G LOSS: 23.06]\n",
      "E: 90, [D ACC: %100.00], [D LOSS: 24.74], [G ACC: %87.50], [G LOSS: 30.71]\n",
      "E: 91, [D ACC: %98.44], [D LOSS: 20.90], [G ACC: %90.62], [G LOSS: 31.79]\n",
      "E: 92, [D ACC: %96.88], [D LOSS: 24.29], [G ACC: %87.50], [G LOSS: 36.48]\n",
      "E: 93, [D ACC: %95.31], [D LOSS: 28.07], [G ACC: %59.38], [G LOSS: 65.64]\n",
      "E: 94, [D ACC: %98.44], [D LOSS: 25.85], [G ACC: %62.50], [G LOSS: 71.50]\n",
      "E: 95, [D ACC: %98.44], [D LOSS: 27.58], [G ACC: %43.75], [G LOSS: 88.36]\n",
      "E: 96, [D ACC: %98.44], [D LOSS: 29.19], [G ACC: %18.75], [G LOSS: 116.67]\n",
      "E: 97, [D ACC: %85.94], [D LOSS: 39.81], [G ACC: %9.38], [G LOSS: 156.46]\n",
      "E: 98, [D ACC: %89.06], [D LOSS: 52.90], [G ACC: %6.25], [G LOSS: 190.77]\n",
      "E: 99, [D ACC: %98.44], [D LOSS: 26.15], [G ACC: %18.75], [G LOSS: 155.05]\n",
      "E: 100, [D ACC: %98.44], [D LOSS: 29.87], [G ACC: %81.25], [G LOSS: 44.04]\n",
      "E: 101, [D ACC: %96.88], [D LOSS: 26.91], [G ACC: %100.00], [G LOSS: 11.31]\n",
      "E: 102, [D ACC: %98.44], [D LOSS: 20.66], [G ACC: %100.00], [G LOSS: 5.29]\n",
      "E: 103, [D ACC: %96.88], [D LOSS: 20.71], [G ACC: %100.00], [G LOSS: 3.49]\n",
      "E: 104, [D ACC: %98.44], [D LOSS: 36.91], [G ACC: %100.00], [G LOSS: 3.35]\n",
      "E: 105, [D ACC: %98.44], [D LOSS: 41.57], [G ACC: %100.00], [G LOSS: 2.68]\n",
      "E: 106, [D ACC: %96.88], [D LOSS: 34.86], [G ACC: %100.00], [G LOSS: 3.00]\n",
      "E: 107, [D ACC: %96.88], [D LOSS: 17.31], [G ACC: %100.00], [G LOSS: 3.92]\n",
      "E: 108, [D ACC: %98.44], [D LOSS: 33.22], [G ACC: %100.00], [G LOSS: 2.96]\n",
      "E: 109, [D ACC: %98.44], [D LOSS: 36.85], [G ACC: %100.00], [G LOSS: 3.13]\n",
      "E: 110, [D ACC: %100.00], [D LOSS: 13.56], [G ACC: %100.00], [G LOSS: 2.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 111, [D ACC: %98.44], [D LOSS: 14.14], [G ACC: %100.00], [G LOSS: 4.14]\n",
      "E: 112, [D ACC: %96.88], [D LOSS: 16.97], [G ACC: %100.00], [G LOSS: 3.13]\n",
      "E: 113, [D ACC: %98.44], [D LOSS: 22.11], [G ACC: %100.00], [G LOSS: 4.48]\n",
      "E: 114, [D ACC: %96.88], [D LOSS: 24.93], [G ACC: %100.00], [G LOSS: 3.30]\n",
      "E: 115, [D ACC: %98.44], [D LOSS: 17.14], [G ACC: %100.00], [G LOSS: 4.47]\n",
      "E: 116, [D ACC: %100.00], [D LOSS: 21.96], [G ACC: %100.00], [G LOSS: 4.47]\n",
      "E: 117, [D ACC: %98.44], [D LOSS: 32.15], [G ACC: %100.00], [G LOSS: 5.26]\n",
      "E: 118, [D ACC: %100.00], [D LOSS: 14.38], [G ACC: %100.00], [G LOSS: 4.06]\n",
      "E: 119, [D ACC: %98.44], [D LOSS: 15.11], [G ACC: %100.00], [G LOSS: 4.27]\n",
      "E: 120, [D ACC: %98.44], [D LOSS: 34.62], [G ACC: %100.00], [G LOSS: 4.28]\n",
      "E: 121, [D ACC: %98.44], [D LOSS: 13.04], [G ACC: %100.00], [G LOSS: 4.75]\n",
      "E: 122, [D ACC: %100.00], [D LOSS: 20.11], [G ACC: %100.00], [G LOSS: 4.07]\n",
      "E: 123, [D ACC: %98.44], [D LOSS: 10.76], [G ACC: %100.00], [G LOSS: 5.35]\n",
      "E: 124, [D ACC: %96.88], [D LOSS: 16.55], [G ACC: %100.00], [G LOSS: 5.40]\n",
      "E: 125, [D ACC: %100.00], [D LOSS: 21.57], [G ACC: %100.00], [G LOSS: 3.40]\n",
      "E: 126, [D ACC: %96.88], [D LOSS: 15.90], [G ACC: %100.00], [G LOSS: 5.20]\n",
      "E: 127, [D ACC: %98.44], [D LOSS: 30.27], [G ACC: %100.00], [G LOSS: 5.33]\n",
      "E: 128, [D ACC: %96.88], [D LOSS: 13.60], [G ACC: %100.00], [G LOSS: 5.09]\n",
      "E: 129, [D ACC: %98.44], [D LOSS: 18.12], [G ACC: %100.00], [G LOSS: 6.21]\n",
      "E: 130, [D ACC: %95.31], [D LOSS: 17.70], [G ACC: %100.00], [G LOSS: 3.14]\n",
      "E: 131, [D ACC: %100.00], [D LOSS: 19.08], [G ACC: %100.00], [G LOSS: 5.13]\n",
      "E: 132, [D ACC: %100.00], [D LOSS: 20.87], [G ACC: %100.00], [G LOSS: 3.34]\n",
      "E: 133, [D ACC: %98.44], [D LOSS: 19.85], [G ACC: %100.00], [G LOSS: 5.78]\n",
      "E: 134, [D ACC: %100.00], [D LOSS: 25.36], [G ACC: %100.00], [G LOSS: 8.55]\n",
      "E: 135, [D ACC: %100.00], [D LOSS: 22.84], [G ACC: %100.00], [G LOSS: 9.95]\n",
      "E: 136, [D ACC: %96.88], [D LOSS: 24.82], [G ACC: %100.00], [G LOSS: 9.69]\n",
      "E: 137, [D ACC: %100.00], [D LOSS: 18.51], [G ACC: %100.00], [G LOSS: 16.74]\n",
      "E: 138, [D ACC: %100.00], [D LOSS: 16.82], [G ACC: %100.00], [G LOSS: 14.43]\n",
      "E: 139, [D ACC: %100.00], [D LOSS: 19.88], [G ACC: %96.88], [G LOSS: 13.14]\n",
      "E: 140, [D ACC: %95.31], [D LOSS: 21.67], [G ACC: %100.00], [G LOSS: 17.51]\n",
      "E: 141, [D ACC: %96.88], [D LOSS: 22.02], [G ACC: %100.00], [G LOSS: 23.82]\n",
      "E: 142, [D ACC: %95.31], [D LOSS: 23.30], [G ACC: %81.25], [G LOSS: 35.89]\n",
      "E: 143, [D ACC: %92.19], [D LOSS: 33.79], [G ACC: %37.50], [G LOSS: 95.15]\n",
      "E: 144, [D ACC: %73.44], [D LOSS: 49.29], [G ACC: %15.62], [G LOSS: 145.07]\n",
      "E: 145, [D ACC: %73.44], [D LOSS: 48.53], [G ACC: %3.12], [G LOSS: 176.52]\n",
      "E: 146, [D ACC: %93.75], [D LOSS: 47.47], [G ACC: %21.88], [G LOSS: 128.87]\n",
      "E: 147, [D ACC: %98.44], [D LOSS: 27.94], [G ACC: %34.38], [G LOSS: 87.77]\n",
      "E: 148, [D ACC: %98.44], [D LOSS: 20.73], [G ACC: %75.00], [G LOSS: 57.43]\n",
      "E: 149, [D ACC: %92.19], [D LOSS: 26.81], [G ACC: %93.75], [G LOSS: 29.20]\n",
      "E: 150, [D ACC: %87.50], [D LOSS: 43.45], [G ACC: %87.50], [G LOSS: 38.27]\n",
      "E: 151, [D ACC: %84.38], [D LOSS: 46.76], [G ACC: %62.50], [G LOSS: 62.03]\n",
      "E: 152, [D ACC: %93.75], [D LOSS: 20.63], [G ACC: %81.25], [G LOSS: 43.22]\n",
      "E: 153, [D ACC: %96.88], [D LOSS: 14.58], [G ACC: %93.75], [G LOSS: 27.07]\n",
      "E: 154, [D ACC: %100.00], [D LOSS: 12.87], [G ACC: %96.88], [G LOSS: 21.80]\n",
      "E: 155, [D ACC: %96.88], [D LOSS: 19.44], [G ACC: %100.00], [G LOSS: 12.29]\n",
      "E: 156, [D ACC: %98.44], [D LOSS: 25.14], [G ACC: %100.00], [G LOSS: 13.85]\n",
      "E: 157, [D ACC: %96.88], [D LOSS: 31.38], [G ACC: %96.88], [G LOSS: 23.66]\n",
      "E: 158, [D ACC: %96.88], [D LOSS: 22.45], [G ACC: %93.75], [G LOSS: 20.95]\n",
      "E: 159, [D ACC: %84.38], [D LOSS: 34.12], [G ACC: %71.88], [G LOSS: 47.11]\n",
      "E: 160, [D ACC: %90.62], [D LOSS: 36.08], [G ACC: %71.88], [G LOSS: 49.34]\n",
      "E: 161, [D ACC: %93.75], [D LOSS: 28.60], [G ACC: %37.50], [G LOSS: 93.78]\n",
      "E: 162, [D ACC: %93.75], [D LOSS: 26.11], [G ACC: %40.62], [G LOSS: 111.62]\n",
      "E: 163, [D ACC: %92.19], [D LOSS: 25.80], [G ACC: %78.12], [G LOSS: 46.23]\n",
      "E: 164, [D ACC: %92.19], [D LOSS: 31.95], [G ACC: %75.00], [G LOSS: 48.54]\n",
      "E: 165, [D ACC: %92.19], [D LOSS: 36.67], [G ACC: %59.38], [G LOSS: 54.63]\n",
      "E: 166, [D ACC: %96.88], [D LOSS: 19.72], [G ACC: %93.75], [G LOSS: 38.65]\n",
      "E: 167, [D ACC: %93.75], [D LOSS: 26.47], [G ACC: %90.62], [G LOSS: 21.01]\n",
      "E: 168, [D ACC: %96.88], [D LOSS: 23.56], [G ACC: %93.75], [G LOSS: 18.96]\n",
      "E: 169, [D ACC: %100.00], [D LOSS: 16.44], [G ACC: %100.00], [G LOSS: 6.88]\n",
      "E: 170, [D ACC: %100.00], [D LOSS: 17.03], [G ACC: %100.00], [G LOSS: 7.07]\n",
      "E: 171, [D ACC: %90.62], [D LOSS: 26.14], [G ACC: %100.00], [G LOSS: 11.23]\n",
      "E: 172, [D ACC: %96.88], [D LOSS: 16.07], [G ACC: %100.00], [G LOSS: 5.15]\n",
      "E: 173, [D ACC: %96.88], [D LOSS: 41.00], [G ACC: %100.00], [G LOSS: 5.41]\n",
      "E: 174, [D ACC: %96.88], [D LOSS: 23.85], [G ACC: %100.00], [G LOSS: 10.23]\n",
      "E: 175, [D ACC: %96.88], [D LOSS: 19.07], [G ACC: %96.88], [G LOSS: 9.85]\n",
      "E: 176, [D ACC: %95.31], [D LOSS: 26.95], [G ACC: %100.00], [G LOSS: 9.18]\n",
      "E: 177, [D ACC: %87.50], [D LOSS: 30.62], [G ACC: %100.00], [G LOSS: 11.26]\n",
      "E: 178, [D ACC: %87.50], [D LOSS: 44.81], [G ACC: %90.62], [G LOSS: 26.24]\n",
      "E: 179, [D ACC: %60.94], [D LOSS: 95.61], [G ACC: %84.38], [G LOSS: 37.33]\n",
      "E: 180, [D ACC: %79.69], [D LOSS: 49.39], [G ACC: %62.50], [G LOSS: 63.49]\n",
      "E: 181, [D ACC: %81.25], [D LOSS: 49.47], [G ACC: %43.75], [G LOSS: 88.24]\n",
      "E: 182, [D ACC: %81.25], [D LOSS: 43.17], [G ACC: %15.62], [G LOSS: 154.18]\n",
      "E: 183, [D ACC: %98.44], [D LOSS: 16.73], [G ACC: %18.75], [G LOSS: 192.67]\n",
      "E: 184, [D ACC: %100.00], [D LOSS: 22.03], [G ACC: %21.88], [G LOSS: 141.44]\n",
      "E: 185, [D ACC: %93.75], [D LOSS: 29.35], [G ACC: %93.75], [G LOSS: 20.98]\n",
      "E: 186, [D ACC: %79.69], [D LOSS: 51.34], [G ACC: %93.75], [G LOSS: 12.37]\n",
      "E: 187, [D ACC: %75.00], [D LOSS: 66.45], [G ACC: %96.88], [G LOSS: 19.85]\n",
      "E: 188, [D ACC: %87.50], [D LOSS: 45.93], [G ACC: %81.25], [G LOSS: 45.18]\n",
      "E: 189, [D ACC: %95.31], [D LOSS: 21.41], [G ACC: %78.12], [G LOSS: 45.25]\n",
      "E: 190, [D ACC: %96.88], [D LOSS: 19.84], [G ACC: %62.50], [G LOSS: 59.82]\n",
      "E: 191, [D ACC: %98.44], [D LOSS: 23.35], [G ACC: %93.75], [G LOSS: 31.24]\n",
      "E: 192, [D ACC: %96.88], [D LOSS: 11.97], [G ACC: %96.88], [G LOSS: 23.50]\n",
      "E: 193, [D ACC: %98.44], [D LOSS: 13.82], [G ACC: %100.00], [G LOSS: 9.42]\n",
      "E: 194, [D ACC: %100.00], [D LOSS: 12.95], [G ACC: %100.00], [G LOSS: 4.51]\n",
      "E: 195, [D ACC: %93.75], [D LOSS: 22.08], [G ACC: %100.00], [G LOSS: 2.66]\n",
      "E: 196, [D ACC: %96.88], [D LOSS: 17.94], [G ACC: %100.00], [G LOSS: 2.56]\n",
      "E: 197, [D ACC: %93.75], [D LOSS: 16.40], [G ACC: %100.00], [G LOSS: 3.27]\n",
      "E: 198, [D ACC: %100.00], [D LOSS: 14.03], [G ACC: %100.00], [G LOSS: 7.09]\n",
      "E: 199, [D ACC: %98.44], [D LOSS: 21.33], [G ACC: %100.00], [G LOSS: 7.40]\n",
      "E: 200, [D ACC: %92.19], [D LOSS: 33.68], [G ACC: %100.00], [G LOSS: 14.82]\n",
      "E: 201, [D ACC: %92.19], [D LOSS: 27.41], [G ACC: %78.12], [G LOSS: 38.22]\n",
      "E: 202, [D ACC: %90.62], [D LOSS: 32.98], [G ACC: %65.62], [G LOSS: 62.36]\n",
      "E: 203, [D ACC: %95.31], [D LOSS: 20.23], [G ACC: %65.62], [G LOSS: 66.88]\n",
      "E: 204, [D ACC: %100.00], [D LOSS: 17.27], [G ACC: %90.62], [G LOSS: 29.04]\n",
      "E: 205, [D ACC: %78.12], [D LOSS: 47.32], [G ACC: %68.75], [G LOSS: 60.34]\n",
      "E: 206, [D ACC: %96.88], [D LOSS: 15.39], [G ACC: %75.00], [G LOSS: 56.16]\n",
      "E: 207, [D ACC: %96.88], [D LOSS: 25.77], [G ACC: %90.62], [G LOSS: 27.83]\n",
      "E: 208, [D ACC: %89.06], [D LOSS: 29.02], [G ACC: %78.12], [G LOSS: 41.13]\n",
      "E: 209, [D ACC: %82.81], [D LOSS: 41.43], [G ACC: %96.88], [G LOSS: 12.55]\n",
      "E: 210, [D ACC: %87.50], [D LOSS: 36.24], [G ACC: %96.88], [G LOSS: 17.30]\n",
      "E: 211, [D ACC: %98.44], [D LOSS: 20.19], [G ACC: %87.50], [G LOSS: 28.70]\n",
      "E: 212, [D ACC: %92.19], [D LOSS: 25.18], [G ACC: %93.75], [G LOSS: 27.45]\n",
      "E: 213, [D ACC: %87.50], [D LOSS: 35.30], [G ACC: %56.25], [G LOSS: 85.84]\n",
      "E: 214, [D ACC: %100.00], [D LOSS: 7.21], [G ACC: %40.62], [G LOSS: 114.91]\n",
      "E: 215, [D ACC: %96.88], [D LOSS: 29.40], [G ACC: %56.25], [G LOSS: 78.39]\n",
      "E: 216, [D ACC: %95.31], [D LOSS: 18.24], [G ACC: %87.50], [G LOSS: 38.44]\n",
      "E: 217, [D ACC: %96.88], [D LOSS: 21.86], [G ACC: %90.62], [G LOSS: 33.81]\n",
      "E: 218, [D ACC: %98.44], [D LOSS: 20.89], [G ACC: %96.88], [G LOSS: 29.01]\n",
      "E: 219, [D ACC: %100.00], [D LOSS: 17.79], [G ACC: %100.00], [G LOSS: 12.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 220, [D ACC: %98.44], [D LOSS: 21.58], [G ACC: %100.00], [G LOSS: 8.05]\n",
      "E: 221, [D ACC: %100.00], [D LOSS: 8.68], [G ACC: %100.00], [G LOSS: 7.52]\n",
      "E: 222, [D ACC: %100.00], [D LOSS: 13.58], [G ACC: %100.00], [G LOSS: 7.73]\n",
      "E: 223, [D ACC: %98.44], [D LOSS: 8.18], [G ACC: %100.00], [G LOSS: 5.67]\n",
      "E: 224, [D ACC: %98.44], [D LOSS: 31.22], [G ACC: %100.00], [G LOSS: 5.71]\n",
      "E: 225, [D ACC: %98.44], [D LOSS: 16.14], [G ACC: %100.00], [G LOSS: 4.74]\n",
      "E: 226, [D ACC: %98.44], [D LOSS: 8.70], [G ACC: %100.00], [G LOSS: 4.74]\n",
      "E: 227, [D ACC: %100.00], [D LOSS: 19.48], [G ACC: %100.00], [G LOSS: 4.64]\n",
      "E: 228, [D ACC: %98.44], [D LOSS: 29.91], [G ACC: %100.00], [G LOSS: 5.83]\n",
      "E: 229, [D ACC: %100.00], [D LOSS: 10.10], [G ACC: %100.00], [G LOSS: 5.62]\n",
      "E: 230, [D ACC: %100.00], [D LOSS: 13.88], [G ACC: %100.00], [G LOSS: 8.88]\n",
      "E: 231, [D ACC: %100.00], [D LOSS: 19.75], [G ACC: %100.00], [G LOSS: 10.61]\n",
      "E: 232, [D ACC: %100.00], [D LOSS: 11.47], [G ACC: %100.00], [G LOSS: 12.61]\n",
      "E: 233, [D ACC: %98.44], [D LOSS: 23.30], [G ACC: %100.00], [G LOSS: 10.43]\n",
      "E: 234, [D ACC: %100.00], [D LOSS: 6.93], [G ACC: %100.00], [G LOSS: 6.52]\n",
      "E: 235, [D ACC: %100.00], [D LOSS: 14.65], [G ACC: %100.00], [G LOSS: 4.47]\n",
      "E: 236, [D ACC: %98.44], [D LOSS: 7.30], [G ACC: %100.00], [G LOSS: 1.68]\n",
      "E: 237, [D ACC: %100.00], [D LOSS: 8.77], [G ACC: %100.00], [G LOSS: 1.60]\n",
      "E: 238, [D ACC: %100.00], [D LOSS: 16.87], [G ACC: %100.00], [G LOSS: 1.35]\n",
      "E: 239, [D ACC: %98.44], [D LOSS: 16.08], [G ACC: %100.00], [G LOSS: 0.77]\n",
      "E: 240, [D ACC: %100.00], [D LOSS: 11.36], [G ACC: %100.00], [G LOSS: 0.57]\n",
      "E: 241, [D ACC: %100.00], [D LOSS: 16.63], [G ACC: %100.00], [G LOSS: 0.56]\n",
      "E: 242, [D ACC: %98.44], [D LOSS: 21.59], [G ACC: %100.00], [G LOSS: 0.39]\n",
      "E: 243, [D ACC: %100.00], [D LOSS: 24.15], [G ACC: %100.00], [G LOSS: 0.68]\n",
      "E: 244, [D ACC: %100.00], [D LOSS: 10.41], [G ACC: %100.00], [G LOSS: 0.29]\n",
      "E: 245, [D ACC: %100.00], [D LOSS: 10.71], [G ACC: %100.00], [G LOSS: 0.43]\n",
      "E: 246, [D ACC: %100.00], [D LOSS: 10.60], [G ACC: %100.00], [G LOSS: 0.47]\n",
      "E: 247, [D ACC: %100.00], [D LOSS: 14.33], [G ACC: %100.00], [G LOSS: 0.32]\n",
      "E: 248, [D ACC: %96.88], [D LOSS: 8.58], [G ACC: %100.00], [G LOSS: 0.39]\n",
      "E: 249, [D ACC: %100.00], [D LOSS: 8.31], [G ACC: %100.00], [G LOSS: 0.25]\n",
      "E: 250, [D ACC: %100.00], [D LOSS: 11.29], [G ACC: %100.00], [G LOSS: 0.26]\n",
      "E: 251, [D ACC: %100.00], [D LOSS: 10.58], [G ACC: %100.00], [G LOSS: 0.14]\n",
      "E: 252, [D ACC: %100.00], [D LOSS: 15.50], [G ACC: %100.00], [G LOSS: 0.15]\n",
      "E: 253, [D ACC: %100.00], [D LOSS: 6.01], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 254, [D ACC: %100.00], [D LOSS: 6.46], [G ACC: %100.00], [G LOSS: 0.27]\n",
      "E: 255, [D ACC: %98.44], [D LOSS: 12.98], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 256, [D ACC: %98.44], [D LOSS: 7.12], [G ACC: %100.00], [G LOSS: 0.27]\n",
      "E: 257, [D ACC: %100.00], [D LOSS: 6.52], [G ACC: %100.00], [G LOSS: 0.21]\n",
      "E: 258, [D ACC: %98.44], [D LOSS: 5.94], [G ACC: %100.00], [G LOSS: 0.14]\n",
      "E: 259, [D ACC: %98.44], [D LOSS: 7.19], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 260, [D ACC: %100.00], [D LOSS: 5.91], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 261, [D ACC: %95.31], [D LOSS: 7.72], [G ACC: %100.00], [G LOSS: 0.12]\n",
      "E: 262, [D ACC: %100.00], [D LOSS: 11.22], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 263, [D ACC: %100.00], [D LOSS: 13.91], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 264, [D ACC: %100.00], [D LOSS: 17.24], [G ACC: %100.00], [G LOSS: 0.14]\n",
      "E: 265, [D ACC: %100.00], [D LOSS: 15.12], [G ACC: %100.00], [G LOSS: 0.19]\n",
      "E: 266, [D ACC: %100.00], [D LOSS: 11.65], [G ACC: %100.00], [G LOSS: 0.13]\n",
      "E: 267, [D ACC: %100.00], [D LOSS: 8.60], [G ACC: %100.00], [G LOSS: 0.22]\n",
      "E: 268, [D ACC: %98.44], [D LOSS: 23.35], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 269, [D ACC: %98.44], [D LOSS: 5.96], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 270, [D ACC: %100.00], [D LOSS: 14.26], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 271, [D ACC: %100.00], [D LOSS: 6.48], [G ACC: %100.00], [G LOSS: 0.13]\n",
      "E: 272, [D ACC: %98.44], [D LOSS: 20.50], [G ACC: %100.00], [G LOSS: 0.21]\n",
      "E: 273, [D ACC: %100.00], [D LOSS: 13.19], [G ACC: %100.00], [G LOSS: 0.24]\n",
      "E: 274, [D ACC: %100.00], [D LOSS: 13.05], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 275, [D ACC: %100.00], [D LOSS: 18.72], [G ACC: %100.00], [G LOSS: 0.19]\n",
      "E: 276, [D ACC: %100.00], [D LOSS: 9.65], [G ACC: %100.00], [G LOSS: 0.31]\n",
      "E: 277, [D ACC: %100.00], [D LOSS: 12.02], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 278, [D ACC: %100.00], [D LOSS: 9.23], [G ACC: %100.00], [G LOSS: 0.24]\n",
      "E: 279, [D ACC: %100.00], [D LOSS: 8.51], [G ACC: %100.00], [G LOSS: 0.24]\n",
      "E: 280, [D ACC: %100.00], [D LOSS: 8.18], [G ACC: %100.00], [G LOSS: 0.22]\n",
      "E: 281, [D ACC: %98.44], [D LOSS: 21.98], [G ACC: %100.00], [G LOSS: 0.28]\n",
      "E: 282, [D ACC: %92.19], [D LOSS: 20.06], [G ACC: %100.00], [G LOSS: 0.28]\n",
      "E: 283, [D ACC: %100.00], [D LOSS: 14.88], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 284, [D ACC: %95.31], [D LOSS: 10.30], [G ACC: %100.00], [G LOSS: 0.22]\n",
      "E: 285, [D ACC: %98.44], [D LOSS: 15.23], [G ACC: %100.00], [G LOSS: 0.21]\n",
      "E: 286, [D ACC: %98.44], [D LOSS: 12.94], [G ACC: %100.00], [G LOSS: 0.47]\n",
      "E: 287, [D ACC: %96.88], [D LOSS: 13.98], [G ACC: %100.00], [G LOSS: 0.96]\n",
      "E: 288, [D ACC: %100.00], [D LOSS: 5.03], [G ACC: %100.00], [G LOSS: 0.42]\n",
      "E: 289, [D ACC: %100.00], [D LOSS: 4.94], [G ACC: %100.00], [G LOSS: 0.39]\n",
      "E: 290, [D ACC: %98.44], [D LOSS: 8.50], [G ACC: %100.00], [G LOSS: 0.74]\n",
      "E: 291, [D ACC: %100.00], [D LOSS: 6.16], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 292, [D ACC: %98.44], [D LOSS: 18.03], [G ACC: %100.00], [G LOSS: 1.17]\n",
      "E: 293, [D ACC: %100.00], [D LOSS: 10.19], [G ACC: %100.00], [G LOSS: 1.41]\n",
      "E: 294, [D ACC: %98.44], [D LOSS: 9.23], [G ACC: %100.00], [G LOSS: 1.88]\n",
      "E: 295, [D ACC: %95.31], [D LOSS: 18.19], [G ACC: %100.00], [G LOSS: 3.62]\n",
      "E: 296, [D ACC: %98.44], [D LOSS: 11.19], [G ACC: %100.00], [G LOSS: 10.61]\n",
      "E: 297, [D ACC: %96.88], [D LOSS: 9.76], [G ACC: %100.00], [G LOSS: 6.60]\n",
      "E: 298, [D ACC: %96.88], [D LOSS: 11.02], [G ACC: %100.00], [G LOSS: 2.88]\n",
      "E: 299, [D ACC: %98.44], [D LOSS: 14.25], [G ACC: %100.00], [G LOSS: 1.33]\n",
      "E: 300, [D ACC: %98.44], [D LOSS: 11.58], [G ACC: %100.00], [G LOSS: 0.95]\n",
      "E: 301, [D ACC: %100.00], [D LOSS: 7.32], [G ACC: %100.00], [G LOSS: 0.97]\n",
      "E: 302, [D ACC: %96.88], [D LOSS: 17.61], [G ACC: %100.00], [G LOSS: 2.03]\n",
      "E: 303, [D ACC: %100.00], [D LOSS: 6.78], [G ACC: %100.00], [G LOSS: 1.31]\n",
      "E: 304, [D ACC: %96.88], [D LOSS: 10.18], [G ACC: %100.00], [G LOSS: 6.27]\n",
      "E: 305, [D ACC: %96.88], [D LOSS: 33.82], [G ACC: %100.00], [G LOSS: 3.71]\n",
      "E: 306, [D ACC: %100.00], [D LOSS: 15.50], [G ACC: %100.00], [G LOSS: 2.19]\n",
      "E: 307, [D ACC: %100.00], [D LOSS: 9.36], [G ACC: %100.00], [G LOSS: 1.63]\n",
      "E: 308, [D ACC: %98.44], [D LOSS: 6.13], [G ACC: %100.00], [G LOSS: 3.85]\n",
      "E: 309, [D ACC: %98.44], [D LOSS: 15.11], [G ACC: %100.00], [G LOSS: 3.28]\n",
      "E: 310, [D ACC: %96.88], [D LOSS: 19.29], [G ACC: %100.00], [G LOSS: 4.41]\n",
      "E: 311, [D ACC: %98.44], [D LOSS: 10.49], [G ACC: %100.00], [G LOSS: 4.41]\n",
      "E: 312, [D ACC: %98.44], [D LOSS: 10.87], [G ACC: %96.88], [G LOSS: 12.25]\n",
      "E: 313, [D ACC: %73.44], [D LOSS: 69.50], [G ACC: %62.50], [G LOSS: 79.76]\n",
      "E: 314, [D ACC: %96.88], [D LOSS: 12.37], [G ACC: %56.25], [G LOSS: 85.37]\n",
      "E: 315, [D ACC: %90.62], [D LOSS: 42.63], [G ACC: %90.62], [G LOSS: 30.93]\n",
      "E: 316, [D ACC: %90.62], [D LOSS: 24.66], [G ACC: %84.38], [G LOSS: 36.17]\n",
      "E: 317, [D ACC: %87.50], [D LOSS: 28.21], [G ACC: %96.88], [G LOSS: 11.27]\n",
      "E: 318, [D ACC: %95.31], [D LOSS: 17.61], [G ACC: %100.00], [G LOSS: 4.59]\n",
      "E: 319, [D ACC: %96.88], [D LOSS: 18.77], [G ACC: %100.00], [G LOSS: 2.20]\n",
      "E: 320, [D ACC: %85.94], [D LOSS: 32.79], [G ACC: %100.00], [G LOSS: 7.83]\n",
      "E: 321, [D ACC: %89.06], [D LOSS: 20.03], [G ACC: %96.88], [G LOSS: 8.46]\n",
      "E: 322, [D ACC: %98.44], [D LOSS: 30.20], [G ACC: %96.88], [G LOSS: 9.24]\n",
      "E: 323, [D ACC: %92.19], [D LOSS: 27.42], [G ACC: %96.88], [G LOSS: 8.45]\n",
      "E: 324, [D ACC: %84.38], [D LOSS: 52.24], [G ACC: %93.75], [G LOSS: 16.06]\n",
      "E: 325, [D ACC: %67.19], [D LOSS: 80.54], [G ACC: %71.88], [G LOSS: 51.42]\n",
      "E: 326, [D ACC: %65.62], [D LOSS: 87.58], [G ACC: %40.62], [G LOSS: 144.54]\n",
      "E: 327, [D ACC: %53.12], [D LOSS: 165.90], [G ACC: %28.12], [G LOSS: 168.43]\n",
      "E: 328, [D ACC: %51.56], [D LOSS: 161.79], [G ACC: %31.25], [G LOSS: 137.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 329, [D ACC: %89.06], [D LOSS: 27.48], [G ACC: %12.50], [G LOSS: 247.69]\n",
      "E: 330, [D ACC: %95.31], [D LOSS: 30.87], [G ACC: %65.62], [G LOSS: 60.12]\n",
      "E: 331, [D ACC: %85.94], [D LOSS: 45.09], [G ACC: %93.75], [G LOSS: 17.38]\n",
      "E: 332, [D ACC: %78.12], [D LOSS: 70.32], [G ACC: %96.88], [G LOSS: 10.51]\n",
      "E: 333, [D ACC: %81.25], [D LOSS: 47.75], [G ACC: %93.75], [G LOSS: 17.06]\n",
      "E: 334, [D ACC: %67.19], [D LOSS: 67.51], [G ACC: %90.62], [G LOSS: 29.75]\n",
      "E: 335, [D ACC: %76.56], [D LOSS: 55.37], [G ACC: %50.00], [G LOSS: 108.73]\n",
      "E: 336, [D ACC: %92.19], [D LOSS: 30.51], [G ACC: %12.50], [G LOSS: 189.17]\n",
      "E: 337, [D ACC: %76.56], [D LOSS: 56.44], [G ACC: %18.75], [G LOSS: 162.46]\n",
      "E: 338, [D ACC: %75.00], [D LOSS: 63.05], [G ACC: %9.38], [G LOSS: 219.84]\n",
      "E: 339, [D ACC: %82.81], [D LOSS: 47.40], [G ACC: %12.50], [G LOSS: 204.51]\n",
      "E: 340, [D ACC: %78.12], [D LOSS: 47.19], [G ACC: %9.38], [G LOSS: 265.09]\n",
      "E: 341, [D ACC: %82.81], [D LOSS: 47.67], [G ACC: %9.38], [G LOSS: 230.56]\n",
      "E: 342, [D ACC: %73.44], [D LOSS: 87.50], [G ACC: %37.50], [G LOSS: 131.42]\n",
      "E: 343, [D ACC: %60.94], [D LOSS: 84.37], [G ACC: %81.25], [G LOSS: 39.71]\n",
      "E: 344, [D ACC: %62.50], [D LOSS: 72.17], [G ACC: %78.12], [G LOSS: 40.67]\n",
      "E: 345, [D ACC: %90.62], [D LOSS: 22.85], [G ACC: %68.75], [G LOSS: 58.07]\n",
      "E: 346, [D ACC: %92.19], [D LOSS: 44.73], [G ACC: %84.38], [G LOSS: 31.49]\n",
      "E: 347, [D ACC: %81.25], [D LOSS: 54.52], [G ACC: %93.75], [G LOSS: 14.53]\n",
      "E: 348, [D ACC: %76.56], [D LOSS: 62.22], [G ACC: %75.00], [G LOSS: 50.18]\n",
      "E: 349, [D ACC: %84.38], [D LOSS: 34.89], [G ACC: %65.62], [G LOSS: 72.20]\n",
      "E: 350, [D ACC: %54.69], [D LOSS: 92.31], [G ACC: %62.50], [G LOSS: 79.67]\n",
      "E: 351, [D ACC: %82.81], [D LOSS: 47.24], [G ACC: %34.38], [G LOSS: 115.32]\n",
      "E: 352, [D ACC: %89.06], [D LOSS: 26.17], [G ACC: %25.00], [G LOSS: 130.64]\n",
      "E: 353, [D ACC: %90.62], [D LOSS: 40.15], [G ACC: %15.62], [G LOSS: 157.89]\n",
      "E: 354, [D ACC: %93.75], [D LOSS: 25.83], [G ACC: %12.50], [G LOSS: 150.67]\n",
      "E: 355, [D ACC: %78.12], [D LOSS: 59.62], [G ACC: %59.38], [G LOSS: 87.32]\n",
      "E: 356, [D ACC: %73.44], [D LOSS: 80.35], [G ACC: %50.00], [G LOSS: 86.32]\n",
      "E: 357, [D ACC: %84.38], [D LOSS: 44.47], [G ACC: %40.62], [G LOSS: 129.87]\n",
      "E: 358, [D ACC: %89.06], [D LOSS: 25.11], [G ACC: %62.50], [G LOSS: 69.45]\n",
      "E: 359, [D ACC: %85.94], [D LOSS: 32.89], [G ACC: %90.62], [G LOSS: 35.39]\n",
      "E: 360, [D ACC: %82.81], [D LOSS: 36.73], [G ACC: %84.38], [G LOSS: 39.84]\n",
      "E: 361, [D ACC: %87.50], [D LOSS: 31.60], [G ACC: %90.62], [G LOSS: 25.20]\n",
      "E: 362, [D ACC: %85.94], [D LOSS: 43.02], [G ACC: %90.62], [G LOSS: 28.08]\n",
      "E: 363, [D ACC: %89.06], [D LOSS: 27.77], [G ACC: %93.75], [G LOSS: 29.46]\n",
      "E: 364, [D ACC: %90.62], [D LOSS: 26.34], [G ACC: %90.62], [G LOSS: 25.15]\n",
      "E: 365, [D ACC: %92.19], [D LOSS: 24.92], [G ACC: %81.25], [G LOSS: 39.29]\n",
      "E: 366, [D ACC: %84.38], [D LOSS: 45.08], [G ACC: %62.50], [G LOSS: 64.45]\n",
      "E: 367, [D ACC: %93.75], [D LOSS: 34.46], [G ACC: %65.62], [G LOSS: 71.23]\n",
      "E: 368, [D ACC: %87.50], [D LOSS: 29.88], [G ACC: %62.50], [G LOSS: 83.66]\n",
      "E: 369, [D ACC: %82.81], [D LOSS: 52.43], [G ACC: %50.00], [G LOSS: 110.90]\n",
      "E: 370, [D ACC: %92.19], [D LOSS: 15.53], [G ACC: %53.12], [G LOSS: 100.22]\n",
      "E: 371, [D ACC: %90.62], [D LOSS: 25.62], [G ACC: %65.62], [G LOSS: 62.75]\n",
      "E: 372, [D ACC: %87.50], [D LOSS: 39.98], [G ACC: %71.88], [G LOSS: 57.47]\n",
      "E: 373, [D ACC: %75.00], [D LOSS: 51.27], [G ACC: %62.50], [G LOSS: 59.55]\n",
      "E: 374, [D ACC: %70.31], [D LOSS: 53.54], [G ACC: %50.00], [G LOSS: 72.90]\n",
      "E: 375, [D ACC: %85.94], [D LOSS: 41.10], [G ACC: %31.25], [G LOSS: 130.10]\n",
      "E: 376, [D ACC: %82.81], [D LOSS: 41.08], [G ACC: %43.75], [G LOSS: 101.64]\n",
      "E: 377, [D ACC: %90.62], [D LOSS: 32.23], [G ACC: %50.00], [G LOSS: 83.09]\n",
      "E: 378, [D ACC: %89.06], [D LOSS: 33.42], [G ACC: %53.12], [G LOSS: 66.08]\n",
      "E: 379, [D ACC: %82.81], [D LOSS: 41.14], [G ACC: %71.88], [G LOSS: 51.71]\n",
      "E: 380, [D ACC: %95.31], [D LOSS: 23.83], [G ACC: %90.62], [G LOSS: 31.05]\n",
      "E: 381, [D ACC: %96.88], [D LOSS: 11.02], [G ACC: %78.12], [G LOSS: 52.43]\n",
      "E: 382, [D ACC: %96.88], [D LOSS: 12.54], [G ACC: %81.25], [G LOSS: 43.46]\n",
      "E: 383, [D ACC: %96.88], [D LOSS: 14.63], [G ACC: %75.00], [G LOSS: 38.56]\n",
      "E: 384, [D ACC: %98.44], [D LOSS: 9.37], [G ACC: %71.88], [G LOSS: 48.93]\n",
      "E: 385, [D ACC: %96.88], [D LOSS: 14.11], [G ACC: %84.38], [G LOSS: 40.22]\n",
      "E: 386, [D ACC: %98.44], [D LOSS: 17.87], [G ACC: %68.75], [G LOSS: 54.52]\n",
      "E: 387, [D ACC: %100.00], [D LOSS: 9.33], [G ACC: %81.25], [G LOSS: 44.12]\n",
      "E: 388, [D ACC: %98.44], [D LOSS: 12.14], [G ACC: %90.62], [G LOSS: 22.63]\n",
      "E: 389, [D ACC: %95.31], [D LOSS: 23.86], [G ACC: %90.62], [G LOSS: 20.53]\n",
      "E: 390, [D ACC: %98.44], [D LOSS: 14.23], [G ACC: %100.00], [G LOSS: 14.45]\n",
      "E: 391, [D ACC: %96.88], [D LOSS: 10.16], [G ACC: %100.00], [G LOSS: 3.84]\n",
      "E: 392, [D ACC: %100.00], [D LOSS: 7.82], [G ACC: %100.00], [G LOSS: 0.83]\n",
      "E: 393, [D ACC: %96.88], [D LOSS: 10.01], [G ACC: %100.00], [G LOSS: 0.21]\n",
      "E: 394, [D ACC: %100.00], [D LOSS: 6.02], [G ACC: %100.00], [G LOSS: 0.11]\n",
      "E: 395, [D ACC: %100.00], [D LOSS: 7.54], [G ACC: %100.00], [G LOSS: 0.12]\n",
      "E: 396, [D ACC: %98.44], [D LOSS: 13.67], [G ACC: %100.00], [G LOSS: 0.08]\n",
      "E: 397, [D ACC: %98.44], [D LOSS: 17.55], [G ACC: %100.00], [G LOSS: 0.16]\n",
      "E: 398, [D ACC: %100.00], [D LOSS: 7.90], [G ACC: %100.00], [G LOSS: 0.11]\n",
      "E: 399, [D ACC: %98.44], [D LOSS: 23.07], [G ACC: %100.00], [G LOSS: 0.12]\n",
      "E: 400, [D ACC: %100.00], [D LOSS: 9.17], [G ACC: %100.00], [G LOSS: 0.12]\n",
      "E: 401, [D ACC: %95.31], [D LOSS: 15.18], [G ACC: %100.00], [G LOSS: 0.15]\n",
      "E: 402, [D ACC: %96.88], [D LOSS: 25.15], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 403, [D ACC: %98.44], [D LOSS: 18.65], [G ACC: %100.00], [G LOSS: 0.40]\n",
      "E: 404, [D ACC: %100.00], [D LOSS: 9.10], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 405, [D ACC: %98.44], [D LOSS: 9.30], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 406, [D ACC: %98.44], [D LOSS: 7.49], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 407, [D ACC: %100.00], [D LOSS: 6.64], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 408, [D ACC: %96.88], [D LOSS: 13.06], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 409, [D ACC: %100.00], [D LOSS: 18.70], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 410, [D ACC: %100.00], [D LOSS: 6.77], [G ACC: %100.00], [G LOSS: 0.11]\n",
      "E: 411, [D ACC: %100.00], [D LOSS: 16.31], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 412, [D ACC: %100.00], [D LOSS: 7.27], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 413, [D ACC: %98.44], [D LOSS: 19.38], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 414, [D ACC: %96.88], [D LOSS: 10.55], [G ACC: %100.00], [G LOSS: 0.22]\n",
      "E: 415, [D ACC: %100.00], [D LOSS: 6.75], [G ACC: %100.00], [G LOSS: 0.16]\n",
      "E: 416, [D ACC: %100.00], [D LOSS: 5.00], [G ACC: %100.00], [G LOSS: 0.14]\n",
      "E: 417, [D ACC: %98.44], [D LOSS: 9.30], [G ACC: %100.00], [G LOSS: 0.14]\n",
      "E: 418, [D ACC: %98.44], [D LOSS: 13.75], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 419, [D ACC: %96.88], [D LOSS: 16.39], [G ACC: %100.00], [G LOSS: 0.15]\n",
      "E: 420, [D ACC: %100.00], [D LOSS: 6.48], [G ACC: %100.00], [G LOSS: 0.13]\n",
      "E: 421, [D ACC: %96.88], [D LOSS: 12.30], [G ACC: %100.00], [G LOSS: 0.19]\n",
      "E: 422, [D ACC: %100.00], [D LOSS: 7.00], [G ACC: %100.00], [G LOSS: 0.15]\n",
      "E: 423, [D ACC: %95.31], [D LOSS: 10.25], [G ACC: %100.00], [G LOSS: 0.31]\n",
      "E: 424, [D ACC: %96.88], [D LOSS: 10.02], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 425, [D ACC: %98.44], [D LOSS: 8.91], [G ACC: %100.00], [G LOSS: 0.29]\n",
      "E: 426, [D ACC: %98.44], [D LOSS: 8.27], [G ACC: %100.00], [G LOSS: 0.37]\n",
      "E: 427, [D ACC: %98.44], [D LOSS: 24.26], [G ACC: %100.00], [G LOSS: 0.19]\n",
      "E: 428, [D ACC: %96.88], [D LOSS: 9.34], [G ACC: %100.00], [G LOSS: 0.21]\n",
      "E: 429, [D ACC: %100.00], [D LOSS: 5.92], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 430, [D ACC: %95.31], [D LOSS: 11.64], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 431, [D ACC: %98.44], [D LOSS: 10.30], [G ACC: %100.00], [G LOSS: 0.30]\n",
      "E: 432, [D ACC: %96.88], [D LOSS: 11.36], [G ACC: %100.00], [G LOSS: 0.41]\n",
      "E: 433, [D ACC: %100.00], [D LOSS: 22.08], [G ACC: %100.00], [G LOSS: 0.28]\n",
      "E: 434, [D ACC: %98.44], [D LOSS: 7.33], [G ACC: %100.00], [G LOSS: 0.57]\n",
      "E: 435, [D ACC: %100.00], [D LOSS: 24.28], [G ACC: %100.00], [G LOSS: 0.33]\n",
      "E: 436, [D ACC: %100.00], [D LOSS: 13.54], [G ACC: %100.00], [G LOSS: 0.29]\n",
      "E: 437, [D ACC: %100.00], [D LOSS: 12.76], [G ACC: %100.00], [G LOSS: 0.46]\n",
      "E: 438, [D ACC: %100.00], [D LOSS: 20.52], [G ACC: %100.00], [G LOSS: 0.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 439, [D ACC: %100.00], [D LOSS: 7.88], [G ACC: %100.00], [G LOSS: 0.42]\n",
      "E: 440, [D ACC: %100.00], [D LOSS: 6.11], [G ACC: %100.00], [G LOSS: 0.53]\n",
      "E: 441, [D ACC: %98.44], [D LOSS: 21.79], [G ACC: %100.00], [G LOSS: 0.66]\n",
      "E: 442, [D ACC: %98.44], [D LOSS: 7.33], [G ACC: %100.00], [G LOSS: 0.40]\n",
      "E: 443, [D ACC: %98.44], [D LOSS: 4.38], [G ACC: %100.00], [G LOSS: 0.46]\n",
      "E: 444, [D ACC: %96.88], [D LOSS: 15.96], [G ACC: %100.00], [G LOSS: 0.56]\n",
      "E: 445, [D ACC: %98.44], [D LOSS: 6.04], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 446, [D ACC: %100.00], [D LOSS: 5.29], [G ACC: %100.00], [G LOSS: 0.48]\n",
      "E: 447, [D ACC: %100.00], [D LOSS: 4.96], [G ACC: %100.00], [G LOSS: 0.70]\n",
      "E: 448, [D ACC: %100.00], [D LOSS: 8.83], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 449, [D ACC: %100.00], [D LOSS: 6.04], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 450, [D ACC: %100.00], [D LOSS: 15.03], [G ACC: %100.00], [G LOSS: 0.46]\n",
      "E: 451, [D ACC: %96.88], [D LOSS: 11.37], [G ACC: %100.00], [G LOSS: 0.47]\n",
      "E: 452, [D ACC: %100.00], [D LOSS: 7.59], [G ACC: %100.00], [G LOSS: 0.55]\n",
      "E: 453, [D ACC: %98.44], [D LOSS: 5.27], [G ACC: %100.00], [G LOSS: 0.59]\n",
      "E: 454, [D ACC: %100.00], [D LOSS: 9.61], [G ACC: %100.00], [G LOSS: 0.56]\n",
      "E: 455, [D ACC: %100.00], [D LOSS: 7.13], [G ACC: %100.00], [G LOSS: 0.64]\n",
      "E: 456, [D ACC: %100.00], [D LOSS: 17.96], [G ACC: %100.00], [G LOSS: 0.57]\n",
      "E: 457, [D ACC: %100.00], [D LOSS: 6.00], [G ACC: %100.00], [G LOSS: 0.99]\n",
      "E: 458, [D ACC: %98.44], [D LOSS: 11.89], [G ACC: %100.00], [G LOSS: 1.02]\n",
      "E: 459, [D ACC: %100.00], [D LOSS: 11.95], [G ACC: %100.00], [G LOSS: 1.06]\n",
      "E: 460, [D ACC: %100.00], [D LOSS: 21.06], [G ACC: %100.00], [G LOSS: 1.42]\n",
      "E: 461, [D ACC: %98.44], [D LOSS: 12.87], [G ACC: %100.00], [G LOSS: 1.04]\n",
      "E: 462, [D ACC: %98.44], [D LOSS: 10.99], [G ACC: %100.00], [G LOSS: 1.63]\n",
      "E: 463, [D ACC: %100.00], [D LOSS: 11.60], [G ACC: %100.00], [G LOSS: 2.62]\n",
      "E: 464, [D ACC: %100.00], [D LOSS: 12.99], [G ACC: %100.00], [G LOSS: 1.09]\n",
      "E: 465, [D ACC: %96.88], [D LOSS: 14.78], [G ACC: %100.00], [G LOSS: 2.14]\n",
      "E: 466, [D ACC: %100.00], [D LOSS: 10.55], [G ACC: %100.00], [G LOSS: 3.23]\n",
      "E: 467, [D ACC: %100.00], [D LOSS: 7.93], [G ACC: %100.00], [G LOSS: 2.84]\n",
      "E: 468, [D ACC: %98.44], [D LOSS: 15.97], [G ACC: %100.00], [G LOSS: 4.16]\n",
      "E: 469, [D ACC: %98.44], [D LOSS: 7.31], [G ACC: %100.00], [G LOSS: 4.09]\n",
      "E: 470, [D ACC: %98.44], [D LOSS: 5.65], [G ACC: %100.00], [G LOSS: 6.23]\n",
      "E: 471, [D ACC: %100.00], [D LOSS: 9.20], [G ACC: %100.00], [G LOSS: 3.84]\n",
      "E: 472, [D ACC: %100.00], [D LOSS: 4.56], [G ACC: %100.00], [G LOSS: 3.33]\n",
      "E: 473, [D ACC: %100.00], [D LOSS: 10.28], [G ACC: %100.00], [G LOSS: 4.20]\n",
      "E: 474, [D ACC: %98.44], [D LOSS: 9.88], [G ACC: %100.00], [G LOSS: 5.56]\n",
      "E: 475, [D ACC: %100.00], [D LOSS: 7.94], [G ACC: %100.00], [G LOSS: 7.44]\n",
      "E: 476, [D ACC: %95.31], [D LOSS: 16.84], [G ACC: %100.00], [G LOSS: 8.19]\n",
      "E: 477, [D ACC: %100.00], [D LOSS: 9.08], [G ACC: %100.00], [G LOSS: 4.51]\n",
      "E: 478, [D ACC: %98.44], [D LOSS: 10.60], [G ACC: %100.00], [G LOSS: 3.16]\n",
      "E: 479, [D ACC: %98.44], [D LOSS: 9.44], [G ACC: %100.00], [G LOSS: 2.31]\n",
      "E: 480, [D ACC: %96.88], [D LOSS: 8.38], [G ACC: %100.00], [G LOSS: 3.06]\n",
      "E: 481, [D ACC: %95.31], [D LOSS: 17.73], [G ACC: %100.00], [G LOSS: 0.53]\n",
      "E: 482, [D ACC: %92.19], [D LOSS: 18.82], [G ACC: %100.00], [G LOSS: 3.10]\n",
      "E: 483, [D ACC: %93.75], [D LOSS: 20.48], [G ACC: %100.00], [G LOSS: 3.99]\n",
      "E: 484, [D ACC: %100.00], [D LOSS: 14.21], [G ACC: %100.00], [G LOSS: 13.00]\n",
      "E: 485, [D ACC: %93.75], [D LOSS: 32.92], [G ACC: %100.00], [G LOSS: 6.43]\n",
      "E: 486, [D ACC: %96.88], [D LOSS: 14.33], [G ACC: %100.00], [G LOSS: 12.42]\n",
      "E: 487, [D ACC: %98.44], [D LOSS: 13.82], [G ACC: %96.88], [G LOSS: 18.06]\n",
      "E: 488, [D ACC: %100.00], [D LOSS: 7.60], [G ACC: %100.00], [G LOSS: 7.89]\n",
      "E: 489, [D ACC: %98.44], [D LOSS: 8.12], [G ACC: %100.00], [G LOSS: 8.57]\n",
      "E: 490, [D ACC: %100.00], [D LOSS: 13.14], [G ACC: %100.00], [G LOSS: 7.76]\n",
      "E: 491, [D ACC: %92.19], [D LOSS: 20.69], [G ACC: %100.00], [G LOSS: 19.85]\n",
      "E: 492, [D ACC: %100.00], [D LOSS: 15.25], [G ACC: %100.00], [G LOSS: 29.82]\n",
      "E: 493, [D ACC: %90.62], [D LOSS: 36.97], [G ACC: %96.88], [G LOSS: 25.81]\n",
      "E: 494, [D ACC: %60.94], [D LOSS: 107.45], [G ACC: %0.00], [G LOSS: 310.43]\n",
      "E: 495, [D ACC: %73.44], [D LOSS: 59.15], [G ACC: %15.62], [G LOSS: 135.64]\n",
      "E: 496, [D ACC: %96.88], [D LOSS: 16.59], [G ACC: %3.12], [G LOSS: 172.37]\n",
      "E: 497, [D ACC: %79.69], [D LOSS: 40.96], [G ACC: %3.12], [G LOSS: 202.69]\n",
      "E: 498, [D ACC: %98.44], [D LOSS: 10.14], [G ACC: %0.00], [G LOSS: 233.49]\n",
      "E: 499, [D ACC: %100.00], [D LOSS: 22.31], [G ACC: %9.38], [G LOSS: 188.18]\n",
      "E: 500, [D ACC: %90.62], [D LOSS: 24.02], [G ACC: %43.75], [G LOSS: 75.91]\n",
      "E: 501, [D ACC: %73.44], [D LOSS: 71.20], [G ACC: %68.75], [G LOSS: 52.05]\n",
      "E: 502, [D ACC: %100.00], [D LOSS: 7.07], [G ACC: %100.00], [G LOSS: 0.03]\n",
      "E: 503, [D ACC: %90.62], [D LOSS: 23.58], [G ACC: %100.00], [G LOSS: 0.12]\n",
      "E: 504, [D ACC: %98.44], [D LOSS: 9.53], [G ACC: %100.00], [G LOSS: 0.28]\n",
      "E: 505, [D ACC: %100.00], [D LOSS: 9.53], [G ACC: %100.00], [G LOSS: 0.80]\n",
      "E: 506, [D ACC: %100.00], [D LOSS: 4.55], [G ACC: %100.00], [G LOSS: 0.43]\n",
      "E: 507, [D ACC: %100.00], [D LOSS: 6.48], [G ACC: %100.00], [G LOSS: 0.78]\n",
      "E: 508, [D ACC: %100.00], [D LOSS: 5.56], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 509, [D ACC: %100.00], [D LOSS: 5.75], [G ACC: %100.00], [G LOSS: 0.60]\n",
      "E: 510, [D ACC: %100.00], [D LOSS: 6.51], [G ACC: %100.00], [G LOSS: 0.65]\n",
      "E: 511, [D ACC: %100.00], [D LOSS: 7.57], [G ACC: %100.00], [G LOSS: 0.78]\n",
      "E: 512, [D ACC: %98.44], [D LOSS: 6.81], [G ACC: %100.00], [G LOSS: 0.58]\n",
      "E: 513, [D ACC: %98.44], [D LOSS: 28.53], [G ACC: %100.00], [G LOSS: 1.00]\n",
      "E: 514, [D ACC: %100.00], [D LOSS: 13.58], [G ACC: %100.00], [G LOSS: 1.37]\n",
      "E: 515, [D ACC: %100.00], [D LOSS: 6.40], [G ACC: %100.00], [G LOSS: 0.69]\n",
      "E: 516, [D ACC: %96.88], [D LOSS: 11.00], [G ACC: %100.00], [G LOSS: 0.77]\n",
      "E: 517, [D ACC: %100.00], [D LOSS: 6.11], [G ACC: %100.00], [G LOSS: 1.12]\n",
      "E: 518, [D ACC: %98.44], [D LOSS: 7.36], [G ACC: %100.00], [G LOSS: 0.63]\n",
      "E: 519, [D ACC: %100.00], [D LOSS: 4.70], [G ACC: %100.00], [G LOSS: 0.90]\n",
      "E: 520, [D ACC: %98.44], [D LOSS: 5.93], [G ACC: %100.00], [G LOSS: 0.77]\n",
      "E: 521, [D ACC: %100.00], [D LOSS: 4.84], [G ACC: %100.00], [G LOSS: 1.03]\n",
      "E: 522, [D ACC: %100.00], [D LOSS: 2.44], [G ACC: %100.00], [G LOSS: 0.91]\n",
      "E: 523, [D ACC: %98.44], [D LOSS: 4.37], [G ACC: %100.00], [G LOSS: 1.07]\n",
      "E: 524, [D ACC: %100.00], [D LOSS: 5.01], [G ACC: %100.00], [G LOSS: 2.39]\n",
      "E: 525, [D ACC: %100.00], [D LOSS: 6.17], [G ACC: %100.00], [G LOSS: 0.64]\n",
      "E: 526, [D ACC: %100.00], [D LOSS: 2.29], [G ACC: %100.00], [G LOSS: 2.79]\n",
      "E: 527, [D ACC: %98.44], [D LOSS: 4.29], [G ACC: %100.00], [G LOSS: 0.59]\n",
      "E: 528, [D ACC: %100.00], [D LOSS: 8.72], [G ACC: %100.00], [G LOSS: 0.70]\n",
      "E: 529, [D ACC: %98.44], [D LOSS: 6.73], [G ACC: %100.00], [G LOSS: 0.94]\n",
      "E: 530, [D ACC: %100.00], [D LOSS: 10.07], [G ACC: %100.00], [G LOSS: 0.96]\n",
      "E: 531, [D ACC: %100.00], [D LOSS: 7.75], [G ACC: %100.00], [G LOSS: 0.99]\n",
      "E: 532, [D ACC: %100.00], [D LOSS: 5.52], [G ACC: %100.00], [G LOSS: 1.31]\n",
      "E: 533, [D ACC: %100.00], [D LOSS: 6.97], [G ACC: %100.00], [G LOSS: 1.77]\n",
      "E: 534, [D ACC: %98.44], [D LOSS: 8.13], [G ACC: %100.00], [G LOSS: 2.09]\n",
      "E: 535, [D ACC: %98.44], [D LOSS: 6.75], [G ACC: %100.00], [G LOSS: 3.07]\n",
      "E: 536, [D ACC: %98.44], [D LOSS: 5.87], [G ACC: %100.00], [G LOSS: 2.42]\n",
      "E: 537, [D ACC: %98.44], [D LOSS: 11.16], [G ACC: %100.00], [G LOSS: 1.85]\n",
      "E: 538, [D ACC: %98.44], [D LOSS: 17.66], [G ACC: %100.00], [G LOSS: 1.62]\n",
      "E: 539, [D ACC: %96.88], [D LOSS: 7.28], [G ACC: %100.00], [G LOSS: 1.74]\n",
      "E: 540, [D ACC: %100.00], [D LOSS: 5.75], [G ACC: %100.00], [G LOSS: 2.11]\n",
      "E: 541, [D ACC: %98.44], [D LOSS: 14.57], [G ACC: %100.00], [G LOSS: 3.91]\n",
      "E: 542, [D ACC: %100.00], [D LOSS: 6.62], [G ACC: %100.00], [G LOSS: 5.27]\n",
      "E: 543, [D ACC: %100.00], [D LOSS: 8.73], [G ACC: %100.00], [G LOSS: 7.62]\n",
      "E: 544, [D ACC: %98.44], [D LOSS: 3.99], [G ACC: %100.00], [G LOSS: 5.78]\n",
      "E: 545, [D ACC: %100.00], [D LOSS: 3.35], [G ACC: %100.00], [G LOSS: 3.64]\n",
      "E: 546, [D ACC: %100.00], [D LOSS: 9.54], [G ACC: %100.00], [G LOSS: 4.99]\n",
      "E: 547, [D ACC: %100.00], [D LOSS: 4.78], [G ACC: %100.00], [G LOSS: 7.08]\n",
      "E: 548, [D ACC: %100.00], [D LOSS: 6.85], [G ACC: %100.00], [G LOSS: 9.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 549, [D ACC: %100.00], [D LOSS: 3.59], [G ACC: %100.00], [G LOSS: 3.91]\n",
      "E: 550, [D ACC: %98.44], [D LOSS: 7.67], [G ACC: %100.00], [G LOSS: 0.92]\n",
      "E: 551, [D ACC: %100.00], [D LOSS: 14.79], [G ACC: %100.00], [G LOSS: 0.72]\n",
      "E: 552, [D ACC: %100.00], [D LOSS: 4.59], [G ACC: %100.00], [G LOSS: 1.04]\n",
      "E: 553, [D ACC: %95.31], [D LOSS: 10.59], [G ACC: %100.00], [G LOSS: 1.88]\n",
      "E: 554, [D ACC: %98.44], [D LOSS: 7.64], [G ACC: %100.00], [G LOSS: 1.74]\n",
      "E: 555, [D ACC: %96.88], [D LOSS: 14.46], [G ACC: %100.00], [G LOSS: 1.18]\n",
      "E: 556, [D ACC: %100.00], [D LOSS: 8.30], [G ACC: %100.00], [G LOSS: 3.65]\n",
      "E: 557, [D ACC: %100.00], [D LOSS: 7.52], [G ACC: %100.00], [G LOSS: 3.65]\n",
      "E: 558, [D ACC: %100.00], [D LOSS: 18.49], [G ACC: %96.88], [G LOSS: 17.29]\n",
      "E: 559, [D ACC: %98.44], [D LOSS: 10.93], [G ACC: %9.38], [G LOSS: 143.72]\n",
      "E: 560, [D ACC: %78.12], [D LOSS: 57.26], [G ACC: %6.25], [G LOSS: 278.93]\n",
      "E: 561, [D ACC: %90.62], [D LOSS: 27.15], [G ACC: %9.38], [G LOSS: 255.51]\n",
      "E: 562, [D ACC: %100.00], [D LOSS: 16.54], [G ACC: %6.25], [G LOSS: 238.80]\n",
      "E: 563, [D ACC: %100.00], [D LOSS: 9.16], [G ACC: %25.00], [G LOSS: 169.27]\n",
      "E: 564, [D ACC: %98.44], [D LOSS: 8.92], [G ACC: %68.75], [G LOSS: 65.54]\n",
      "E: 565, [D ACC: %100.00], [D LOSS: 6.57], [G ACC: %75.00], [G LOSS: 56.40]\n",
      "E: 566, [D ACC: %98.44], [D LOSS: 10.22], [G ACC: %87.50], [G LOSS: 29.36]\n",
      "E: 567, [D ACC: %95.31], [D LOSS: 34.31], [G ACC: %90.62], [G LOSS: 23.97]\n",
      "E: 568, [D ACC: %98.44], [D LOSS: 18.65], [G ACC: %93.75], [G LOSS: 24.57]\n",
      "E: 569, [D ACC: %98.44], [D LOSS: 22.35], [G ACC: %84.38], [G LOSS: 41.94]\n",
      "E: 570, [D ACC: %98.44], [D LOSS: 10.28], [G ACC: %93.75], [G LOSS: 17.76]\n",
      "E: 571, [D ACC: %96.88], [D LOSS: 9.94], [G ACC: %96.88], [G LOSS: 23.23]\n",
      "E: 572, [D ACC: %95.31], [D LOSS: 14.26], [G ACC: %96.88], [G LOSS: 20.13]\n",
      "E: 573, [D ACC: %100.00], [D LOSS: 8.98], [G ACC: %93.75], [G LOSS: 23.37]\n",
      "E: 574, [D ACC: %95.31], [D LOSS: 25.01], [G ACC: %84.38], [G LOSS: 42.39]\n",
      "E: 575, [D ACC: %98.44], [D LOSS: 10.44], [G ACC: %90.62], [G LOSS: 33.06]\n",
      "E: 576, [D ACC: %100.00], [D LOSS: 7.16], [G ACC: %93.75], [G LOSS: 27.56]\n",
      "E: 577, [D ACC: %100.00], [D LOSS: 20.56], [G ACC: %93.75], [G LOSS: 23.03]\n",
      "E: 578, [D ACC: %100.00], [D LOSS: 12.13], [G ACC: %96.88], [G LOSS: 20.71]\n",
      "E: 579, [D ACC: %98.44], [D LOSS: 12.53], [G ACC: %84.38], [G LOSS: 31.60]\n",
      "E: 580, [D ACC: %100.00], [D LOSS: 8.43], [G ACC: %93.75], [G LOSS: 31.89]\n",
      "E: 581, [D ACC: %100.00], [D LOSS: 6.78], [G ACC: %96.88], [G LOSS: 21.95]\n",
      "E: 582, [D ACC: %96.88], [D LOSS: 7.32], [G ACC: %100.00], [G LOSS: 13.46]\n",
      "E: 583, [D ACC: %100.00], [D LOSS: 11.99], [G ACC: %100.00], [G LOSS: 7.73]\n",
      "E: 584, [D ACC: %98.44], [D LOSS: 6.66], [G ACC: %100.00], [G LOSS: 5.94]\n",
      "E: 585, [D ACC: %100.00], [D LOSS: 2.83], [G ACC: %100.00], [G LOSS: 6.12]\n",
      "E: 586, [D ACC: %98.44], [D LOSS: 5.63], [G ACC: %100.00], [G LOSS: 4.38]\n",
      "E: 587, [D ACC: %100.00], [D LOSS: 25.28], [G ACC: %100.00], [G LOSS: 2.68]\n",
      "E: 588, [D ACC: %100.00], [D LOSS: 7.34], [G ACC: %100.00], [G LOSS: 1.92]\n",
      "E: 589, [D ACC: %100.00], [D LOSS: 8.69], [G ACC: %100.00], [G LOSS: 1.69]\n",
      "E: 590, [D ACC: %100.00], [D LOSS: 20.95], [G ACC: %100.00], [G LOSS: 1.69]\n",
      "E: 591, [D ACC: %100.00], [D LOSS: 7.34], [G ACC: %100.00], [G LOSS: 1.96]\n",
      "E: 592, [D ACC: %100.00], [D LOSS: 2.21], [G ACC: %100.00], [G LOSS: 1.50]\n",
      "E: 593, [D ACC: %100.00], [D LOSS: 7.92], [G ACC: %100.00], [G LOSS: 1.70]\n",
      "E: 594, [D ACC: %98.44], [D LOSS: 4.42], [G ACC: %100.00], [G LOSS: 1.88]\n",
      "E: 595, [D ACC: %100.00], [D LOSS: 4.54], [G ACC: %100.00], [G LOSS: 1.84]\n",
      "E: 596, [D ACC: %100.00], [D LOSS: 4.68], [G ACC: %100.00], [G LOSS: 1.19]\n",
      "E: 597, [D ACC: %100.00], [D LOSS: 2.35], [G ACC: %100.00], [G LOSS: 1.24]\n",
      "E: 598, [D ACC: %100.00], [D LOSS: 4.38], [G ACC: %100.00], [G LOSS: 1.01]\n",
      "E: 599, [D ACC: %98.44], [D LOSS: 4.05], [G ACC: %100.00], [G LOSS: 0.94]\n",
      "E: 600, [D ACC: %100.00], [D LOSS: 4.62], [G ACC: %100.00], [G LOSS: 0.73]\n",
      "E: 601, [D ACC: %100.00], [D LOSS: 2.69], [G ACC: %100.00], [G LOSS: 0.89]\n",
      "E: 602, [D ACC: %98.44], [D LOSS: 11.58], [G ACC: %100.00], [G LOSS: 0.71]\n",
      "E: 603, [D ACC: %100.00], [D LOSS: 2.66], [G ACC: %100.00], [G LOSS: 0.72]\n",
      "E: 604, [D ACC: %96.88], [D LOSS: 8.77], [G ACC: %100.00], [G LOSS: 0.67]\n",
      "E: 605, [D ACC: %100.00], [D LOSS: 3.73], [G ACC: %100.00], [G LOSS: 0.93]\n",
      "E: 606, [D ACC: %100.00], [D LOSS: 4.74], [G ACC: %100.00], [G LOSS: 0.68]\n",
      "E: 607, [D ACC: %100.00], [D LOSS: 8.34], [G ACC: %100.00], [G LOSS: 0.58]\n",
      "E: 608, [D ACC: %100.00], [D LOSS: 13.39], [G ACC: %100.00], [G LOSS: 0.50]\n",
      "E: 609, [D ACC: %100.00], [D LOSS: 5.29], [G ACC: %100.00], [G LOSS: 0.53]\n",
      "E: 610, [D ACC: %100.00], [D LOSS: 5.99], [G ACC: %100.00], [G LOSS: 0.63]\n",
      "E: 611, [D ACC: %96.88], [D LOSS: 9.14], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 612, [D ACC: %96.88], [D LOSS: 7.42], [G ACC: %100.00], [G LOSS: 0.41]\n",
      "E: 613, [D ACC: %100.00], [D LOSS: 12.10], [G ACC: %100.00], [G LOSS: 0.46]\n",
      "E: 614, [D ACC: %100.00], [D LOSS: 13.00], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 615, [D ACC: %100.00], [D LOSS: 3.55], [G ACC: %100.00], [G LOSS: 0.41]\n",
      "E: 616, [D ACC: %100.00], [D LOSS: 3.26], [G ACC: %100.00], [G LOSS: 0.56]\n",
      "E: 617, [D ACC: %96.88], [D LOSS: 5.08], [G ACC: %100.00], [G LOSS: 0.64]\n",
      "E: 618, [D ACC: %100.00], [D LOSS: 4.92], [G ACC: %100.00], [G LOSS: 0.42]\n",
      "E: 619, [D ACC: %100.00], [D LOSS: 8.93], [G ACC: %100.00], [G LOSS: 0.36]\n",
      "E: 620, [D ACC: %100.00], [D LOSS: 13.15], [G ACC: %100.00], [G LOSS: 0.67]\n",
      "E: 621, [D ACC: %100.00], [D LOSS: 4.44], [G ACC: %100.00], [G LOSS: 0.51]\n",
      "E: 622, [D ACC: %100.00], [D LOSS: 3.65], [G ACC: %100.00], [G LOSS: 0.42]\n",
      "E: 623, [D ACC: %98.44], [D LOSS: 5.45], [G ACC: %100.00], [G LOSS: 0.53]\n",
      "E: 624, [D ACC: %100.00], [D LOSS: 3.72], [G ACC: %100.00], [G LOSS: 0.61]\n",
      "E: 625, [D ACC: %100.00], [D LOSS: 3.79], [G ACC: %100.00], [G LOSS: 0.80]\n",
      "E: 626, [D ACC: %98.44], [D LOSS: 6.88], [G ACC: %100.00], [G LOSS: 0.48]\n",
      "E: 627, [D ACC: %100.00], [D LOSS: 4.08], [G ACC: %100.00], [G LOSS: 0.46]\n",
      "E: 628, [D ACC: %100.00], [D LOSS: 6.74], [G ACC: %100.00], [G LOSS: 0.60]\n",
      "E: 629, [D ACC: %100.00], [D LOSS: 2.88], [G ACC: %100.00], [G LOSS: 0.56]\n",
      "E: 630, [D ACC: %100.00], [D LOSS: 2.74], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 631, [D ACC: %98.44], [D LOSS: 12.12], [G ACC: %100.00], [G LOSS: 0.64]\n",
      "E: 632, [D ACC: %96.88], [D LOSS: 8.22], [G ACC: %100.00], [G LOSS: 0.54]\n",
      "E: 633, [D ACC: %100.00], [D LOSS: 4.32], [G ACC: %100.00], [G LOSS: 0.56]\n",
      "E: 634, [D ACC: %100.00], [D LOSS: 3.21], [G ACC: %100.00], [G LOSS: 0.52]\n",
      "E: 635, [D ACC: %100.00], [D LOSS: 2.50], [G ACC: %100.00], [G LOSS: 0.52]\n",
      "E: 636, [D ACC: %98.44], [D LOSS: 4.45], [G ACC: %100.00], [G LOSS: 0.48]\n",
      "E: 637, [D ACC: %100.00], [D LOSS: 3.21], [G ACC: %100.00], [G LOSS: 0.58]\n",
      "E: 638, [D ACC: %100.00], [D LOSS: 5.29], [G ACC: %100.00], [G LOSS: 0.47]\n",
      "E: 639, [D ACC: %98.44], [D LOSS: 5.89], [G ACC: %100.00], [G LOSS: 0.58]\n",
      "E: 640, [D ACC: %100.00], [D LOSS: 7.22], [G ACC: %100.00], [G LOSS: 0.48]\n",
      "E: 641, [D ACC: %100.00], [D LOSS: 2.92], [G ACC: %100.00], [G LOSS: 0.31]\n",
      "E: 642, [D ACC: %100.00], [D LOSS: 12.85], [G ACC: %100.00], [G LOSS: 0.66]\n",
      "E: 643, [D ACC: %100.00], [D LOSS: 8.00], [G ACC: %100.00], [G LOSS: 0.38]\n",
      "E: 644, [D ACC: %100.00], [D LOSS: 8.25], [G ACC: %100.00], [G LOSS: 0.40]\n",
      "E: 645, [D ACC: %100.00], [D LOSS: 3.68], [G ACC: %100.00], [G LOSS: 0.40]\n",
      "E: 646, [D ACC: %98.44], [D LOSS: 11.33], [G ACC: %100.00], [G LOSS: 1.06]\n",
      "E: 647, [D ACC: %100.00], [D LOSS: 5.83], [G ACC: %100.00], [G LOSS: 0.63]\n",
      "E: 648, [D ACC: %100.00], [D LOSS: 3.41], [G ACC: %100.00], [G LOSS: 0.68]\n",
      "E: 649, [D ACC: %100.00], [D LOSS: 8.94], [G ACC: %100.00], [G LOSS: 0.93]\n",
      "E: 650, [D ACC: %100.00], [D LOSS: 2.29], [G ACC: %100.00], [G LOSS: 0.48]\n",
      "E: 651, [D ACC: %100.00], [D LOSS: 4.96], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 652, [D ACC: %100.00], [D LOSS: 8.90], [G ACC: %100.00], [G LOSS: 0.75]\n",
      "E: 653, [D ACC: %100.00], [D LOSS: 3.46], [G ACC: %100.00], [G LOSS: 0.42]\n",
      "E: 654, [D ACC: %95.31], [D LOSS: 8.03], [G ACC: %100.00], [G LOSS: 0.43]\n",
      "E: 655, [D ACC: %98.44], [D LOSS: 6.31], [G ACC: %100.00], [G LOSS: 0.53]\n",
      "E: 656, [D ACC: %100.00], [D LOSS: 7.61], [G ACC: %100.00], [G LOSS: 0.94]\n",
      "E: 657, [D ACC: %100.00], [D LOSS: 6.77], [G ACC: %100.00], [G LOSS: 0.43]\n",
      "E: 658, [D ACC: %100.00], [D LOSS: 4.88], [G ACC: %100.00], [G LOSS: 0.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 659, [D ACC: %100.00], [D LOSS: 5.47], [G ACC: %100.00], [G LOSS: 0.51]\n",
      "E: 660, [D ACC: %98.44], [D LOSS: 6.52], [G ACC: %100.00], [G LOSS: 0.68]\n",
      "E: 661, [D ACC: %98.44], [D LOSS: 6.33], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 662, [D ACC: %100.00], [D LOSS: 5.42], [G ACC: %100.00], [G LOSS: 0.55]\n",
      "E: 663, [D ACC: %100.00], [D LOSS: 6.89], [G ACC: %100.00], [G LOSS: 0.52]\n",
      "E: 664, [D ACC: %98.44], [D LOSS: 5.76], [G ACC: %100.00], [G LOSS: 0.74]\n",
      "E: 665, [D ACC: %100.00], [D LOSS: 4.76], [G ACC: %100.00], [G LOSS: 0.76]\n",
      "E: 666, [D ACC: %100.00], [D LOSS: 4.46], [G ACC: %100.00], [G LOSS: 0.91]\n",
      "E: 667, [D ACC: %100.00], [D LOSS: 4.71], [G ACC: %100.00], [G LOSS: 0.76]\n",
      "E: 668, [D ACC: %98.44], [D LOSS: 8.79], [G ACC: %100.00], [G LOSS: 0.87]\n",
      "E: 669, [D ACC: %100.00], [D LOSS: 19.35], [G ACC: %100.00], [G LOSS: 1.51]\n",
      "E: 670, [D ACC: %100.00], [D LOSS: 6.68], [G ACC: %100.00], [G LOSS: 1.29]\n",
      "E: 671, [D ACC: %96.88], [D LOSS: 9.78], [G ACC: %100.00], [G LOSS: 2.43]\n",
      "E: 672, [D ACC: %100.00], [D LOSS: 9.81], [G ACC: %100.00], [G LOSS: 1.73]\n",
      "E: 673, [D ACC: %100.00], [D LOSS: 4.36], [G ACC: %100.00], [G LOSS: 1.77]\n",
      "E: 674, [D ACC: %96.88], [D LOSS: 6.50], [G ACC: %100.00], [G LOSS: 1.40]\n",
      "E: 675, [D ACC: %98.44], [D LOSS: 9.53], [G ACC: %100.00], [G LOSS: 1.39]\n",
      "E: 676, [D ACC: %100.00], [D LOSS: 3.61], [G ACC: %100.00], [G LOSS: 2.00]\n",
      "E: 677, [D ACC: %98.44], [D LOSS: 6.31], [G ACC: %100.00], [G LOSS: 1.36]\n",
      "E: 678, [D ACC: %100.00], [D LOSS: 3.46], [G ACC: %100.00], [G LOSS: 2.38]\n",
      "E: 679, [D ACC: %100.00], [D LOSS: 6.12], [G ACC: %100.00], [G LOSS: 3.47]\n",
      "E: 680, [D ACC: %100.00], [D LOSS: 5.63], [G ACC: %100.00], [G LOSS: 4.52]\n",
      "E: 681, [D ACC: %100.00], [D LOSS: 10.65], [G ACC: %100.00], [G LOSS: 2.95]\n",
      "E: 682, [D ACC: %100.00], [D LOSS: 4.60], [G ACC: %100.00], [G LOSS: 3.99]\n",
      "E: 683, [D ACC: %100.00], [D LOSS: 3.57], [G ACC: %100.00], [G LOSS: 4.63]\n",
      "E: 684, [D ACC: %100.00], [D LOSS: 3.22], [G ACC: %100.00], [G LOSS: 1.15]\n",
      "E: 685, [D ACC: %100.00], [D LOSS: 8.88], [G ACC: %100.00], [G LOSS: 1.75]\n",
      "E: 686, [D ACC: %100.00], [D LOSS: 8.42], [G ACC: %100.00], [G LOSS: 2.52]\n",
      "E: 687, [D ACC: %100.00], [D LOSS: 4.55], [G ACC: %100.00], [G LOSS: 1.80]\n",
      "E: 688, [D ACC: %98.44], [D LOSS: 5.65], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 689, [D ACC: %100.00], [D LOSS: 2.91], [G ACC: %100.00], [G LOSS: 0.47]\n",
      "E: 690, [D ACC: %98.44], [D LOSS: 4.81], [G ACC: %100.00], [G LOSS: 0.41]\n",
      "E: 691, [D ACC: %98.44], [D LOSS: 5.10], [G ACC: %100.00], [G LOSS: 0.66]\n",
      "E: 692, [D ACC: %100.00], [D LOSS: 3.74], [G ACC: %100.00], [G LOSS: 0.39]\n",
      "E: 693, [D ACC: %100.00], [D LOSS: 7.81], [G ACC: %100.00], [G LOSS: 0.52]\n",
      "E: 694, [D ACC: %100.00], [D LOSS: 4.49], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 695, [D ACC: %100.00], [D LOSS: 4.92], [G ACC: %100.00], [G LOSS: 0.44]\n",
      "E: 696, [D ACC: %100.00], [D LOSS: 2.95], [G ACC: %100.00], [G LOSS: 0.39]\n",
      "E: 697, [D ACC: %100.00], [D LOSS: 3.78], [G ACC: %100.00], [G LOSS: 0.42]\n",
      "E: 698, [D ACC: %98.44], [D LOSS: 4.67], [G ACC: %100.00], [G LOSS: 0.36]\n",
      "E: 699, [D ACC: %100.00], [D LOSS: 3.77], [G ACC: %100.00], [G LOSS: 0.40]\n",
      "E: 700, [D ACC: %100.00], [D LOSS: 4.52], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 701, [D ACC: %98.44], [D LOSS: 5.20], [G ACC: %100.00], [G LOSS: 0.43]\n",
      "E: 702, [D ACC: %100.00], [D LOSS: 7.79], [G ACC: %100.00], [G LOSS: 0.20]\n",
      "E: 703, [D ACC: %100.00], [D LOSS: 7.08], [G ACC: %100.00], [G LOSS: 0.25]\n",
      "E: 704, [D ACC: %100.00], [D LOSS: 2.67], [G ACC: %100.00], [G LOSS: 0.22]\n",
      "E: 705, [D ACC: %100.00], [D LOSS: 2.00], [G ACC: %100.00], [G LOSS: 0.16]\n",
      "E: 706, [D ACC: %100.00], [D LOSS: 2.39], [G ACC: %100.00], [G LOSS: 0.19]\n",
      "E: 707, [D ACC: %96.88], [D LOSS: 5.09], [G ACC: %100.00], [G LOSS: 0.16]\n",
      "E: 708, [D ACC: %100.00], [D LOSS: 2.53], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 709, [D ACC: %100.00], [D LOSS: 2.07], [G ACC: %100.00], [G LOSS: 0.14]\n",
      "E: 710, [D ACC: %100.00], [D LOSS: 4.52], [G ACC: %100.00], [G LOSS: 0.18]\n",
      "E: 711, [D ACC: %100.00], [D LOSS: 6.42], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 712, [D ACC: %100.00], [D LOSS: 2.06], [G ACC: %100.00], [G LOSS: 0.17]\n",
      "E: 713, [D ACC: %98.44], [D LOSS: 5.59], [G ACC: %100.00], [G LOSS: 0.25]\n",
      "E: 714, [D ACC: %96.88], [D LOSS: 7.96], [G ACC: %100.00], [G LOSS: 0.50]\n",
      "E: 715, [D ACC: %100.00], [D LOSS: 6.29], [G ACC: %100.00], [G LOSS: 0.27]\n",
      "E: 716, [D ACC: %96.88], [D LOSS: 4.88], [G ACC: %100.00], [G LOSS: 0.33]\n",
      "E: 717, [D ACC: %100.00], [D LOSS: 8.16], [G ACC: %100.00], [G LOSS: 0.22]\n",
      "E: 718, [D ACC: %100.00], [D LOSS: 3.70], [G ACC: %100.00], [G LOSS: 0.30]\n",
      "E: 719, [D ACC: %100.00], [D LOSS: 5.02], [G ACC: %100.00], [G LOSS: 0.41]\n",
      "E: 720, [D ACC: %100.00], [D LOSS: 2.26], [G ACC: %100.00], [G LOSS: 0.38]\n",
      "E: 721, [D ACC: %100.00], [D LOSS: 2.53], [G ACC: %100.00], [G LOSS: 0.60]\n",
      "E: 722, [D ACC: %100.00], [D LOSS: 5.77], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 723, [D ACC: %100.00], [D LOSS: 5.58], [G ACC: %100.00], [G LOSS: 0.58]\n",
      "E: 724, [D ACC: %100.00], [D LOSS: 5.74], [G ACC: %100.00], [G LOSS: 0.91]\n",
      "E: 725, [D ACC: %100.00], [D LOSS: 11.56], [G ACC: %100.00], [G LOSS: 0.75]\n",
      "E: 726, [D ACC: %96.88], [D LOSS: 4.16], [G ACC: %100.00], [G LOSS: 1.53]\n",
      "E: 727, [D ACC: %100.00], [D LOSS: 3.38], [G ACC: %100.00], [G LOSS: 1.46]\n",
      "E: 728, [D ACC: %100.00], [D LOSS: 6.43], [G ACC: %100.00], [G LOSS: 1.15]\n",
      "E: 729, [D ACC: %100.00], [D LOSS: 4.48], [G ACC: %100.00], [G LOSS: 1.39]\n",
      "E: 730, [D ACC: %100.00], [D LOSS: 3.12], [G ACC: %100.00], [G LOSS: 1.83]\n",
      "E: 731, [D ACC: %100.00], [D LOSS: 2.57], [G ACC: %100.00], [G LOSS: 3.15]\n",
      "E: 732, [D ACC: %100.00], [D LOSS: 6.81], [G ACC: %100.00], [G LOSS: 3.55]\n",
      "E: 733, [D ACC: %100.00], [D LOSS: 4.48], [G ACC: %100.00], [G LOSS: 3.93]\n",
      "E: 734, [D ACC: %100.00], [D LOSS: 3.07], [G ACC: %100.00], [G LOSS: 3.77]\n",
      "E: 735, [D ACC: %98.44], [D LOSS: 9.26], [G ACC: %100.00], [G LOSS: 3.55]\n",
      "E: 736, [D ACC: %100.00], [D LOSS: 1.96], [G ACC: %96.88], [G LOSS: 5.79]\n",
      "E: 737, [D ACC: %100.00], [D LOSS: 4.31], [G ACC: %100.00], [G LOSS: 4.82]\n",
      "E: 738, [D ACC: %98.44], [D LOSS: 10.00], [G ACC: %100.00], [G LOSS: 4.94]\n",
      "E: 739, [D ACC: %100.00], [D LOSS: 8.47], [G ACC: %96.88], [G LOSS: 19.25]\n",
      "E: 740, [D ACC: %98.44], [D LOSS: 4.59], [G ACC: %84.38], [G LOSS: 39.70]\n",
      "E: 741, [D ACC: %92.19], [D LOSS: 38.37], [G ACC: %0.00], [G LOSS: 419.34]\n",
      "E: 742, [D ACC: %98.44], [D LOSS: 12.20], [G ACC: %6.25], [G LOSS: 466.17]\n",
      "E: 743, [D ACC: %100.00], [D LOSS: 7.27], [G ACC: %18.75], [G LOSS: 243.83]\n",
      "E: 744, [D ACC: %89.06], [D LOSS: 31.81], [G ACC: %81.25], [G LOSS: 39.73]\n",
      "E: 745, [D ACC: %100.00], [D LOSS: 5.05], [G ACC: %90.62], [G LOSS: 22.95]\n",
      "E: 746, [D ACC: %100.00], [D LOSS: 6.32], [G ACC: %96.88], [G LOSS: 15.05]\n",
      "E: 747, [D ACC: %98.44], [D LOSS: 9.93], [G ACC: %90.62], [G LOSS: 28.88]\n",
      "E: 748, [D ACC: %96.88], [D LOSS: 13.42], [G ACC: %84.38], [G LOSS: 35.78]\n",
      "E: 749, [D ACC: %100.00], [D LOSS: 5.11], [G ACC: %46.88], [G LOSS: 86.61]\n",
      "E: 750, [D ACC: %96.88], [D LOSS: 7.60], [G ACC: %37.50], [G LOSS: 135.68]\n",
      "E: 751, [D ACC: %89.06], [D LOSS: 22.18], [G ACC: %15.62], [G LOSS: 224.43]\n",
      "E: 752, [D ACC: %75.00], [D LOSS: 50.64], [G ACC: %3.12], [G LOSS: 361.64]\n",
      "E: 753, [D ACC: %100.00], [D LOSS: 6.89], [G ACC: %3.12], [G LOSS: 428.15]\n",
      "E: 754, [D ACC: %98.44], [D LOSS: 9.81], [G ACC: %0.00], [G LOSS: 384.42]\n",
      "E: 755, [D ACC: %93.75], [D LOSS: 15.92], [G ACC: %0.00], [G LOSS: 405.92]\n",
      "E: 756, [D ACC: %90.62], [D LOSS: 21.35], [G ACC: %0.00], [G LOSS: 513.42]\n",
      "E: 757, [D ACC: %96.88], [D LOSS: 10.80], [G ACC: %0.00], [G LOSS: 451.28]\n",
      "E: 758, [D ACC: %98.44], [D LOSS: 10.53], [G ACC: %3.12], [G LOSS: 356.35]\n",
      "E: 759, [D ACC: %96.88], [D LOSS: 9.90], [G ACC: %15.62], [G LOSS: 209.23]\n",
      "E: 760, [D ACC: %98.44], [D LOSS: 8.83], [G ACC: %25.00], [G LOSS: 221.26]\n",
      "E: 761, [D ACC: %100.00], [D LOSS: 6.03], [G ACC: %21.88], [G LOSS: 182.48]\n",
      "E: 762, [D ACC: %100.00], [D LOSS: 5.15], [G ACC: %40.62], [G LOSS: 193.35]\n",
      "E: 763, [D ACC: %100.00], [D LOSS: 8.61], [G ACC: %46.88], [G LOSS: 147.07]\n",
      "E: 764, [D ACC: %100.00], [D LOSS: 4.10], [G ACC: %62.50], [G LOSS: 95.26]\n",
      "E: 765, [D ACC: %100.00], [D LOSS: 6.46], [G ACC: %71.88], [G LOSS: 52.84]\n",
      "E: 766, [D ACC: %98.44], [D LOSS: 9.55], [G ACC: %78.12], [G LOSS: 38.16]\n",
      "E: 767, [D ACC: %96.88], [D LOSS: 13.66], [G ACC: %53.12], [G LOSS: 92.53]\n",
      "E: 768, [D ACC: %95.31], [D LOSS: 19.64], [G ACC: %71.88], [G LOSS: 78.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 769, [D ACC: %95.31], [D LOSS: 11.60], [G ACC: %62.50], [G LOSS: 101.55]\n",
      "E: 770, [D ACC: %93.75], [D LOSS: 12.10], [G ACC: %65.62], [G LOSS: 83.69]\n",
      "E: 771, [D ACC: %90.62], [D LOSS: 36.60], [G ACC: %50.00], [G LOSS: 105.27]\n",
      "E: 772, [D ACC: %95.31], [D LOSS: 21.98], [G ACC: %56.25], [G LOSS: 123.18]\n",
      "E: 773, [D ACC: %98.44], [D LOSS: 7.81], [G ACC: %40.62], [G LOSS: 146.17]\n",
      "E: 774, [D ACC: %95.31], [D LOSS: 10.68], [G ACC: %46.88], [G LOSS: 117.86]\n",
      "E: 775, [D ACC: %98.44], [D LOSS: 12.91], [G ACC: %62.50], [G LOSS: 71.61]\n",
      "E: 776, [D ACC: %92.19], [D LOSS: 19.77], [G ACC: %78.12], [G LOSS: 47.49]\n",
      "E: 777, [D ACC: %92.19], [D LOSS: 17.77], [G ACC: %87.50], [G LOSS: 33.34]\n",
      "E: 778, [D ACC: %95.31], [D LOSS: 18.85], [G ACC: %93.75], [G LOSS: 29.70]\n",
      "E: 779, [D ACC: %95.31], [D LOSS: 17.00], [G ACC: %81.25], [G LOSS: 39.31]\n",
      "E: 780, [D ACC: %100.00], [D LOSS: 5.61], [G ACC: %93.75], [G LOSS: 26.33]\n",
      "E: 781, [D ACC: %96.88], [D LOSS: 11.66], [G ACC: %87.50], [G LOSS: 25.25]\n",
      "E: 782, [D ACC: %93.75], [D LOSS: 29.67], [G ACC: %84.38], [G LOSS: 39.44]\n",
      "E: 783, [D ACC: %95.31], [D LOSS: 22.70], [G ACC: %78.12], [G LOSS: 33.07]\n",
      "E: 784, [D ACC: %90.62], [D LOSS: 20.35], [G ACC: %71.88], [G LOSS: 58.01]\n",
      "E: 785, [D ACC: %84.38], [D LOSS: 23.46], [G ACC: %65.62], [G LOSS: 71.23]\n",
      "E: 786, [D ACC: %95.31], [D LOSS: 17.59], [G ACC: %59.38], [G LOSS: 75.37]\n",
      "E: 787, [D ACC: %93.75], [D LOSS: 21.57], [G ACC: %62.50], [G LOSS: 80.24]\n",
      "E: 788, [D ACC: %98.44], [D LOSS: 9.47], [G ACC: %43.75], [G LOSS: 114.29]\n",
      "E: 789, [D ACC: %98.44], [D LOSS: 9.38], [G ACC: %56.25], [G LOSS: 93.62]\n",
      "E: 790, [D ACC: %90.62], [D LOSS: 51.22], [G ACC: %53.12], [G LOSS: 100.48]\n",
      "E: 791, [D ACC: %98.44], [D LOSS: 7.22], [G ACC: %62.50], [G LOSS: 74.88]\n",
      "E: 792, [D ACC: %95.31], [D LOSS: 40.58], [G ACC: %65.62], [G LOSS: 70.74]\n",
      "E: 793, [D ACC: %100.00], [D LOSS: 4.15], [G ACC: %62.50], [G LOSS: 67.74]\n",
      "E: 794, [D ACC: %96.88], [D LOSS: 12.12], [G ACC: %71.88], [G LOSS: 62.95]\n",
      "E: 795, [D ACC: %100.00], [D LOSS: 5.58], [G ACC: %84.38], [G LOSS: 55.46]\n",
      "E: 796, [D ACC: %100.00], [D LOSS: 5.91], [G ACC: %100.00], [G LOSS: 2.18]\n",
      "E: 797, [D ACC: %100.00], [D LOSS: 4.00], [G ACC: %100.00], [G LOSS: 0.55]\n",
      "E: 798, [D ACC: %100.00], [D LOSS: 7.51], [G ACC: %100.00], [G LOSS: 1.12]\n",
      "E: 799, [D ACC: %100.00], [D LOSS: 4.50], [G ACC: %100.00], [G LOSS: 1.67]\n",
      "E: 800, [D ACC: %98.44], [D LOSS: 3.00], [G ACC: %100.00], [G LOSS: 0.36]\n",
      "E: 801, [D ACC: %100.00], [D LOSS: 3.22], [G ACC: %100.00], [G LOSS: 0.78]\n",
      "E: 802, [D ACC: %100.00], [D LOSS: 2.49], [G ACC: %100.00], [G LOSS: 0.35]\n",
      "E: 803, [D ACC: %100.00], [D LOSS: 2.84], [G ACC: %100.00], [G LOSS: 1.70]\n",
      "E: 804, [D ACC: %98.44], [D LOSS: 15.34], [G ACC: %100.00], [G LOSS: 0.57]\n",
      "E: 805, [D ACC: %100.00], [D LOSS: 4.21], [G ACC: %100.00], [G LOSS: 0.16]\n",
      "E: 806, [D ACC: %98.44], [D LOSS: 4.91], [G ACC: %100.00], [G LOSS: 0.85]\n",
      "E: 807, [D ACC: %100.00], [D LOSS: 10.33], [G ACC: %100.00], [G LOSS: 0.55]\n",
      "E: 808, [D ACC: %100.00], [D LOSS: 2.79], [G ACC: %100.00], [G LOSS: 1.29]\n",
      "E: 809, [D ACC: %98.44], [D LOSS: 7.20], [G ACC: %100.00], [G LOSS: 0.32]\n",
      "E: 810, [D ACC: %100.00], [D LOSS: 7.15], [G ACC: %100.00], [G LOSS: 0.81]\n",
      "E: 811, [D ACC: %98.44], [D LOSS: 10.01], [G ACC: %96.88], [G LOSS: 6.40]\n",
      "E: 812, [D ACC: %100.00], [D LOSS: 2.20], [G ACC: %96.88], [G LOSS: 3.96]\n",
      "E: 813, [D ACC: %100.00], [D LOSS: 7.38], [G ACC: %100.00], [G LOSS: 1.19]\n",
      "E: 814, [D ACC: %100.00], [D LOSS: 2.95], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 815, [D ACC: %98.44], [D LOSS: 9.49], [G ACC: %100.00], [G LOSS: 1.04]\n",
      "E: 816, [D ACC: %100.00], [D LOSS: 6.37], [G ACC: %100.00], [G LOSS: 1.17]\n",
      "E: 817, [D ACC: %100.00], [D LOSS: 10.44], [G ACC: %100.00], [G LOSS: 0.71]\n",
      "E: 818, [D ACC: %100.00], [D LOSS: 3.32], [G ACC: %100.00], [G LOSS: 0.87]\n",
      "E: 819, [D ACC: %100.00], [D LOSS: 3.89], [G ACC: %100.00], [G LOSS: 2.36]\n",
      "E: 820, [D ACC: %100.00], [D LOSS: 5.31], [G ACC: %100.00], [G LOSS: 1.23]\n",
      "E: 821, [D ACC: %98.44], [D LOSS: 3.87], [G ACC: %100.00], [G LOSS: 2.48]\n",
      "E: 822, [D ACC: %100.00], [D LOSS: 3.92], [G ACC: %100.00], [G LOSS: 1.05]\n",
      "E: 823, [D ACC: %100.00], [D LOSS: 10.54], [G ACC: %100.00], [G LOSS: 0.91]\n",
      "E: 824, [D ACC: %98.44], [D LOSS: 11.83], [G ACC: %100.00], [G LOSS: 1.82]\n",
      "E: 825, [D ACC: %100.00], [D LOSS: 3.73], [G ACC: %100.00], [G LOSS: 0.89]\n",
      "E: 826, [D ACC: %100.00], [D LOSS: 3.60], [G ACC: %100.00], [G LOSS: 3.22]\n",
      "E: 827, [D ACC: %100.00], [D LOSS: 2.69], [G ACC: %100.00], [G LOSS: 3.80]\n",
      "E: 828, [D ACC: %100.00], [D LOSS: 5.46], [G ACC: %100.00], [G LOSS: 2.53]\n",
      "E: 829, [D ACC: %98.44], [D LOSS: 4.24], [G ACC: %100.00], [G LOSS: 2.86]\n",
      "E: 830, [D ACC: %98.44], [D LOSS: 4.51], [G ACC: %100.00], [G LOSS: 2.01]\n",
      "E: 831, [D ACC: %100.00], [D LOSS: 3.14], [G ACC: %100.00], [G LOSS: 3.58]\n",
      "E: 832, [D ACC: %100.00], [D LOSS: 8.66], [G ACC: %96.88], [G LOSS: 12.92]\n",
      "E: 833, [D ACC: %100.00], [D LOSS: 4.66], [G ACC: %100.00], [G LOSS: 7.11]\n",
      "E: 834, [D ACC: %100.00], [D LOSS: 8.02], [G ACC: %87.50], [G LOSS: 35.03]\n",
      "E: 835, [D ACC: %100.00], [D LOSS: 4.23], [G ACC: %87.50], [G LOSS: 28.39]\n",
      "E: 836, [D ACC: %100.00], [D LOSS: 3.03], [G ACC: %90.62], [G LOSS: 26.60]\n",
      "E: 837, [D ACC: %100.00], [D LOSS: 2.72], [G ACC: %100.00], [G LOSS: 8.11]\n",
      "E: 838, [D ACC: %100.00], [D LOSS: 12.06], [G ACC: %100.00], [G LOSS: 2.68]\n",
      "E: 839, [D ACC: %100.00], [D LOSS: 1.82], [G ACC: %96.88], [G LOSS: 4.28]\n",
      "E: 840, [D ACC: %100.00], [D LOSS: 1.68], [G ACC: %100.00], [G LOSS: 0.98]\n",
      "E: 841, [D ACC: %100.00], [D LOSS: 7.73], [G ACC: %100.00], [G LOSS: 1.11]\n",
      "E: 842, [D ACC: %100.00], [D LOSS: 2.51], [G ACC: %100.00], [G LOSS: 0.60]\n",
      "E: 843, [D ACC: %100.00], [D LOSS: 2.04], [G ACC: %100.00], [G LOSS: 0.95]\n",
      "E: 844, [D ACC: %100.00], [D LOSS: 3.27], [G ACC: %100.00], [G LOSS: 0.75]\n",
      "E: 845, [D ACC: %100.00], [D LOSS: 3.29], [G ACC: %100.00], [G LOSS: 0.40]\n",
      "E: 846, [D ACC: %100.00], [D LOSS: 1.66], [G ACC: %100.00], [G LOSS: 1.42]\n",
      "E: 847, [D ACC: %98.44], [D LOSS: 4.02], [G ACC: %100.00], [G LOSS: 0.62]\n",
      "E: 848, [D ACC: %100.00], [D LOSS: 1.76], [G ACC: %100.00], [G LOSS: 0.57]\n",
      "E: 849, [D ACC: %100.00], [D LOSS: 7.97], [G ACC: %100.00], [G LOSS: 0.44]\n",
      "E: 850, [D ACC: %100.00], [D LOSS: 2.73], [G ACC: %100.00], [G LOSS: 0.63]\n",
      "E: 851, [D ACC: %98.44], [D LOSS: 5.21], [G ACC: %100.00], [G LOSS: 1.34]\n",
      "E: 852, [D ACC: %100.00], [D LOSS: 2.28], [G ACC: %100.00], [G LOSS: 0.36]\n",
      "E: 853, [D ACC: %98.44], [D LOSS: 3.92], [G ACC: %100.00], [G LOSS: 1.44]\n",
      "E: 854, [D ACC: %98.44], [D LOSS: 7.88], [G ACC: %100.00], [G LOSS: 0.83]\n",
      "E: 855, [D ACC: %100.00], [D LOSS: 3.22], [G ACC: %100.00], [G LOSS: 1.81]\n",
      "E: 856, [D ACC: %100.00], [D LOSS: 6.51], [G ACC: %100.00], [G LOSS: 0.57]\n",
      "E: 857, [D ACC: %98.44], [D LOSS: 3.67], [G ACC: %100.00], [G LOSS: 0.74]\n",
      "E: 858, [D ACC: %96.88], [D LOSS: 6.16], [G ACC: %100.00], [G LOSS: 0.54]\n",
      "E: 859, [D ACC: %100.00], [D LOSS: 2.03], [G ACC: %100.00], [G LOSS: 1.47]\n",
      "E: 860, [D ACC: %98.44], [D LOSS: 4.59], [G ACC: %100.00], [G LOSS: 0.69]\n",
      "E: 861, [D ACC: %100.00], [D LOSS: 4.37], [G ACC: %100.00], [G LOSS: 0.67]\n",
      "E: 862, [D ACC: %100.00], [D LOSS: 5.17], [G ACC: %100.00], [G LOSS: 0.76]\n",
      "E: 863, [D ACC: %100.00], [D LOSS: 3.01], [G ACC: %100.00], [G LOSS: 0.54]\n",
      "E: 864, [D ACC: %100.00], [D LOSS: 9.27], [G ACC: %100.00], [G LOSS: 0.55]\n",
      "E: 865, [D ACC: %100.00], [D LOSS: 3.04], [G ACC: %100.00], [G LOSS: 0.73]\n",
      "E: 866, [D ACC: %100.00], [D LOSS: 11.78], [G ACC: %100.00], [G LOSS: 0.73]\n",
      "E: 867, [D ACC: %98.44], [D LOSS: 3.48], [G ACC: %100.00], [G LOSS: 0.78]\n",
      "E: 868, [D ACC: %100.00], [D LOSS: 7.50], [G ACC: %100.00], [G LOSS: 0.70]\n",
      "E: 869, [D ACC: %100.00], [D LOSS: 8.57], [G ACC: %100.00], [G LOSS: 1.31]\n",
      "E: 870, [D ACC: %100.00], [D LOSS: 6.61], [G ACC: %100.00], [G LOSS: 1.10]\n",
      "E: 871, [D ACC: %100.00], [D LOSS: 2.91], [G ACC: %100.00], [G LOSS: 2.15]\n",
      "E: 872, [D ACC: %100.00], [D LOSS: 2.72], [G ACC: %100.00], [G LOSS: 2.88]\n",
      "E: 873, [D ACC: %100.00], [D LOSS: 5.64], [G ACC: %100.00], [G LOSS: 3.07]\n",
      "E: 874, [D ACC: %100.00], [D LOSS: 9.53], [G ACC: %100.00], [G LOSS: 2.46]\n",
      "E: 875, [D ACC: %100.00], [D LOSS: 3.78], [G ACC: %100.00], [G LOSS: 5.52]\n",
      "E: 876, [D ACC: %100.00], [D LOSS: 4.57], [G ACC: %96.88], [G LOSS: 12.22]\n",
      "E: 877, [D ACC: %100.00], [D LOSS: 5.12], [G ACC: %100.00], [G LOSS: 8.14]\n",
      "E: 878, [D ACC: %100.00], [D LOSS: 2.79], [G ACC: %100.00], [G LOSS: 7.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 879, [D ACC: %100.00], [D LOSS: 11.02], [G ACC: %100.00], [G LOSS: 6.35]\n",
      "E: 880, [D ACC: %100.00], [D LOSS: 1.59], [G ACC: %100.00], [G LOSS: 7.58]\n",
      "E: 881, [D ACC: %100.00], [D LOSS: 5.94], [G ACC: %100.00], [G LOSS: 6.60]\n",
      "E: 882, [D ACC: %98.44], [D LOSS: 4.17], [G ACC: %96.88], [G LOSS: 13.45]\n",
      "E: 883, [D ACC: %100.00], [D LOSS: 4.11], [G ACC: %93.75], [G LOSS: 36.76]\n",
      "E: 884, [D ACC: %100.00], [D LOSS: 9.50], [G ACC: %12.50], [G LOSS: 140.94]\n",
      "E: 885, [D ACC: %100.00], [D LOSS: 2.33], [G ACC: %0.00], [G LOSS: 382.83]\n",
      "E: 886, [D ACC: %100.00], [D LOSS: 3.43], [G ACC: %3.12], [G LOSS: 284.34]\n",
      "E: 887, [D ACC: %82.81], [D LOSS: 36.89], [G ACC: %0.00], [G LOSS: 442.73]\n",
      "E: 888, [D ACC: %100.00], [D LOSS: 3.81], [G ACC: %0.00], [G LOSS: 534.81]\n",
      "E: 889, [D ACC: %98.44], [D LOSS: 4.27], [G ACC: %0.00], [G LOSS: 496.46]\n",
      "E: 890, [D ACC: %100.00], [D LOSS: 7.81], [G ACC: %0.00], [G LOSS: 432.62]\n",
      "E: 891, [D ACC: %100.00], [D LOSS: 3.76], [G ACC: %3.12], [G LOSS: 367.22]\n",
      "E: 892, [D ACC: %100.00], [D LOSS: 5.84], [G ACC: %21.88], [G LOSS: 228.33]\n",
      "E: 893, [D ACC: %98.44], [D LOSS: 3.86], [G ACC: %31.25], [G LOSS: 174.99]\n",
      "E: 894, [D ACC: %100.00], [D LOSS: 4.43], [G ACC: %50.00], [G LOSS: 98.16]\n",
      "E: 895, [D ACC: %100.00], [D LOSS: 4.13], [G ACC: %56.25], [G LOSS: 88.20]\n",
      "E: 896, [D ACC: %100.00], [D LOSS: 4.77], [G ACC: %93.75], [G LOSS: 20.84]\n",
      "E: 897, [D ACC: %100.00], [D LOSS: 9.30], [G ACC: %93.75], [G LOSS: 11.13]\n",
      "E: 898, [D ACC: %100.00], [D LOSS: 7.19], [G ACC: %96.88], [G LOSS: 7.99]\n",
      "E: 899, [D ACC: %96.88], [D LOSS: 8.75], [G ACC: %100.00], [G LOSS: 4.86]\n",
      "E: 900, [D ACC: %100.00], [D LOSS: 5.23], [G ACC: %96.88], [G LOSS: 7.13]\n",
      "E: 901, [D ACC: %100.00], [D LOSS: 4.62], [G ACC: %100.00], [G LOSS: 3.74]\n",
      "E: 902, [D ACC: %100.00], [D LOSS: 8.16], [G ACC: %100.00], [G LOSS: 7.49]\n",
      "E: 903, [D ACC: %100.00], [D LOSS: 4.06], [G ACC: %96.88], [G LOSS: 9.16]\n",
      "E: 904, [D ACC: %100.00], [D LOSS: 4.95], [G ACC: %96.88], [G LOSS: 13.48]\n",
      "E: 905, [D ACC: %98.44], [D LOSS: 10.51], [G ACC: %100.00], [G LOSS: 4.77]\n",
      "E: 906, [D ACC: %100.00], [D LOSS: 5.44], [G ACC: %100.00], [G LOSS: 7.03]\n",
      "E: 907, [D ACC: %100.00], [D LOSS: 3.92], [G ACC: %100.00], [G LOSS: 4.94]\n",
      "E: 908, [D ACC: %100.00], [D LOSS: 8.62], [G ACC: %100.00], [G LOSS: 2.53]\n",
      "E: 909, [D ACC: %98.44], [D LOSS: 11.15], [G ACC: %100.00], [G LOSS: 5.58]\n",
      "E: 910, [D ACC: %100.00], [D LOSS: 12.86], [G ACC: %93.75], [G LOSS: 22.01]\n",
      "E: 911, [D ACC: %100.00], [D LOSS: 6.59], [G ACC: %90.62], [G LOSS: 16.91]\n",
      "E: 912, [D ACC: %98.44], [D LOSS: 13.69], [G ACC: %75.00], [G LOSS: 49.31]\n",
      "E: 913, [D ACC: %92.19], [D LOSS: 27.71], [G ACC: %46.88], [G LOSS: 98.45]\n",
      "E: 914, [D ACC: %95.31], [D LOSS: 17.74], [G ACC: %78.12], [G LOSS: 53.34]\n",
      "E: 915, [D ACC: %53.12], [D LOSS: 209.37], [G ACC: %3.12], [G LOSS: 430.32]\n",
      "E: 916, [D ACC: %100.00], [D LOSS: 3.99], [G ACC: %0.00], [G LOSS: 636.80]\n",
      "E: 917, [D ACC: %100.00], [D LOSS: 4.70], [G ACC: %0.00], [G LOSS: 638.63]\n",
      "E: 918, [D ACC: %100.00], [D LOSS: 5.57], [G ACC: %0.00], [G LOSS: 579.64]\n",
      "E: 919, [D ACC: %100.00], [D LOSS: 3.21], [G ACC: %0.00], [G LOSS: 480.53]\n",
      "E: 920, [D ACC: %98.44], [D LOSS: 30.19], [G ACC: %0.00], [G LOSS: 349.27]\n",
      "E: 921, [D ACC: %98.44], [D LOSS: 6.13], [G ACC: %6.25], [G LOSS: 246.50]\n",
      "E: 922, [D ACC: %87.50], [D LOSS: 48.31], [G ACC: %6.25], [G LOSS: 222.46]\n",
      "E: 923, [D ACC: %96.88], [D LOSS: 13.40], [G ACC: %6.25], [G LOSS: 237.78]\n",
      "E: 924, [D ACC: %100.00], [D LOSS: 6.32], [G ACC: %12.50], [G LOSS: 173.17]\n",
      "E: 925, [D ACC: %93.75], [D LOSS: 20.91], [G ACC: %40.62], [G LOSS: 152.33]\n",
      "E: 926, [D ACC: %98.44], [D LOSS: 5.59], [G ACC: %90.62], [G LOSS: 31.21]\n",
      "E: 927, [D ACC: %98.44], [D LOSS: 26.24], [G ACC: %93.75], [G LOSS: 17.75]\n",
      "E: 928, [D ACC: %100.00], [D LOSS: 4.53], [G ACC: %96.88], [G LOSS: 8.68]\n",
      "E: 929, [D ACC: %100.00], [D LOSS: 4.94], [G ACC: %100.00], [G LOSS: 5.97]\n",
      "E: 930, [D ACC: %100.00], [D LOSS: 7.28], [G ACC: %100.00], [G LOSS: 3.66]\n",
      "E: 931, [D ACC: %100.00], [D LOSS: 4.65], [G ACC: %100.00], [G LOSS: 2.78]\n",
      "E: 932, [D ACC: %100.00], [D LOSS: 7.61], [G ACC: %100.00], [G LOSS: 4.45]\n",
      "E: 933, [D ACC: %100.00], [D LOSS: 1.72], [G ACC: %100.00], [G LOSS: 1.89]\n",
      "E: 934, [D ACC: %100.00], [D LOSS: 4.46], [G ACC: %100.00], [G LOSS: 4.98]\n",
      "E: 935, [D ACC: %98.44], [D LOSS: 17.49], [G ACC: %100.00], [G LOSS: 2.99]\n",
      "E: 936, [D ACC: %100.00], [D LOSS: 5.19], [G ACC: %100.00], [G LOSS: 2.09]\n",
      "E: 937, [D ACC: %100.00], [D LOSS: 4.70], [G ACC: %100.00], [G LOSS: 2.20]\n",
      "E: 938, [D ACC: %100.00], [D LOSS: 4.75], [G ACC: %100.00], [G LOSS: 5.63]\n",
      "E: 939, [D ACC: %100.00], [D LOSS: 3.64], [G ACC: %100.00], [G LOSS: 1.15]\n",
      "E: 940, [D ACC: %98.44], [D LOSS: 15.32], [G ACC: %100.00], [G LOSS: 0.49]\n",
      "E: 941, [D ACC: %98.44], [D LOSS: 7.79], [G ACC: %100.00], [G LOSS: 0.92]\n",
      "E: 942, [D ACC: %100.00], [D LOSS: 4.02], [G ACC: %100.00], [G LOSS: 1.48]\n",
      "E: 943, [D ACC: %100.00], [D LOSS: 3.38], [G ACC: %100.00], [G LOSS: 1.52]\n",
      "E: 944, [D ACC: %98.44], [D LOSS: 3.26], [G ACC: %100.00], [G LOSS: 1.38]\n",
      "E: 945, [D ACC: %100.00], [D LOSS: 1.91], [G ACC: %100.00], [G LOSS: 3.84]\n",
      "E: 946, [D ACC: %100.00], [D LOSS: 6.01], [G ACC: %100.00], [G LOSS: 2.33]\n",
      "E: 947, [D ACC: %98.44], [D LOSS: 5.31], [G ACC: %100.00], [G LOSS: 3.50]\n",
      "E: 948, [D ACC: %95.31], [D LOSS: 16.74], [G ACC: %100.00], [G LOSS: 2.04]\n",
      "E: 949, [D ACC: %100.00], [D LOSS: 4.52], [G ACC: %100.00], [G LOSS: 5.36]\n",
      "E: 950, [D ACC: %100.00], [D LOSS: 9.74], [G ACC: %96.88], [G LOSS: 7.94]\n",
      "E: 951, [D ACC: %100.00], [D LOSS: 4.77], [G ACC: %100.00], [G LOSS: 8.30]\n",
      "E: 952, [D ACC: %100.00], [D LOSS: 4.68], [G ACC: %90.62], [G LOSS: 18.79]\n",
      "E: 953, [D ACC: %100.00], [D LOSS: 6.31], [G ACC: %84.38], [G LOSS: 31.51]\n",
      "E: 954, [D ACC: %100.00], [D LOSS: 5.89], [G ACC: %75.00], [G LOSS: 40.53]\n",
      "E: 955, [D ACC: %96.88], [D LOSS: 12.82], [G ACC: %28.12], [G LOSS: 177.27]\n",
      "E: 956, [D ACC: %98.44], [D LOSS: 7.78], [G ACC: %18.75], [G LOSS: 280.63]\n",
      "E: 957, [D ACC: %93.75], [D LOSS: 27.83], [G ACC: %6.25], [G LOSS: 234.12]\n",
      "E: 958, [D ACC: %95.31], [D LOSS: 18.14], [G ACC: %3.12], [G LOSS: 242.68]\n",
      "E: 959, [D ACC: %95.31], [D LOSS: 14.63], [G ACC: %21.88], [G LOSS: 175.53]\n",
      "E: 960, [D ACC: %95.31], [D LOSS: 14.75], [G ACC: %31.25], [G LOSS: 144.42]\n",
      "E: 961, [D ACC: %98.44], [D LOSS: 8.93], [G ACC: %25.00], [G LOSS: 182.43]\n",
      "E: 962, [D ACC: %95.31], [D LOSS: 22.33], [G ACC: %15.62], [G LOSS: 209.70]\n",
      "E: 963, [D ACC: %95.31], [D LOSS: 16.23], [G ACC: %18.75], [G LOSS: 252.47]\n",
      "E: 964, [D ACC: %95.31], [D LOSS: 13.67], [G ACC: %9.38], [G LOSS: 307.03]\n",
      "E: 965, [D ACC: %100.00], [D LOSS: 9.75], [G ACC: %15.62], [G LOSS: 269.07]\n",
      "E: 966, [D ACC: %93.75], [D LOSS: 15.57], [G ACC: %15.62], [G LOSS: 253.58]\n",
      "E: 967, [D ACC: %93.75], [D LOSS: 15.36], [G ACC: %9.38], [G LOSS: 324.95]\n",
      "E: 968, [D ACC: %98.44], [D LOSS: 12.15], [G ACC: %6.25], [G LOSS: 306.97]\n",
      "E: 969, [D ACC: %98.44], [D LOSS: 9.46], [G ACC: %25.00], [G LOSS: 245.01]\n",
      "E: 970, [D ACC: %100.00], [D LOSS: 5.40], [G ACC: %53.12], [G LOSS: 163.70]\n",
      "E: 971, [D ACC: %100.00], [D LOSS: 9.61], [G ACC: %75.00], [G LOSS: 62.47]\n",
      "E: 972, [D ACC: %93.75], [D LOSS: 16.91], [G ACC: %87.50], [G LOSS: 22.62]\n",
      "E: 973, [D ACC: %96.88], [D LOSS: 8.86], [G ACC: %90.62], [G LOSS: 22.00]\n",
      "E: 974, [D ACC: %100.00], [D LOSS: 5.22], [G ACC: %100.00], [G LOSS: 4.71]\n",
      "E: 975, [D ACC: %96.88], [D LOSS: 16.34], [G ACC: %96.88], [G LOSS: 7.90]\n",
      "E: 976, [D ACC: %98.44], [D LOSS: 8.91], [G ACC: %100.00], [G LOSS: 2.00]\n",
      "E: 977, [D ACC: %100.00], [D LOSS: 4.96], [G ACC: %100.00], [G LOSS: 5.41]\n",
      "E: 978, [D ACC: %100.00], [D LOSS: 5.30], [G ACC: %100.00], [G LOSS: 2.52]\n",
      "E: 979, [D ACC: %100.00], [D LOSS: 5.40], [G ACC: %100.00], [G LOSS: 2.71]\n",
      "E: 980, [D ACC: %100.00], [D LOSS: 7.97], [G ACC: %96.88], [G LOSS: 5.05]\n",
      "E: 981, [D ACC: %100.00], [D LOSS: 7.89], [G ACC: %100.00], [G LOSS: 9.50]\n",
      "E: 982, [D ACC: %100.00], [D LOSS: 5.11], [G ACC: %100.00], [G LOSS: 2.77]\n",
      "E: 983, [D ACC: %100.00], [D LOSS: 2.03], [G ACC: %96.88], [G LOSS: 5.61]\n",
      "E: 984, [D ACC: %98.44], [D LOSS: 6.92], [G ACC: %100.00], [G LOSS: 3.04]\n",
      "E: 985, [D ACC: %100.00], [D LOSS: 3.23], [G ACC: %96.88], [G LOSS: 8.43]\n",
      "E: 986, [D ACC: %100.00], [D LOSS: 4.02], [G ACC: %96.88], [G LOSS: 9.56]\n",
      "E: 987, [D ACC: %100.00], [D LOSS: 5.66], [G ACC: %100.00], [G LOSS: 2.75]\n",
      "E: 988, [D ACC: %100.00], [D LOSS: 4.37], [G ACC: %100.00], [G LOSS: 2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 989, [D ACC: %100.00], [D LOSS: 4.16], [G ACC: %100.00], [G LOSS: 2.35]\n",
      "E: 990, [D ACC: %100.00], [D LOSS: 9.11], [G ACC: %96.88], [G LOSS: 13.53]\n",
      "E: 991, [D ACC: %100.00], [D LOSS: 3.66], [G ACC: %96.88], [G LOSS: 10.13]\n",
      "E: 992, [D ACC: %100.00], [D LOSS: 4.57], [G ACC: %100.00], [G LOSS: 4.98]\n",
      "E: 993, [D ACC: %100.00], [D LOSS: 3.45], [G ACC: %96.88], [G LOSS: 7.70]\n",
      "E: 994, [D ACC: %100.00], [D LOSS: 2.77], [G ACC: %100.00], [G LOSS: 4.90]\n",
      "E: 995, [D ACC: %100.00], [D LOSS: 4.56], [G ACC: %100.00], [G LOSS: 2.29]\n",
      "E: 996, [D ACC: %100.00], [D LOSS: 9.84], [G ACC: %100.00], [G LOSS: 2.78]\n",
      "E: 997, [D ACC: %100.00], [D LOSS: 2.91], [G ACC: %100.00], [G LOSS: 3.69]\n",
      "E: 998, [D ACC: %100.00], [D LOSS: 2.96], [G ACC: %100.00], [G LOSS: 4.88]\n",
      "E: 999, [D ACC: %100.00], [D LOSS: 7.11], [G ACC: %100.00], [G LOSS: 7.84]\n",
      "E: 1000, [D ACC: %100.00], [D LOSS: 5.33], [G ACC: %100.00], [G LOSS: 5.03]\n",
      "E: 1001, [D ACC: %98.44], [D LOSS: 5.28], [G ACC: %100.00], [G LOSS: 2.50]\n",
      "E: 1002, [D ACC: %95.31], [D LOSS: 5.99], [G ACC: %100.00], [G LOSS: 3.84]\n",
      "E: 1003, [D ACC: %100.00], [D LOSS: 4.36], [G ACC: %100.00], [G LOSS: 2.62]\n",
      "E: 1004, [D ACC: %100.00], [D LOSS: 3.12], [G ACC: %100.00], [G LOSS: 2.61]\n",
      "E: 1005, [D ACC: %100.00], [D LOSS: 2.97], [G ACC: %100.00], [G LOSS: 2.21]\n",
      "E: 1006, [D ACC: %100.00], [D LOSS: 3.16], [G ACC: %100.00], [G LOSS: 2.40]\n",
      "E: 1007, [D ACC: %100.00], [D LOSS: 2.27], [G ACC: %100.00], [G LOSS: 3.50]\n",
      "E: 1008, [D ACC: %100.00], [D LOSS: 6.29], [G ACC: %96.88], [G LOSS: 4.59]\n",
      "E: 1009, [D ACC: %98.44], [D LOSS: 2.44], [G ACC: %100.00], [G LOSS: 2.38]\n",
      "E: 1010, [D ACC: %100.00], [D LOSS: 2.26], [G ACC: %100.00], [G LOSS: 1.70]\n",
      "E: 1011, [D ACC: %100.00], [D LOSS: 1.77], [G ACC: %100.00], [G LOSS: 1.70]\n",
      "E: 1012, [D ACC: %100.00], [D LOSS: 3.38], [G ACC: %96.88], [G LOSS: 7.42]\n",
      "E: 1013, [D ACC: %100.00], [D LOSS: 6.55], [G ACC: %96.88], [G LOSS: 10.50]\n",
      "E: 1014, [D ACC: %100.00], [D LOSS: 3.70], [G ACC: %100.00], [G LOSS: 2.98]\n",
      "E: 1015, [D ACC: %100.00], [D LOSS: 6.13], [G ACC: %100.00], [G LOSS: 4.65]\n",
      "E: 1016, [D ACC: %100.00], [D LOSS: 0.92], [G ACC: %100.00], [G LOSS: 1.75]\n",
      "E: 1017, [D ACC: %100.00], [D LOSS: 3.19], [G ACC: %100.00], [G LOSS: 2.37]\n",
      "E: 1018, [D ACC: %100.00], [D LOSS: 2.98], [G ACC: %100.00], [G LOSS: 1.20]\n",
      "E: 1019, [D ACC: %100.00], [D LOSS: 1.51], [G ACC: %100.00], [G LOSS: 2.12]\n",
      "E: 1020, [D ACC: %100.00], [D LOSS: 2.50], [G ACC: %100.00], [G LOSS: 1.21]\n",
      "E: 1021, [D ACC: %100.00], [D LOSS: 6.30], [G ACC: %100.00], [G LOSS: 2.58]\n",
      "E: 1022, [D ACC: %98.44], [D LOSS: 4.99], [G ACC: %100.00], [G LOSS: 0.77]\n",
      "E: 1023, [D ACC: %100.00], [D LOSS: 3.43], [G ACC: %100.00], [G LOSS: 1.50]\n",
      "E: 1024, [D ACC: %100.00], [D LOSS: 3.73], [G ACC: %100.00], [G LOSS: 1.39]\n",
      "E: 1025, [D ACC: %100.00], [D LOSS: 1.57], [G ACC: %100.00], [G LOSS: 0.95]\n",
      "E: 1026, [D ACC: %100.00], [D LOSS: 2.87], [G ACC: %100.00], [G LOSS: 2.68]\n",
      "E: 1027, [D ACC: %100.00], [D LOSS: 1.25], [G ACC: %100.00], [G LOSS: 1.34]\n",
      "E: 1028, [D ACC: %100.00], [D LOSS: 3.58], [G ACC: %100.00], [G LOSS: 1.82]\n",
      "E: 1029, [D ACC: %100.00], [D LOSS: 1.89], [G ACC: %100.00], [G LOSS: 1.03]\n",
      "E: 1030, [D ACC: %100.00], [D LOSS: 3.17], [G ACC: %100.00], [G LOSS: 1.17]\n",
      "E: 1031, [D ACC: %100.00], [D LOSS: 1.12], [G ACC: %100.00], [G LOSS: 1.09]\n",
      "E: 1032, [D ACC: %100.00], [D LOSS: 2.87], [G ACC: %100.00], [G LOSS: 0.79]\n",
      "E: 1033, [D ACC: %98.44], [D LOSS: 11.09], [G ACC: %100.00], [G LOSS: 0.98]\n",
      "E: 1034, [D ACC: %100.00], [D LOSS: 1.73], [G ACC: %100.00], [G LOSS: 0.43]\n",
      "E: 1035, [D ACC: %100.00], [D LOSS: 3.54], [G ACC: %100.00], [G LOSS: 0.69]\n",
      "E: 1036, [D ACC: %100.00], [D LOSS: 2.54], [G ACC: %100.00], [G LOSS: 0.92]\n",
      "E: 1037, [D ACC: %100.00], [D LOSS: 10.30], [G ACC: %100.00], [G LOSS: 1.18]\n",
      "E: 1038, [D ACC: %100.00], [D LOSS: 1.71], [G ACC: %100.00], [G LOSS: 0.78]\n",
      "E: 1039, [D ACC: %100.00], [D LOSS: 3.38], [G ACC: %100.00], [G LOSS: 0.99]\n",
      "E: 1040, [D ACC: %100.00], [D LOSS: 4.73], [G ACC: %100.00], [G LOSS: 1.11]\n",
      "E: 1041, [D ACC: %100.00], [D LOSS: 2.54], [G ACC: %100.00], [G LOSS: 0.97]\n",
      "E: 1042, [D ACC: %98.44], [D LOSS: 4.44], [G ACC: %100.00], [G LOSS: 2.31]\n",
      "E: 1043, [D ACC: %100.00], [D LOSS: 2.09], [G ACC: %100.00], [G LOSS: 0.85]\n",
      "E: 1044, [D ACC: %100.00], [D LOSS: 4.84], [G ACC: %100.00], [G LOSS: 1.16]\n",
      "E: 1045, [D ACC: %100.00], [D LOSS: 3.74], [G ACC: %100.00], [G LOSS: 0.66]\n",
      "E: 1046, [D ACC: %100.00], [D LOSS: 1.36], [G ACC: %100.00], [G LOSS: 1.49]\n",
      "E: 1047, [D ACC: %96.88], [D LOSS: 4.37], [G ACC: %100.00], [G LOSS: 1.17]\n",
      "E: 1048, [D ACC: %100.00], [D LOSS: 2.59], [G ACC: %100.00], [G LOSS: 4.06]\n",
      "E: 1049, [D ACC: %100.00], [D LOSS: 4.91], [G ACC: %100.00], [G LOSS: 1.83]\n",
      "E: 1050, [D ACC: %100.00], [D LOSS: 5.89], [G ACC: %100.00], [G LOSS: 1.64]\n",
      "E: 1051, [D ACC: %98.44], [D LOSS: 4.19], [G ACC: %100.00], [G LOSS: 1.78]\n",
      "E: 1052, [D ACC: %100.00], [D LOSS: 2.08], [G ACC: %100.00], [G LOSS: 1.19]\n",
      "E: 1053, [D ACC: %100.00], [D LOSS: 1.52], [G ACC: %100.00], [G LOSS: 1.44]\n",
      "E: 1054, [D ACC: %100.00], [D LOSS: 2.50], [G ACC: %100.00], [G LOSS: 1.68]\n",
      "E: 1055, [D ACC: %100.00], [D LOSS: 1.60], [G ACC: %100.00], [G LOSS: 1.65]\n",
      "E: 1056, [D ACC: %100.00], [D LOSS: 3.13], [G ACC: %100.00], [G LOSS: 1.40]\n",
      "E: 1057, [D ACC: %100.00], [D LOSS: 2.55], [G ACC: %100.00], [G LOSS: 1.29]\n",
      "E: 1058, [D ACC: %100.00], [D LOSS: 2.92], [G ACC: %100.00], [G LOSS: 0.63]\n",
      "E: 1059, [D ACC: %100.00], [D LOSS: 5.28], [G ACC: %100.00], [G LOSS: 2.25]\n",
      "E: 1060, [D ACC: %98.44], [D LOSS: 4.45], [G ACC: %100.00], [G LOSS: 2.87]\n",
      "E: 1061, [D ACC: %100.00], [D LOSS: 4.98], [G ACC: %100.00], [G LOSS: 3.06]\n",
      "E: 1062, [D ACC: %100.00], [D LOSS: 4.87], [G ACC: %100.00], [G LOSS: 1.81]\n",
      "E: 1063, [D ACC: %100.00], [D LOSS: 3.56], [G ACC: %100.00], [G LOSS: 2.94]\n",
      "E: 1064, [D ACC: %98.44], [D LOSS: 4.06], [G ACC: %100.00], [G LOSS: 0.64]\n",
      "E: 1065, [D ACC: %100.00], [D LOSS: 4.95], [G ACC: %100.00], [G LOSS: 2.92]\n",
      "E: 1066, [D ACC: %100.00], [D LOSS: 3.65], [G ACC: %100.00], [G LOSS: 3.04]\n",
      "E: 1067, [D ACC: %100.00], [D LOSS: 1.73], [G ACC: %100.00], [G LOSS: 1.32]\n",
      "E: 1068, [D ACC: %98.44], [D LOSS: 2.93], [G ACC: %100.00], [G LOSS: 1.32]\n",
      "E: 1069, [D ACC: %100.00], [D LOSS: 3.63], [G ACC: %100.00], [G LOSS: 4.27]\n",
      "E: 1070, [D ACC: %100.00], [D LOSS: 3.04], [G ACC: %100.00], [G LOSS: 1.96]\n",
      "E: 1071, [D ACC: %100.00], [D LOSS: 2.43], [G ACC: %100.00], [G LOSS: 2.26]\n",
      "E: 1072, [D ACC: %100.00], [D LOSS: 1.81], [G ACC: %100.00], [G LOSS: 5.32]\n",
      "E: 1073, [D ACC: %100.00], [D LOSS: 2.42], [G ACC: %100.00], [G LOSS: 2.99]\n",
      "E: 1074, [D ACC: %100.00], [D LOSS: 1.21], [G ACC: %100.00], [G LOSS: 1.18]\n",
      "E: 1075, [D ACC: %100.00], [D LOSS: 2.36], [G ACC: %100.00], [G LOSS: 1.06]\n",
      "E: 1076, [D ACC: %100.00], [D LOSS: 3.57], [G ACC: %100.00], [G LOSS: 1.14]\n",
      "E: 1077, [D ACC: %100.00], [D LOSS: 2.03], [G ACC: %100.00], [G LOSS: 1.91]\n",
      "E: 1078, [D ACC: %100.00], [D LOSS: 3.46], [G ACC: %100.00], [G LOSS: 1.19]\n",
      "E: 1079, [D ACC: %98.44], [D LOSS: 2.58], [G ACC: %100.00], [G LOSS: 2.67]\n",
      "E: 1080, [D ACC: %100.00], [D LOSS: 5.48], [G ACC: %100.00], [G LOSS: 1.28]\n",
      "E: 1081, [D ACC: %100.00], [D LOSS: 1.94], [G ACC: %100.00], [G LOSS: 3.65]\n",
      "E: 1082, [D ACC: %100.00], [D LOSS: 2.89], [G ACC: %100.00], [G LOSS: 2.16]\n",
      "E: 1083, [D ACC: %100.00], [D LOSS: 3.89], [G ACC: %100.00], [G LOSS: 1.41]\n",
      "E: 1084, [D ACC: %100.00], [D LOSS: 3.99], [G ACC: %100.00], [G LOSS: 3.85]\n",
      "E: 1085, [D ACC: %100.00], [D LOSS: 2.45], [G ACC: %100.00], [G LOSS: 4.50]\n",
      "E: 1086, [D ACC: %100.00], [D LOSS: 2.79], [G ACC: %100.00], [G LOSS: 3.89]\n",
      "E: 1087, [D ACC: %100.00], [D LOSS: 4.12], [G ACC: %100.00], [G LOSS: 3.55]\n",
      "E: 1088, [D ACC: %98.44], [D LOSS: 2.87], [G ACC: %93.75], [G LOSS: 8.83]\n",
      "E: 1089, [D ACC: %100.00], [D LOSS: 1.19], [G ACC: %100.00], [G LOSS: 3.93]\n",
      "E: 1090, [D ACC: %100.00], [D LOSS: 2.09], [G ACC: %100.00], [G LOSS: 4.74]\n",
      "E: 1091, [D ACC: %98.44], [D LOSS: 2.66], [G ACC: %100.00], [G LOSS: 4.36]\n",
      "E: 1092, [D ACC: %100.00], [D LOSS: 2.85], [G ACC: %100.00], [G LOSS: 5.99]\n",
      "E: 1093, [D ACC: %100.00], [D LOSS: 3.51], [G ACC: %96.88], [G LOSS: 7.19]\n",
      "E: 1094, [D ACC: %100.00], [D LOSS: 2.08], [G ACC: %93.75], [G LOSS: 14.70]\n",
      "E: 1095, [D ACC: %100.00], [D LOSS: 1.63], [G ACC: %100.00], [G LOSS: 5.68]\n",
      "E: 1096, [D ACC: %100.00], [D LOSS: 2.12], [G ACC: %100.00], [G LOSS: 4.73]\n",
      "E: 1097, [D ACC: %100.00], [D LOSS: 6.36], [G ACC: %100.00], [G LOSS: 4.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 1098, [D ACC: %98.44], [D LOSS: 4.04], [G ACC: %100.00], [G LOSS: 4.86]\n",
      "E: 1099, [D ACC: %100.00], [D LOSS: 2.62], [G ACC: %96.88], [G LOSS: 12.76]\n",
      "E: 1100, [D ACC: %100.00], [D LOSS: 1.42], [G ACC: %100.00], [G LOSS: 8.07]\n",
      "E: 1101, [D ACC: %100.00], [D LOSS: 2.85], [G ACC: %96.88], [G LOSS: 14.43]\n",
      "E: 1102, [D ACC: %100.00], [D LOSS: 1.10], [G ACC: %93.75], [G LOSS: 17.84]\n",
      "E: 1103, [D ACC: %100.00], [D LOSS: 1.94], [G ACC: %90.62], [G LOSS: 20.49]\n",
      "E: 1104, [D ACC: %100.00], [D LOSS: 2.55], [G ACC: %100.00], [G LOSS: 6.59]\n",
      "E: 1105, [D ACC: %100.00], [D LOSS: 5.35], [G ACC: %100.00], [G LOSS: 4.49]\n",
      "E: 1106, [D ACC: %98.44], [D LOSS: 4.97], [G ACC: %100.00], [G LOSS: 3.19]\n",
      "E: 1107, [D ACC: %100.00], [D LOSS: 2.94], [G ACC: %100.00], [G LOSS: 3.38]\n",
      "E: 1108, [D ACC: %100.00], [D LOSS: 1.31], [G ACC: %100.00], [G LOSS: 4.11]\n",
      "E: 1109, [D ACC: %100.00], [D LOSS: 1.90], [G ACC: %100.00], [G LOSS: 3.21]\n",
      "E: 1110, [D ACC: %100.00], [D LOSS: 3.58], [G ACC: %100.00], [G LOSS: 2.94]\n",
      "E: 1111, [D ACC: %100.00], [D LOSS: 2.68], [G ACC: %100.00], [G LOSS: 4.01]\n",
      "E: 1112, [D ACC: %100.00], [D LOSS: 2.78], [G ACC: %100.00], [G LOSS: 2.10]\n",
      "E: 1113, [D ACC: %100.00], [D LOSS: 1.82], [G ACC: %100.00], [G LOSS: 1.54]\n",
      "E: 1114, [D ACC: %100.00], [D LOSS: 3.11], [G ACC: %100.00], [G LOSS: 1.39]\n",
      "E: 1115, [D ACC: %100.00], [D LOSS: 1.50], [G ACC: %100.00], [G LOSS: 1.34]\n",
      "E: 1116, [D ACC: %100.00], [D LOSS: 4.74], [G ACC: %100.00], [G LOSS: 1.24]\n",
      "E: 1117, [D ACC: %98.44], [D LOSS: 2.46], [G ACC: %100.00], [G LOSS: 1.61]\n",
      "E: 1118, [D ACC: %100.00], [D LOSS: 2.20], [G ACC: %100.00], [G LOSS: 1.97]\n",
      "E: 1119, [D ACC: %100.00], [D LOSS: 2.20], [G ACC: %100.00], [G LOSS: 1.07]\n",
      "E: 1120, [D ACC: %100.00], [D LOSS: 1.33], [G ACC: %100.00], [G LOSS: 1.41]\n",
      "E: 1121, [D ACC: %100.00], [D LOSS: 2.17], [G ACC: %100.00], [G LOSS: 2.43]\n",
      "E: 1122, [D ACC: %100.00], [D LOSS: 1.67], [G ACC: %100.00], [G LOSS: 1.67]\n",
      "E: 1123, [D ACC: %100.00], [D LOSS: 0.94], [G ACC: %100.00], [G LOSS: 1.04]\n",
      "E: 1124, [D ACC: %100.00], [D LOSS: 1.93], [G ACC: %100.00], [G LOSS: 1.16]\n",
      "E: 1125, [D ACC: %100.00], [D LOSS: 6.01], [G ACC: %100.00], [G LOSS: 2.31]\n",
      "E: 1126, [D ACC: %100.00], [D LOSS: 3.26], [G ACC: %100.00], [G LOSS: 0.92]\n",
      "E: 1127, [D ACC: %100.00], [D LOSS: 1.55], [G ACC: %100.00], [G LOSS: 0.90]\n",
      "E: 1128, [D ACC: %100.00], [D LOSS: 2.45], [G ACC: %100.00], [G LOSS: 0.76]\n",
      "E: 1129, [D ACC: %100.00], [D LOSS: 0.90], [G ACC: %100.00], [G LOSS: 0.82]\n",
      "E: 1130, [D ACC: %100.00], [D LOSS: 1.97], [G ACC: %100.00], [G LOSS: 0.85]\n",
      "E: 1131, [D ACC: %100.00], [D LOSS: 2.04], [G ACC: %100.00], [G LOSS: 1.14]\n",
      "E: 1132, [D ACC: %100.00], [D LOSS: 3.80], [G ACC: %100.00], [G LOSS: 1.32]\n",
      "E: 1133, [D ACC: %100.00], [D LOSS: 1.33], [G ACC: %100.00], [G LOSS: 0.85]\n",
      "E: 1134, [D ACC: %100.00], [D LOSS: 3.23], [G ACC: %100.00], [G LOSS: 0.92]\n",
      "E: 1135, [D ACC: %98.44], [D LOSS: 8.94], [G ACC: %100.00], [G LOSS: 0.90]\n",
      "E: 1136, [D ACC: %100.00], [D LOSS: 1.91], [G ACC: %100.00], [G LOSS: 1.52]\n",
      "E: 1137, [D ACC: %100.00], [D LOSS: 1.60], [G ACC: %100.00], [G LOSS: 1.90]\n",
      "E: 1138, [D ACC: %98.44], [D LOSS: 5.30], [G ACC: %100.00], [G LOSS: 1.59]\n",
      "E: 1139, [D ACC: %100.00], [D LOSS: 0.99], [G ACC: %100.00], [G LOSS: 3.20]\n",
      "E: 1140, [D ACC: %100.00], [D LOSS: 2.85], [G ACC: %100.00], [G LOSS: 3.21]\n",
      "E: 1141, [D ACC: %100.00], [D LOSS: 3.74], [G ACC: %100.00], [G LOSS: 2.41]\n",
      "E: 1142, [D ACC: %100.00], [D LOSS: 1.91], [G ACC: %100.00], [G LOSS: 2.55]\n",
      "E: 1143, [D ACC: %100.00], [D LOSS: 5.12], [G ACC: %100.00], [G LOSS: 6.20]\n",
      "E: 1144, [D ACC: %100.00], [D LOSS: 2.24], [G ACC: %100.00], [G LOSS: 8.24]\n",
      "E: 1145, [D ACC: %100.00], [D LOSS: 2.49], [G ACC: %100.00], [G LOSS: 4.90]\n",
      "E: 1146, [D ACC: %100.00], [D LOSS: 2.58], [G ACC: %100.00], [G LOSS: 5.15]\n",
      "E: 1147, [D ACC: %100.00], [D LOSS: 2.83], [G ACC: %96.88], [G LOSS: 16.15]\n",
      "E: 1148, [D ACC: %100.00], [D LOSS: 3.47], [G ACC: %87.50], [G LOSS: 32.48]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4001\n",
    "STEPS = 1  # 60000 // BATCH_SIZE\n",
    "\n",
    "\n",
    "train_loss_g = []\n",
    "train_loss_d = []\n",
    "\n",
    "train_acc_g = []\n",
    "train_acc_d = []\n",
    "\n",
    "\n",
    "disc_itr = load_batch()\n",
    "gen_itr = load_noise()\n",
    "\n",
    "# to be visualized\n",
    "next(disc_itr)\n",
    "next(disc_itr)\n",
    "next(disc_itr)\n",
    "next(disc_itr)\n",
    "\n",
    "_, _, _, viz_real, _ = next(disc_itr)\n",
    "vis_noise = np.random.normal(size=(16, noise_len,))\n",
    "print(vis_noise.shape)\n",
    "\n",
    "# epochs\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "        \n",
    "        x, z_fake, y_fake, z_real, y_real = next(disc_itr)\n",
    "\n",
    "        # train\n",
    "        loss_2, acc_2 = discriminator.train_on_batch(z_real, y_real)\n",
    "        loss_1, acc_1 = discriminator.train_on_batch(z_fake, y_fake)\n",
    "\n",
    "        batch_loss = 0.5 * (loss_1 + loss_2)\n",
    "        batch_acc = 0.5 * (acc_1 + acc_2)\n",
    "\n",
    "        loss.append(batch_loss)\n",
    "        acc.append(batch_acc)\n",
    "\n",
    "    train_loss_d.append(np.mean(np.array(loss)))\n",
    "    train_acc_d.append(np.mean(np.array(acc)))\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "\n",
    "        x, y_true = next(gen_itr)\n",
    "\n",
    "        # train\n",
    "        loss_1, acc_1 = gan.train_on_batch(x, y_true)\n",
    "\n",
    "        loss.append(loss_1)\n",
    "        acc.append(acc_1)\n",
    "\n",
    "    train_loss_g.append(np.mean(np.array(loss)))\n",
    "    train_acc_g.append(np.mean(np.array(acc)))\n",
    "\n",
    "\n",
    "    print(\"E: {}, [D ACC: %{:.2f}], [D LOSS: {:.2f}], [G ACC: %{:.2f}], [G LOSS: {:.2f}]\".format(\n",
    "          e,\n",
    "          train_acc_d[-1] * 100,\n",
    "          train_loss_d[-1] * 100,\n",
    "          train_acc_g[-1] * 100,\n",
    "          train_loss_g[-1] * 100\n",
    "      ))\n",
    "\n",
    "    if e % 25 == 0:\n",
    "        ## visualize results\n",
    "        viz_fake = generator.predict(vis_noise)\n",
    "        visualizeGAN(e, viz_real, viz_fake)\n",
    "        \n",
    "        ## save model\n",
    "        pth = os.path.join(models_path, 'gan.h5')\n",
    "        gan.save(pth)\n",
    "\n",
    "        pth = os.path.join(models_path, 'generator-{}-{}-{}.h5'.format(e, train_loss_g[-1], train_acc_g[-1]))\n",
    "        generator.save(pth)\n",
    "\n",
    "        pth = os.path.join(models_path, 'discriminator.h5')\n",
    "        discriminator.save(pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnlcqEI8i9uH"
   },
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LzhKde-CaDu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "plt.plot(train_loss_g, label=\"Generator Loss\");\n",
    "plt.plot(train_loss_d, label=\"Discriminator Loss\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9iCkvcai9uS"
   },
   "source": [
    "## Plot Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YhSUa3fROSg"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "plt.plot(train_acc_g, label=\"Generator Accuracy\");\n",
    "plt.plot(train_acc_d, label=\"Discriminator Accuracy\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "pth = os.path.join(models_path, 'gray-gan.h5')\n",
    "gan.save(pth)\n",
    "\n",
    "pth = os.path.join(models_path, 'gray-generator.h5')\n",
    "generator.save(pth)\n",
    "\n",
    "pth = os.path.join(models_path, 'gray-discriminator.h5')\n",
    "discriminator.save(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST Test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
