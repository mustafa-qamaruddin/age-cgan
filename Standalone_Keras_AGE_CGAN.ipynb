{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "BIEdHLeqDLJH",
    "outputId": "bd25ca97-2443-4c65-d804-d9c0ff7b5586"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_path = './'\n",
    "dataset_path = os.path.join(root_path, 'tf_dataset')\n",
    "\n",
    "models_path = os.path.join(root_path, 'saved_models')\n",
    "if not os.path.exists(models_path):\n",
    "  os.mkdir(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XY64NlGCDdCr"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "\n",
    "import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWYY-ez0D6wt"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "image_feature_description = {\n",
    "    'enc': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'age': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "    'img': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "\n",
    "raw_train_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path,'train.tfrecords'))\n",
    "parsed_train_dataset = raw_train_dataset.map(_parse_image_function)\n",
    "\n",
    "raw_val_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path, 'val.tfrecords'))\n",
    "parsed_val_dataset = raw_val_dataset.map(_parse_image_function)\n",
    "\n",
    "raw_test_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path, 'test.tfrecords'))\n",
    "parsed_test_dataset = raw_test_dataset.map(_parse_image_function)\n",
    "\n",
    "\n",
    "parsed_train_dataset = parsed_train_dataset.repeat()\n",
    "parsed_train_dataset = parsed_train_dataset.shuffle(1000)\n",
    "parsed_train_dataset = parsed_train_dataset.batch(BATCH_SIZE)\n",
    "dataset_iterator = parsed_train_dataset.make_one_shot_iterator()\n",
    "\n",
    "variable_dataset = dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAuHCfjlFBOy"
   },
   "outputs": [],
   "source": [
    "enc_len = 128\n",
    "age_len = 8\n",
    "img_shape = (32, 32, 3)\n",
    "width, height, depth = (32, 32, 3)\n",
    "img_len = np.prod(img_shape)\n",
    "latent_dim = enc_len + age_len + img_len\n",
    "noise_len = 100  # 32 x 32 x 3\n",
    "input_dim = enc_len + age_len + noise_len\n",
    "cond_len = enc_len + age_len\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    model = keras.Sequential([\n",
    "        # dense 1\n",
    "        keras.layers.Dense(3072, input_shape=(latent_dim,)),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # reshape 1d to 3d\n",
    "        keras.layers.Reshape((width, height, depth)),\n",
    "        \n",
    "        # conv block 1\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # conv block 2\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # conv block 3\n",
    "        keras.layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # conv block 4\n",
    "        keras.layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # flatten\n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        # dense 2\n",
    "        keras.layers.Dense(512),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # output\n",
    "        keras.layers.Dense(1),\n",
    "        keras.layers.Activation(tf.nn.sigmoid),\n",
    "    ])\n",
    "    \n",
    "    # condition\n",
    "    c1 = keras.layers.Input(shape=(enc_len,))\n",
    "    c2 = keras.layers.Input(shape=(age_len,))\n",
    "    \n",
    "    # image\n",
    "    z = keras.layers.Input(shape=img_shape)\n",
    "    \n",
    "    # flatten image\n",
    "    z_flat = keras.layers.Flatten()(z)\n",
    "    \n",
    "    # concatenation\n",
    "    inputs = keras.layers.concatenate([c1, c2, z_flat])\n",
    "    \n",
    "    # real or fake\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    return keras.models.Model([c1, c2, z], outputs)\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    model = keras.Sequential([\n",
    "        \n",
    "        # dense 1\n",
    "        keras.layers.Dense(3072, input_shape=(input_dim,)),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # reshape 1d to 3d\n",
    "        keras.layers.Reshape((width, height, depth)),\n",
    "        \n",
    "        # transpose conv block 1\n",
    "        keras.layers.Conv2DTranspose(filters=16, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.UpSampling2D(size=(2,2)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # transpose conv block 2\n",
    "        keras.layers.Conv2DTranspose(filters=16, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.UpSampling2D(size=(2,2)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # transpose conv block 3\n",
    "        keras.layers.Conv2DTranspose(filters=32, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.UpSampling2D(size=(2,2)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # transpose conv block 4\n",
    "        keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same'),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.UpSampling2D(size=(2,2)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # flatten\n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        # dense 3\n",
    "        keras.layers.Dense(3072, input_shape=(input_dim,)),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # reshape 1d to 3d\n",
    "        keras.layers.Reshape((width, height, depth)),\n",
    "        \n",
    "        # output\n",
    "        keras.layers.Activation(tf.nn.tanh),\n",
    "    ])\n",
    "    \n",
    "    # condition\n",
    "    c1 = keras.layers.Input(shape=(enc_len,))\n",
    "    c2 = keras.layers.Input(shape=(age_len,))\n",
    "    \n",
    "    # noise\n",
    "    x = keras.layers.Input(shape=(noise_len,))\n",
    "\n",
    "    # concatenation\n",
    "    inputs = keras.layers.concatenate([c1, c2, x])\n",
    "    \n",
    "    # real or fake\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    return keras.models.Model([c1, c2, x], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoYXZ0kE46LH"
   },
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfo8J8jQ4-FH"
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObTb7HIf5CqA"
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHTmYpPeImn5"
   },
   "outputs": [],
   "source": [
    "GLR = 0.02  # generator\n",
    "DLR = 0.001  # discriminator\n",
    "\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=DLR),\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# condition\n",
    "c1 = keras.layers.Input(shape=(enc_len,))\n",
    "c2 = keras.layers.Input(shape=(age_len,))\n",
    "\n",
    "# noise\n",
    "x = keras.layers.Input(shape=(noise_len,))\n",
    "\n",
    "# freeze discriminator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# output\n",
    "z = generator([c1, c2, x])\n",
    "out = discriminator([c1, c2, z])\n",
    "\n",
    "# GAN\n",
    "gan = keras.models.Model(inputs=[c1, c2, x], outputs=out)\n",
    "\n",
    "gan.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=GLR),\n",
    "    loss=keras.losses.kullback_leibler_divergence,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mmrborLIvNK"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  values = sess.run([variable_dataset])\n",
    "  print(values)\n",
    "  print(values[0]['age'], values[0]['enc'])\n",
    "  print(tf.io.decode_raw(values[0]['img'], tf.uint8).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZNLvw1uCEiz"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  for i in range(4000//16):\n",
    "    values = sess.run([variable_dataset])\n",
    "    print(values[0]['age'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RK7vMyQWLp1V"
   },
   "outputs": [],
   "source": [
    "parsed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "iSszxe2oKY02",
    "outputId": "ee5ce939-c513-4c70-9983-ae17faca6d67"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "EPOCHS = 10000\n",
    "STEPS = 1024 // BATCH_SIZE\n",
    "\n",
    "\n",
    "train_loss_g = []\n",
    "val_loss_g = []\n",
    "\n",
    "train_loss_d = []\n",
    "val_loss_d = []\n",
    "\n",
    "\n",
    "train_acc_g = []\n",
    "val_acc_g = []\n",
    "\n",
    "train_acc_d = []\n",
    "val_acc_d = []\n",
    "\n",
    "\n",
    "y_fake = tf.zeros((BATCH_SIZE,))\n",
    "y_true = tf.ones((BATCH_SIZE,))\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "  K.set_session(sess)\n",
    "  \n",
    "  tf.initialize_all_variables().run()\n",
    "\n",
    "  # epochs\n",
    "  for e in range(EPOCHS):\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "\n",
    "      values = sess.run([variable_dataset])\n",
    "      row = values[0]\n",
    "\n",
    "      sz = row['img'].shape[0]\n",
    "\n",
    "#       if sz != BATCH_SIZE:\n",
    "#         continue\n",
    "\n",
    "      # train discriminator\n",
    "\n",
    "      # fake data\n",
    "      c1 = row['enc']\n",
    "      c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "      x = tf.random.normal((sz, noise_len,)).eval()\n",
    "      z_fake = generator.predict([c1, c2, x])\n",
    "\n",
    "      # real data\n",
    "      c1 = row['enc']\n",
    "      c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "      z_real = tf.reshape(tf.io.decode_raw(row['img'], tf.uint8), (-1, width, height, depth)).eval()\n",
    "\n",
    "      # train\n",
    "      loss_1, acc_1 = discriminator.train_on_batch([c1, c2, z_fake], y_fake.eval())\n",
    "      loss_2, acc_2 = discriminator.train_on_batch([c1, c2, z_real], y_true.eval())\n",
    "\n",
    "      batch_loss = 0.5 * (float(tf.reduce_mean(loss_1).eval()) + float(tf.reduce_mean(loss_2).eval()))\n",
    "      batch_acc = 0.5 * (float(tf.reduce_mean(acc_1).eval()) + float(tf.reduce_mean(acc_2).eval()))\n",
    "\n",
    "      loss.append(batch_loss)\n",
    "      acc.append(batch_acc)\n",
    "\n",
    "    train_loss_d.append(np.mean(np.array(loss)))\n",
    "    train_acc_d.append(np.mean(np.array(acc)))\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "\n",
    "      values = sess.run([variable_dataset])\n",
    "      row = values[0]\n",
    "\n",
    "      sz = row['img'].shape[0]\n",
    "\n",
    "#       if sz != BATCH_SIZE:\n",
    "#         continue\n",
    "\n",
    "      # train generator\n",
    "\n",
    "      # concatenate face + age + noise\n",
    "      c1 = row['enc']\n",
    "      c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "      x = tf.random.normal((sz, noise_len,)).eval()\n",
    "\n",
    "      # train\n",
    "      loss_1, acc_1 = gan.train_on_batch([c1, c2, x], y_true.eval())\n",
    "\n",
    "      batch_loss = float(tf.reduce_mean(loss_1).eval())\n",
    "      batch_acc = float(tf.reduce_mean(acc_1).eval())\n",
    "\n",
    "      loss.append(batch_loss)\n",
    "      acc.append(batch_acc)\n",
    "\n",
    "    train_loss_g.append(np.mean(np.array(loss)))\n",
    "    train_acc_g.append(np.mean(np.array(acc)))\n",
    "\n",
    "\n",
    "    print(\"Epoch: {}, Steps: {}, Discriminator Loss: {:.3f}, Discriminator Accuracy: %{:.2f}, GAN Loss: {:.3f}, GAN Accuracy: %{:.2f}\".format(\n",
    "          e,\n",
    "          STEPS,\n",
    "          train_loss_d[-1],\n",
    "          train_loss_g[-1],\n",
    "          train_acc_d[-1],\n",
    "          train_acc_g[-1]\n",
    "      ))\n",
    " \n",
    "    if e % 100 == 1:\n",
    "\n",
    "      pth = os.path.join(models_path, 'gan.h5')\n",
    "      gan.save(pth)\n",
    "\n",
    "      pth = os.path.join(models_path, 'generator-{}-{}-{}.h5'.format(e, train_loss_g[-1], train_acc_g[-1]))\n",
    "      generator.save(pth)\n",
    "\n",
    "      pth = os.path.join(models_path, 'discriminator.h5')\n",
    "      discriminator.save(pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0Uxu-RaTBiI"
   },
   "outputs": [],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRFK2jHCpfpK"
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKLt5f22piWS"
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LzhKde-CaDu"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_g, label=\"Generator Loss\");\n",
    "plt.plot(train_loss_d, label=\"Discriminator Loss\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YhSUa3fROSg"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_g, label=\"Generator Accuracy\");\n",
    "plt.plot(train_acc_d, label=\"Discriminator Accuracy\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E93kpBKFC84Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Standalone Keras AGE-CGAN",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
