{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAQBOBYyi9sc"
   },
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2813,
     "status": "ok",
     "timestamp": 1554977904593,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "BIEdHLeqDLJH",
    "outputId": "90567bca-f98c-4b16-c01f-adc92d510da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "root_path = './'\n",
    "dataset_path = os.path.join(root_path, 'tf_dataset_gray')\n",
    "\n",
    "\n",
    "models_path = os.path.join(root_path, 'saved_models_face_v9')\n",
    "if not os.path.exists(models_path):\n",
    "    os.mkdir(models_path)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "image_feature_description = {\n",
    "    'enc': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    'age': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "    'img': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "\n",
    "raw_train_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path, 'train.tfrecords'))\n",
    "parsed_train_dataset = raw_train_dataset.map(_parse_image_function)\n",
    "\n",
    "# raw_val_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path, 'val.tfrecords'))\n",
    "# parsed_val_dataset = raw_val_dataset.map(_parse_image_function)\n",
    "\n",
    "# raw_test_dataset = tf.data.TFRecordDataset(os.path.join(dataset_path, 'test.tfrecords'))\n",
    "# parsed_test_dataset = raw_test_dataset.map(_parse_image_function)\n",
    "\n",
    "\n",
    "parsed_train_dataset = parsed_train_dataset.repeat()\n",
    "parsed_train_dataset = parsed_train_dataset.shuffle(7000)\n",
    "parsed_train_dataset = parsed_train_dataset.batch(BATCH_SIZE)\n",
    "dataset_iterator = parsed_train_dataset.make_one_shot_iterator()\n",
    "\n",
    "variable_dataset = dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMouydu3i9sp"
   },
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3123,
     "status": "ok",
     "timestamp": 1554977905340,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "HAuHCfjlFBOy",
    "outputId": "abf88c5b-9df8-41dc-9017-9d8a2007a0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D M1:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_52 (Conv2D)           (None, 15, 15, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_107 (Bat (None, 15, 15, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_108 (Bat (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 3, 3, 3)           867       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 3, 3, 3)           0         \n",
      "=================================================================\n",
      "Total params: 5,859\n",
      "Trainable params: 5,763\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "D M2:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 1)                 36        \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "G M2:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose_88 (Conv2DT (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 14, 14, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_89 (Conv2DT (None, 16, 16, 18)        2610      \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 16, 16, 18)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 16, 16, 18)        72        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_90 (Conv2DT (None, 18, 18, 20)        3260      \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 18, 18, 20)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 18, 18, 20)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_91 (Conv2DT (None, 20, 20, 22)        3982      \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 20, 20, 22)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 20, 20, 22)        88        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_92 (Conv2DT (None, 22, 22, 24)        4776      \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 22, 22, 24)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 22, 22, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_93 (Conv2DT (None, 24, 24, 16)        3472      \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 24, 24, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_94 (Conv2DT (None, 26, 26, 8)         1160      \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 26, 26, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 26, 26, 8)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_95 (Conv2DT (None, 28, 28, 4)         292       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 28, 28, 4)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 28, 28, 4)         16        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_96 (Conv2DT (None, 30, 30, 2)         74        \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 30, 30, 2)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 30, 30, 2)         8         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_97 (Conv2DT (None, 32, 32, 1)         19        \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 32, 32, 1)         0         \n",
      "=================================================================\n",
      "Total params: 20,325\n",
      "Trainable params: 20,065\n",
      "Non-trainable params: 260\n",
      "_________________________________________________________________\n",
      "G M1:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 144)               8496      \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "reshape_18 (Reshape)         (None, 12, 12, 1)         0         \n",
      "=================================================================\n",
      "Total params: 8,496\n",
      "Trainable params: 8,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_len = 0\n",
    "age_len = 8\n",
    "img_shape = (32, 32, 1)\n",
    "width, height, depth = (32, 32, 1)\n",
    "img_len = np.prod(img_shape)\n",
    "latent_dim = enc_len + age_len + img_len\n",
    "noise_len = 50  # 32 x 32 x 3\n",
    "input_dim = enc_len + age_len + noise_len\n",
    "cond_len = enc_len + age_len\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    conv = keras.Sequential([\n",
    "        # conv block 1\n",
    "        keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=2,\n",
    "            input_shape=img_shape\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "\n",
    "        # conv block 2\n",
    "        keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=2\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # conv block 3\n",
    "        keras.layers.Conv2D(\n",
    "            filters=3,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=2\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "    ])\n",
    "    \n",
    "    print(\"D M1:\")\n",
    "    conv.summary()\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # output\n",
    "        keras.layers.Dense(1, input_shape=(age_len+27,)),\n",
    "        keras.layers.Activation(tf.nn.sigmoid),\n",
    "    ])\n",
    "    \n",
    "    clf = keras.Sequential([\n",
    "        # output\n",
    "        keras.layers.Dense(age_len, input_shape=(age_len+27,)),\n",
    "        keras.layers.Activation(tf.nn.softmax),\n",
    "    ])\n",
    "    \n",
    "    print(\"D M2:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # condition\n",
    "#     c1 = keras.layers.Input(shape=(enc_len,))\n",
    "    c2 = keras.layers.Input(shape=(age_len,))\n",
    "    \n",
    "    # image\n",
    "    z = keras.layers.Input(shape=img_shape)\n",
    "    \n",
    "    # convolution\n",
    "    zout = conv(z)\n",
    "    \n",
    "    # flatten image\n",
    "    z_flat = keras.layers.Flatten()(zout)\n",
    "    \n",
    "    # concatenation\n",
    "    inputs = keras.layers.concatenate([c2, z_flat])\n",
    "    \n",
    "    # real or fake\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 0, 1, 2, 3, .. 9\n",
    "    lbl = clf(inputs)\n",
    "    \n",
    "    return keras.models.Model([c2, z], [outputs, lbl])\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    \n",
    "    conv = keras.Sequential([\n",
    "        # transpose conv block 1\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            input_shape=(12, 12, 1)\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 2\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=18,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 3\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=20,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 4\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=22,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 5\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=24,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 6\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 7\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 8\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=4,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # transpose conv block 9\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=2,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        keras.layers.BatchNormalization(),\n",
    "\n",
    "        # transpose conv block 10\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1\n",
    "        ),\n",
    "        \n",
    "        # output\n",
    "        keras.layers.Activation(tf.nn.tanh)\n",
    "    ])\n",
    "    \n",
    "    print(\"G M2:\")\n",
    "    conv.summary()\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # dense 1\n",
    "        keras.layers.Dense(144, input_shape=(input_dim,)),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # reshape 1d to 3d\n",
    "        keras.layers.Reshape((12, 12, 1))\n",
    "    ])\n",
    "    \n",
    "    print(\"G M1:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # condition\n",
    "#     c1 = keras.layers.Input(shape=(enc_len,))\n",
    "    c2 = keras.layers.Input(shape=(age_len,))\n",
    "    \n",
    "    # noise\n",
    "    x = keras.layers.Input(shape=(noise_len,))\n",
    "\n",
    "    # concatenation\n",
    "    inputs = keras.layers.concatenate([c2, x])\n",
    "    \n",
    "    # flat dense output\n",
    "    out_1 = model(inputs)\n",
    "    \n",
    "    # transpose conv output\n",
    "    outputs = conv(out_1)\n",
    "    \n",
    "    return keras.models.Model([c2, x], outputs)\n",
    "\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3046,
     "status": "ok",
     "timestamp": 1554977905345,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "tfo8J8jQ4-FH",
    "outputId": "022c24ae-1c6e-40ad-e167-964953449961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_71 (InputLayer)           (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_72 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 58)           0           input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_90 (Sequential)      (None, 12, 12, 1)    8496        concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_89 (Sequential)      (None, 32, 32, 1)    20325       sequential_90[1][0]              \n",
      "==================================================================================================\n",
      "Total params: 28,821\n",
      "Trainable params: 28,561\n",
      "Non-trainable params: 260\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3013,
     "status": "ok",
     "timestamp": 1554977905348,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "ObTb7HIf5CqA",
    "outputId": "1bdb05c0-cd96-46c6-a124-68ac9ee9ddb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_70 (InputLayer)           (None, 32, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_86 (Sequential)      (None, 3, 3, 3)      5859        input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_69 (InputLayer)           (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 27)           0           sequential_86[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 35)           0           input_69[0][0]                   \n",
      "                                                                 flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_87 (Sequential)      (None, 1)            36          concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_88 (Sequential)      (None, 8)            288         concatenate_35[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 6,183\n",
      "Trainable params: 6,087\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctWfkxy5i9tR"
   },
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHTmYpPeImn5"
   },
   "outputs": [],
   "source": [
    "GLR = 0.0009  # generator\n",
    "DLR = 0.0009  # discriminator\n",
    "\n",
    "\n",
    "# Wasserstein\n",
    "def d_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=keras.optimizers.Adam(DLR, 0.5),\n",
    "    loss=[keras.losses.binary_crossentropy, keras.losses.categorical_crossentropy],\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# condition\n",
    "# c1 = keras.layers.Input(shape=(enc_len,))\n",
    "c2 = keras.layers.Input(shape=(age_len,))\n",
    "\n",
    "# noise\n",
    "x = keras.layers.Input(shape=(noise_len,))\n",
    "\n",
    "# freeze discriminator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# output\n",
    "z = generator([c2, x])\n",
    "out, lbl = discriminator([c2, z])\n",
    "\n",
    "# GAN\n",
    "gan = keras.models.Model(inputs=[c2, x], outputs=[out, lbl])\n",
    "\n",
    "gan.compile(\n",
    "    optimizer=keras.optimizers.Adam(GLR , 0.5),\n",
    "    loss=[keras.losses.binary_crossentropy, keras.losses.categorical_crossentropy],\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3156,
     "status": "ok",
     "timestamp": 1554977905707,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "o_76rZ0ti9tc",
    "outputId": "8eb9fa73-4621-40e3-fab1-fd2d52d7bbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_73 (InputLayer)           (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_74 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_36 (Model)                (None, 32, 32, 1)    28821       input_73[0][0]                   \n",
      "                                                                 input_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_35 (Model)                [(None, 1), (None, 8 6183        input_73[0][0]                   \n",
      "                                                                 model_36[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 35,004\n",
      "Trainable params: 28,561\n",
      "Non-trainable params: 6,443\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REaxJyLqi9tp"
   },
   "source": [
    "## Visualization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25210,
     "status": "ok",
     "timestamp": 1554977927807,
     "user": {
      "displayName": "Mustafa Qamar-ud-Din",
      "photoUrl": "https://lh5.googleusercontent.com/-JYdQ9YNJp_I/AAAAAAAAAAI/AAAAAAAAG-w/S5nt9QpYyIw/s64/photo.jpg",
      "userId": "09937818909262344238"
     },
     "user_tz": -120
    },
    "id": "4kA4g6_lt3D8",
    "outputId": "c5b38cce-ad98-49ac-9b13-950a8e2356ca"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "import os\n",
    "\n",
    "\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "root_path = './'\n",
    "tgt_pth = os.path.join(root_path, 'visualize_face-v23')\n",
    "\n",
    "if not os.path.exists(tgt_pth):\n",
    "  os.mkdir(tgt_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3nB78iWi9ts"
   },
   "outputs": [],
   "source": [
    "def visualizeGAN(e, z_real, z_fake, conditions=None):\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "    r_real = 0\n",
    "    r_fake = 0\n",
    "    for row, axe in enumerate(axes):\n",
    "        for col, cell in enumerate(axe):\n",
    "            if row % 2 == 0 and z_real is not None:\n",
    "                cell.imshow(\n",
    "                    np.squeeze(\n",
    "                        0.5 * z_real[r_real * 4 + col] + 0.5,\n",
    "                        axis=-1\n",
    "                    ),\n",
    "                    cmap='gray'\n",
    "                )\n",
    "            else:\n",
    "                cell.imshow(\n",
    "                    np.squeeze(\n",
    "                        0.5 * z_fake[r_fake * 4 + col] + 0.5,\n",
    "                        axis=-1\n",
    "                    ),\n",
    "                    cmap='gray'\n",
    "                )\n",
    "                cell.set_title(\n",
    "                    str(\n",
    "                        np.argmax(\n",
    "                            conditions[r_fake * 4 + col]\n",
    "                        )\n",
    "                    ) + \": \" + str(\n",
    "                        conditions[r_fake * 4 + col]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            cell.axis(\"off\")\n",
    "\n",
    "        if row % 2 == 0 and z_real is not None:\n",
    "            r_real += 1\n",
    "        else:\n",
    "            r_fake += 1\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(os.path.join(tgt_pth, '{}.jpg'.format(str(e).zfill(3))))\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noise():\n",
    "    \n",
    "    y_true = tf.ones((BATCH_SIZE,))\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.40\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        # run once\n",
    "        y_true = y_true.eval()\n",
    "\n",
    "        while True:\n",
    "            values = sess.run([variable_dataset])\n",
    "            row = values[0]\n",
    "\n",
    "            sz = row['img'].shape[0]\n",
    "\n",
    "            if sz != BATCH_SIZE:\n",
    "                continue\n",
    "            \n",
    "            # fake data\n",
    "            # concatenate face + age + noise\n",
    "#             c1 = row['enc']\n",
    "            c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "            x = tf.random.normal((sz, noise_len,)).eval()\n",
    "            \n",
    "            yield c2, x, y_true\n",
    "\n",
    "\n",
    "def load_batch():\n",
    "    \n",
    "    y_fake = tf.zeros((BATCH_SIZE,))\n",
    "    y_true = tf.ones((BATCH_SIZE,))\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.40\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        # run once\n",
    "        y_fake = y_fake.eval()\n",
    "        y_true = y_true.eval()\n",
    "\n",
    "        while True:\n",
    "            values = sess.run([variable_dataset])\n",
    "            row = values[0]\n",
    "\n",
    "            sz = row['img'].shape[0]\n",
    "\n",
    "            if sz != BATCH_SIZE:\n",
    "                continue\n",
    "            \n",
    "            # fake data\n",
    "            c2 = tf.cast(row['age'], tf.float32).eval()\n",
    "            x = tf.random.normal((sz, noise_len,)).eval()\n",
    "            z_fake = generator.predict([c2, x])\n",
    "\n",
    "            # real data\n",
    "            z_real = tf.reshape(tf.io.decode_raw(row['img'], tf.int64), (-1, width, height, depth))\n",
    "    \n",
    "            z_real = tf.cast(z_real, tf.float32)\n",
    "    \n",
    "            # scale to [-1, +1]\n",
    "            z_real = tf.math.subtract(tf.math.divide(z_real, 127.5), 1)\n",
    "        \n",
    "            z_real = z_real.eval()\n",
    "                        \n",
    "            yield c2, x, z_fake, y_fake, z_real, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GNNmDUZi9t3"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GNNmDUZi9t3"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object load_batch at 0x7fac5de82830>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-34-134ecb5784d7>\", line 77, in load_batch\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1573, in __exit__\n",
      "    exec_tb)\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5069, in get_controller\n",
      "    type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.client.session.Session'> objects\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fac5de99208>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fac5de996d8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 0, D:[ACC: %55.66, LOSS: 270.03], G:[ACC: %22.27, LOSS: 380.58]\n",
      "E: 1, D:[ACC: %57.62, LOSS: 286.27], G:[ACC: %17.97, LOSS: 350.56]\n",
      "E: 2, D:[ACC: %56.05, LOSS: 268.32], G:[ACC: %16.80, LOSS: 319.05]\n",
      "E: 3, D:[ACC: %61.52, LOSS: 264.28], G:[ACC: %12.50, LOSS: 317.68]\n",
      "E: 4, D:[ACC: %70.51, LOSS: 256.27], G:[ACC: %10.55, LOSS: 332.53]\n",
      "E: 5, D:[ACC: %64.45, LOSS: 257.98], G:[ACC: %5.86, LOSS: 335.25]\n",
      "E: 6, D:[ACC: %72.85, LOSS: 246.10], G:[ACC: %3.52, LOSS: 340.41]\n",
      "E: 7, D:[ACC: %77.73, LOSS: 249.35], G:[ACC: %5.08, LOSS: 327.60]\n",
      "E: 8, D:[ACC: %70.51, LOSS: 247.00], G:[ACC: %7.03, LOSS: 320.10]\n",
      "E: 9, D:[ACC: %75.20, LOSS: 242.32], G:[ACC: %3.91, LOSS: 350.69]\n",
      "E: 10, D:[ACC: %76.17, LOSS: 236.38], G:[ACC: %1.17, LOSS: 369.24]\n",
      "E: 11, D:[ACC: %77.93, LOSS: 229.85], G:[ACC: %0.00, LOSS: 398.55]\n",
      "E: 12, D:[ACC: %79.88, LOSS: 225.45], G:[ACC: %0.00, LOSS: 386.73]\n",
      "E: 13, D:[ACC: %91.99, LOSS: 230.25], G:[ACC: %0.00, LOSS: 386.66]\n",
      "E: 14, D:[ACC: %89.45, LOSS: 227.99], G:[ACC: %0.39, LOSS: 389.10]\n",
      "E: 15, D:[ACC: %79.69, LOSS: 234.87], G:[ACC: %0.39, LOSS: 390.64]\n",
      "E: 16, D:[ACC: %85.94, LOSS: 226.75], G:[ACC: %2.34, LOSS: 359.40]\n",
      "E: 17, D:[ACC: %87.89, LOSS: 229.06], G:[ACC: %2.73, LOSS: 344.13]\n",
      "E: 18, D:[ACC: %93.75, LOSS: 228.05], G:[ACC: %2.73, LOSS: 340.40]\n",
      "E: 19, D:[ACC: %90.04, LOSS: 225.94], G:[ACC: %1.95, LOSS: 350.21]\n",
      "E: 20, D:[ACC: %97.66, LOSS: 220.56], G:[ACC: %0.00, LOSS: 387.64]\n",
      "E: 21, D:[ACC: %97.46, LOSS: 223.71], G:[ACC: %0.00, LOSS: 398.30]\n",
      "E: 22, D:[ACC: %96.68, LOSS: 218.61], G:[ACC: %0.39, LOSS: 397.57]\n",
      "E: 23, D:[ACC: %97.07, LOSS: 221.11], G:[ACC: %0.00, LOSS: 367.51]\n",
      "E: 24, D:[ACC: %97.07, LOSS: 220.01], G:[ACC: %0.39, LOSS: 395.09]\n",
      "E: 25, D:[ACC: %91.80, LOSS: 214.61], G:[ACC: %0.39, LOSS: 393.56]\n",
      "E: 26, D:[ACC: %94.14, LOSS: 221.35], G:[ACC: %1.56, LOSS: 380.92]\n",
      "E: 27, D:[ACC: %97.07, LOSS: 223.46], G:[ACC: %2.34, LOSS: 363.87]\n",
      "E: 28, D:[ACC: %97.27, LOSS: 217.42], G:[ACC: %1.95, LOSS: 371.80]\n",
      "E: 29, D:[ACC: %97.27, LOSS: 219.56], G:[ACC: %2.34, LOSS: 376.71]\n",
      "E: 30, D:[ACC: %98.05, LOSS: 222.44], G:[ACC: %0.00, LOSS: 382.53]\n",
      "E: 31, D:[ACC: %98.44, LOSS: 218.92], G:[ACC: %0.39, LOSS: 404.22]\n",
      "E: 32, D:[ACC: %96.68, LOSS: 214.56], G:[ACC: %0.39, LOSS: 415.17]\n",
      "E: 33, D:[ACC: %97.07, LOSS: 215.24], G:[ACC: %0.00, LOSS: 426.81]\n",
      "E: 34, D:[ACC: %98.83, LOSS: 206.38], G:[ACC: %0.00, LOSS: 421.58]\n",
      "E: 35, D:[ACC: %98.05, LOSS: 212.37], G:[ACC: %0.00, LOSS: 418.55]\n",
      "E: 36, D:[ACC: %98.05, LOSS: 215.33], G:[ACC: %0.78, LOSS: 388.84]\n",
      "E: 37, D:[ACC: %97.07, LOSS: 212.80], G:[ACC: %1.56, LOSS: 348.29]\n",
      "E: 38, D:[ACC: %96.88, LOSS: 216.86], G:[ACC: %1.56, LOSS: 371.02]\n",
      "E: 39, D:[ACC: %96.88, LOSS: 213.12], G:[ACC: %0.78, LOSS: 397.74]\n",
      "E: 40, D:[ACC: %98.05, LOSS: 207.85], G:[ACC: %0.00, LOSS: 424.46]\n",
      "E: 41, D:[ACC: %98.05, LOSS: 212.73], G:[ACC: %0.00, LOSS: 415.61]\n",
      "E: 42, D:[ACC: %98.63, LOSS: 211.86], G:[ACC: %0.00, LOSS: 400.67]\n",
      "E: 43, D:[ACC: %97.85, LOSS: 205.23], G:[ACC: %0.00, LOSS: 414.75]\n",
      "E: 44, D:[ACC: %99.61, LOSS: 205.11], G:[ACC: %0.00, LOSS: 397.81]\n",
      "E: 45, D:[ACC: %97.66, LOSS: 207.05], G:[ACC: %1.17, LOSS: 351.39]\n",
      "E: 46, D:[ACC: %97.85, LOSS: 211.38], G:[ACC: %0.00, LOSS: 378.56]\n",
      "E: 47, D:[ACC: %98.63, LOSS: 206.41], G:[ACC: %0.00, LOSS: 387.82]\n",
      "E: 48, D:[ACC: %98.44, LOSS: 200.01], G:[ACC: %0.00, LOSS: 375.46]\n",
      "E: 49, D:[ACC: %97.46, LOSS: 206.12], G:[ACC: %0.39, LOSS: 375.01]\n",
      "E: 50, D:[ACC: %98.63, LOSS: 203.97], G:[ACC: %0.00, LOSS: 374.42]\n",
      "E: 51, D:[ACC: %97.85, LOSS: 205.36], G:[ACC: %0.39, LOSS: 377.50]\n",
      "E: 52, D:[ACC: %98.24, LOSS: 202.12], G:[ACC: %0.78, LOSS: 394.10]\n",
      "E: 53, D:[ACC: %99.22, LOSS: 199.06], G:[ACC: %0.39, LOSS: 391.25]\n",
      "E: 54, D:[ACC: %98.63, LOSS: 200.60], G:[ACC: %0.00, LOSS: 393.21]\n",
      "E: 55, D:[ACC: %97.66, LOSS: 203.85], G:[ACC: %0.00, LOSS: 395.17]\n",
      "E: 56, D:[ACC: %97.46, LOSS: 201.69], G:[ACC: %0.00, LOSS: 397.43]\n",
      "E: 57, D:[ACC: %97.85, LOSS: 203.51], G:[ACC: %0.39, LOSS: 375.17]\n",
      "E: 58, D:[ACC: %98.05, LOSS: 199.75], G:[ACC: %0.00, LOSS: 374.44]\n",
      "E: 59, D:[ACC: %99.22, LOSS: 200.51], G:[ACC: %0.00, LOSS: 394.63]\n",
      "E: 60, D:[ACC: %99.80, LOSS: 194.81], G:[ACC: %0.39, LOSS: 402.84]\n",
      "E: 61, D:[ACC: %99.61, LOSS: 191.19], G:[ACC: %0.00, LOSS: 415.57]\n",
      "E: 62, D:[ACC: %99.02, LOSS: 193.27], G:[ACC: %0.00, LOSS: 404.82]\n",
      "E: 63, D:[ACC: %99.61, LOSS: 192.82], G:[ACC: %0.00, LOSS: 363.22]\n",
      "E: 64, D:[ACC: %99.02, LOSS: 192.72], G:[ACC: %0.00, LOSS: 346.09]\n",
      "E: 65, D:[ACC: %99.80, LOSS: 194.92], G:[ACC: %1.95, LOSS: 328.01]\n",
      "E: 66, D:[ACC: %99.61, LOSS: 200.50], G:[ACC: %2.73, LOSS: 322.75]\n",
      "E: 67, D:[ACC: %95.12, LOSS: 207.35], G:[ACC: %3.52, LOSS: 324.81]\n",
      "E: 68, D:[ACC: %99.41, LOSS: 195.56], G:[ACC: %0.00, LOSS: 386.93]\n",
      "E: 69, D:[ACC: %100.00, LOSS: 188.01], G:[ACC: %0.00, LOSS: 406.58]\n",
      "E: 70, D:[ACC: %100.00, LOSS: 194.63], G:[ACC: %0.00, LOSS: 368.06]\n",
      "E: 71, D:[ACC: %99.80, LOSS: 196.87], G:[ACC: %0.00, LOSS: 338.21]\n",
      "E: 72, D:[ACC: %100.00, LOSS: 192.43], G:[ACC: %0.00, LOSS: 344.81]\n",
      "E: 73, D:[ACC: %100.00, LOSS: 187.66], G:[ACC: %0.00, LOSS: 356.37]\n",
      "E: 74, D:[ACC: %99.80, LOSS: 181.63], G:[ACC: %0.00, LOSS: 347.29]\n",
      "E: 75, D:[ACC: %100.00, LOSS: 186.00], G:[ACC: %0.39, LOSS: 341.28]\n",
      "E: 76, D:[ACC: %100.00, LOSS: 186.10], G:[ACC: %0.00, LOSS: 355.02]\n",
      "E: 77, D:[ACC: %100.00, LOSS: 176.45], G:[ACC: %0.39, LOSS: 365.05]\n",
      "E: 78, D:[ACC: %100.00, LOSS: 179.71], G:[ACC: %0.39, LOSS: 345.42]\n",
      "E: 79, D:[ACC: %99.80, LOSS: 183.64], G:[ACC: %0.39, LOSS: 362.03]\n",
      "E: 80, D:[ACC: %99.80, LOSS: 180.08], G:[ACC: %0.00, LOSS: 365.71]\n",
      "E: 81, D:[ACC: %99.80, LOSS: 176.70], G:[ACC: %0.39, LOSS: 376.81]\n",
      "E: 82, D:[ACC: %99.61, LOSS: 177.54], G:[ACC: %0.39, LOSS: 366.12]\n",
      "E: 83, D:[ACC: %99.61, LOSS: 179.44], G:[ACC: %0.39, LOSS: 356.01]\n",
      "E: 84, D:[ACC: %100.00, LOSS: 181.95], G:[ACC: %0.78, LOSS: 362.90]\n",
      "E: 85, D:[ACC: %100.00, LOSS: 180.92], G:[ACC: %1.95, LOSS: 344.01]\n",
      "E: 86, D:[ACC: %99.61, LOSS: 172.47], G:[ACC: %1.95, LOSS: 342.55]\n",
      "E: 87, D:[ACC: %100.00, LOSS: 180.68], G:[ACC: %2.34, LOSS: 334.06]\n",
      "E: 88, D:[ACC: %99.61, LOSS: 178.68], G:[ACC: %0.78, LOSS: 332.14]\n",
      "E: 89, D:[ACC: %100.00, LOSS: 167.93], G:[ACC: %2.73, LOSS: 326.99]\n",
      "E: 90, D:[ACC: %100.00, LOSS: 178.48], G:[ACC: %3.12, LOSS: 312.10]\n",
      "E: 91, D:[ACC: %99.22, LOSS: 183.67], G:[ACC: %4.69, LOSS: 304.73]\n",
      "E: 92, D:[ACC: %99.61, LOSS: 173.57], G:[ACC: %3.12, LOSS: 310.82]\n",
      "E: 93, D:[ACC: %99.61, LOSS: 175.98], G:[ACC: %1.95, LOSS: 315.26]\n",
      "E: 94, D:[ACC: %99.41, LOSS: 173.81], G:[ACC: %1.17, LOSS: 327.60]\n",
      "E: 95, D:[ACC: %100.00, LOSS: 165.87], G:[ACC: %1.17, LOSS: 326.30]\n",
      "E: 96, D:[ACC: %100.00, LOSS: 164.84], G:[ACC: %0.00, LOSS: 325.30]\n",
      "E: 97, D:[ACC: %99.80, LOSS: 167.86], G:[ACC: %1.56, LOSS: 343.44]\n",
      "E: 98, D:[ACC: %99.61, LOSS: 168.72], G:[ACC: %0.78, LOSS: 318.40]\n",
      "E: 99, D:[ACC: %99.80, LOSS: 174.38], G:[ACC: %2.73, LOSS: 316.13]\n",
      "E: 100, D:[ACC: %100.00, LOSS: 167.30], G:[ACC: %2.34, LOSS: 301.58]\n",
      "E: 101, D:[ACC: %99.80, LOSS: 163.41], G:[ACC: %4.30, LOSS: 301.25]\n",
      "E: 102, D:[ACC: %99.61, LOSS: 171.76], G:[ACC: %7.03, LOSS: 284.07]\n",
      "E: 103, D:[ACC: %99.41, LOSS: 170.73], G:[ACC: %11.33, LOSS: 280.96]\n",
      "E: 104, D:[ACC: %99.41, LOSS: 169.54], G:[ACC: %7.81, LOSS: 290.86]\n",
      "E: 105, D:[ACC: %99.80, LOSS: 171.66], G:[ACC: %5.86, LOSS: 284.36]\n",
      "E: 106, D:[ACC: %99.41, LOSS: 169.90], G:[ACC: %3.12, LOSS: 302.32]\n",
      "E: 107, D:[ACC: %99.80, LOSS: 165.59], G:[ACC: %4.69, LOSS: 294.53]\n",
      "E: 108, D:[ACC: %99.61, LOSS: 167.00], G:[ACC: %16.41, LOSS: 272.32]\n",
      "E: 109, D:[ACC: %100.00, LOSS: 160.88], G:[ACC: %18.36, LOSS: 275.91]\n",
      "E: 110, D:[ACC: %100.00, LOSS: 162.19], G:[ACC: %15.23, LOSS: 279.89]\n",
      "E: 111, D:[ACC: %100.00, LOSS: 168.20], G:[ACC: %10.55, LOSS: 284.50]\n",
      "E: 112, D:[ACC: %100.00, LOSS: 165.07], G:[ACC: %10.94, LOSS: 286.18]\n",
      "E: 113, D:[ACC: %100.00, LOSS: 172.54], G:[ACC: %10.94, LOSS: 278.96]\n",
      "E: 114, D:[ACC: %99.22, LOSS: 169.25], G:[ACC: %10.55, LOSS: 269.79]\n",
      "E: 115, D:[ACC: %98.44, LOSS: 168.10], G:[ACC: %1.95, LOSS: 311.71]\n",
      "E: 116, D:[ACC: %98.05, LOSS: 171.18], G:[ACC: %2.73, LOSS: 310.73]\n",
      "E: 117, D:[ACC: %94.92, LOSS: 172.64], G:[ACC: %0.00, LOSS: 346.58]\n",
      "E: 118, D:[ACC: %97.66, LOSS: 171.66], G:[ACC: %0.39, LOSS: 368.94]\n",
      "E: 119, D:[ACC: %100.00, LOSS: 167.51], G:[ACC: %0.00, LOSS: 380.99]\n",
      "E: 120, D:[ACC: %98.63, LOSS: 165.24], G:[ACC: %0.39, LOSS: 365.03]\n",
      "E: 121, D:[ACC: %99.02, LOSS: 166.25], G:[ACC: %0.78, LOSS: 388.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 122, D:[ACC: %99.22, LOSS: 165.92], G:[ACC: %0.39, LOSS: 406.05]\n",
      "E: 123, D:[ACC: %99.41, LOSS: 163.92], G:[ACC: %0.78, LOSS: 347.43]\n",
      "E: 124, D:[ACC: %98.24, LOSS: 165.46], G:[ACC: %3.91, LOSS: 323.75]\n",
      "E: 125, D:[ACC: %100.00, LOSS: 157.30], G:[ACC: %0.39, LOSS: 332.52]\n",
      "E: 126, D:[ACC: %100.00, LOSS: 158.45], G:[ACC: %4.69, LOSS: 314.14]\n",
      "E: 127, D:[ACC: %99.80, LOSS: 162.90], G:[ACC: %5.47, LOSS: 286.86]\n",
      "E: 128, D:[ACC: %99.61, LOSS: 158.84], G:[ACC: %10.55, LOSS: 274.32]\n",
      "E: 129, D:[ACC: %99.80, LOSS: 157.26], G:[ACC: %22.66, LOSS: 253.20]\n",
      "E: 130, D:[ACC: %99.80, LOSS: 152.55], G:[ACC: %26.56, LOSS: 239.20]\n",
      "E: 131, D:[ACC: %99.80, LOSS: 154.07], G:[ACC: %33.59, LOSS: 233.79]\n",
      "E: 132, D:[ACC: %99.80, LOSS: 153.67], G:[ACC: %28.52, LOSS: 236.13]\n",
      "E: 133, D:[ACC: %100.00, LOSS: 154.03], G:[ACC: %27.34, LOSS: 244.84]\n",
      "E: 134, D:[ACC: %100.00, LOSS: 149.49], G:[ACC: %25.39, LOSS: 237.32]\n",
      "E: 135, D:[ACC: %100.00, LOSS: 152.77], G:[ACC: %21.09, LOSS: 248.19]\n",
      "E: 136, D:[ACC: %100.00, LOSS: 148.33], G:[ACC: %24.22, LOSS: 256.94]\n",
      "E: 137, D:[ACC: %100.00, LOSS: 150.15], G:[ACC: %20.70, LOSS: 243.92]\n",
      "E: 138, D:[ACC: %100.00, LOSS: 143.67], G:[ACC: %25.39, LOSS: 244.72]\n",
      "E: 139, D:[ACC: %100.00, LOSS: 153.22], G:[ACC: %25.78, LOSS: 246.35]\n",
      "E: 140, D:[ACC: %99.80, LOSS: 143.63], G:[ACC: %24.61, LOSS: 241.60]\n",
      "E: 141, D:[ACC: %99.41, LOSS: 151.13], G:[ACC: %32.42, LOSS: 248.29]\n",
      "E: 142, D:[ACC: %99.41, LOSS: 152.45], G:[ACC: %40.62, LOSS: 228.05]\n",
      "E: 143, D:[ACC: %99.80, LOSS: 155.65], G:[ACC: %24.61, LOSS: 253.43]\n",
      "E: 144, D:[ACC: %100.00, LOSS: 148.46], G:[ACC: %35.55, LOSS: 237.07]\n",
      "E: 145, D:[ACC: %100.00, LOSS: 143.55], G:[ACC: %41.41, LOSS: 222.50]\n",
      "E: 146, D:[ACC: %100.00, LOSS: 145.89], G:[ACC: %56.25, LOSS: 205.23]\n",
      "E: 147, D:[ACC: %100.00, LOSS: 144.28], G:[ACC: %74.61, LOSS: 189.38]\n",
      "E: 148, D:[ACC: %99.80, LOSS: 142.92], G:[ACC: %66.80, LOSS: 200.19]\n",
      "E: 149, D:[ACC: %99.61, LOSS: 138.91], G:[ACC: %57.42, LOSS: 206.29]\n",
      "E: 150, D:[ACC: %100.00, LOSS: 147.38], G:[ACC: %64.45, LOSS: 197.82]\n",
      "E: 151, D:[ACC: %100.00, LOSS: 143.82], G:[ACC: %67.58, LOSS: 192.00]\n",
      "E: 152, D:[ACC: %100.00, LOSS: 137.86], G:[ACC: %78.91, LOSS: 183.95]\n",
      "E: 153, D:[ACC: %100.00, LOSS: 135.50], G:[ACC: %73.83, LOSS: 192.69]\n",
      "E: 154, D:[ACC: %100.00, LOSS: 138.15], G:[ACC: %72.27, LOSS: 183.76]\n",
      "E: 155, D:[ACC: %100.00, LOSS: 134.43], G:[ACC: %76.95, LOSS: 189.33]\n",
      "E: 156, D:[ACC: %100.00, LOSS: 140.36], G:[ACC: %80.47, LOSS: 180.24]\n",
      "E: 157, D:[ACC: %100.00, LOSS: 143.37], G:[ACC: %67.58, LOSS: 195.47]\n",
      "E: 158, D:[ACC: %100.00, LOSS: 136.50], G:[ACC: %65.62, LOSS: 188.69]\n",
      "E: 159, D:[ACC: %100.00, LOSS: 135.94], G:[ACC: %82.81, LOSS: 176.72]\n",
      "E: 160, D:[ACC: %99.80, LOSS: 142.88], G:[ACC: %73.05, LOSS: 186.03]\n",
      "E: 161, D:[ACC: %100.00, LOSS: 137.09], G:[ACC: %78.12, LOSS: 181.16]\n",
      "E: 162, D:[ACC: %100.00, LOSS: 137.61], G:[ACC: %77.73, LOSS: 186.65]\n",
      "E: 163, D:[ACC: %99.80, LOSS: 137.49], G:[ACC: %67.58, LOSS: 194.73]\n",
      "E: 164, D:[ACC: %100.00, LOSS: 142.24], G:[ACC: %72.66, LOSS: 178.82]\n",
      "E: 165, D:[ACC: %100.00, LOSS: 127.25], G:[ACC: %80.86, LOSS: 182.82]\n",
      "E: 166, D:[ACC: %100.00, LOSS: 139.18], G:[ACC: %87.50, LOSS: 172.43]\n",
      "E: 167, D:[ACC: %100.00, LOSS: 127.53], G:[ACC: %88.67, LOSS: 168.69]\n",
      "E: 168, D:[ACC: %100.00, LOSS: 133.50], G:[ACC: %86.33, LOSS: 164.97]\n",
      "E: 169, D:[ACC: %100.00, LOSS: 135.80], G:[ACC: %73.05, LOSS: 201.46]\n",
      "E: 170, D:[ACC: %100.00, LOSS: 139.73], G:[ACC: %75.39, LOSS: 188.14]\n",
      "E: 171, D:[ACC: %99.80, LOSS: 136.22], G:[ACC: %21.88, LOSS: 234.80]\n",
      "E: 172, D:[ACC: %100.00, LOSS: 140.44], G:[ACC: %1.95, LOSS: 318.32]\n",
      "E: 173, D:[ACC: %100.00, LOSS: 130.78], G:[ACC: %1.56, LOSS: 302.47]\n",
      "E: 174, D:[ACC: %100.00, LOSS: 145.49], G:[ACC: %1.56, LOSS: 341.15]\n",
      "E: 175, D:[ACC: %100.00, LOSS: 138.49], G:[ACC: %0.39, LOSS: 404.85]\n",
      "E: 176, D:[ACC: %99.80, LOSS: 132.68], G:[ACC: %0.39, LOSS: 360.79]\n",
      "E: 177, D:[ACC: %99.22, LOSS: 144.78], G:[ACC: %0.78, LOSS: 352.51]\n",
      "E: 178, D:[ACC: %99.80, LOSS: 141.49], G:[ACC: %0.39, LOSS: 386.75]\n",
      "E: 179, D:[ACC: %100.00, LOSS: 137.01], G:[ACC: %1.17, LOSS: 326.84]\n",
      "E: 180, D:[ACC: %100.00, LOSS: 140.34], G:[ACC: %5.47, LOSS: 280.68]\n",
      "E: 181, D:[ACC: %99.61, LOSS: 142.33], G:[ACC: %10.94, LOSS: 257.70]\n",
      "E: 182, D:[ACC: %100.00, LOSS: 137.49], G:[ACC: %33.59, LOSS: 225.01]\n",
      "E: 183, D:[ACC: %100.00, LOSS: 135.74], G:[ACC: %51.17, LOSS: 203.56]\n",
      "E: 184, D:[ACC: %99.61, LOSS: 134.60], G:[ACC: %47.66, LOSS: 202.75]\n",
      "E: 185, D:[ACC: %99.80, LOSS: 136.11], G:[ACC: %41.41, LOSS: 201.44]\n",
      "E: 186, D:[ACC: %99.61, LOSS: 126.26], G:[ACC: %43.75, LOSS: 207.49]\n",
      "E: 187, D:[ACC: %99.80, LOSS: 129.38], G:[ACC: %30.47, LOSS: 220.13]\n",
      "E: 188, D:[ACC: %99.61, LOSS: 128.66], G:[ACC: %67.58, LOSS: 192.25]\n",
      "E: 189, D:[ACC: %99.80, LOSS: 134.06], G:[ACC: %21.09, LOSS: 232.90]\n",
      "E: 190, D:[ACC: %100.00, LOSS: 134.42], G:[ACC: %5.47, LOSS: 257.96]\n",
      "E: 191, D:[ACC: %99.61, LOSS: 131.94], G:[ACC: %26.95, LOSS: 218.85]\n",
      "E: 192, D:[ACC: %100.00, LOSS: 129.47], G:[ACC: %23.44, LOSS: 214.13]\n",
      "E: 193, D:[ACC: %99.80, LOSS: 131.66], G:[ACC: %11.72, LOSS: 250.27]\n",
      "E: 194, D:[ACC: %99.80, LOSS: 125.14], G:[ACC: %23.44, LOSS: 234.99]\n",
      "E: 195, D:[ACC: %100.00, LOSS: 122.54], G:[ACC: %33.59, LOSS: 227.11]\n",
      "E: 196, D:[ACC: %99.80, LOSS: 131.66], G:[ACC: %16.41, LOSS: 248.85]\n",
      "E: 197, D:[ACC: %100.00, LOSS: 126.41], G:[ACC: %45.70, LOSS: 215.76]\n",
      "E: 198, D:[ACC: %99.61, LOSS: 141.85], G:[ACC: %20.70, LOSS: 240.87]\n",
      "E: 199, D:[ACC: %99.61, LOSS: 128.88], G:[ACC: %14.06, LOSS: 264.07]\n",
      "E: 200, D:[ACC: %100.00, LOSS: 135.28], G:[ACC: %35.55, LOSS: 217.47]\n",
      "E: 201, D:[ACC: %99.61, LOSS: 135.73], G:[ACC: %80.08, LOSS: 176.63]\n",
      "E: 202, D:[ACC: %99.80, LOSS: 131.10], G:[ACC: %91.41, LOSS: 159.03]\n",
      "E: 203, D:[ACC: %99.41, LOSS: 131.78], G:[ACC: %95.70, LOSS: 143.38]\n",
      "E: 204, D:[ACC: %100.00, LOSS: 126.01], G:[ACC: %98.44, LOSS: 139.29]\n",
      "E: 205, D:[ACC: %100.00, LOSS: 122.65], G:[ACC: %97.66, LOSS: 145.21]\n",
      "E: 206, D:[ACC: %100.00, LOSS: 121.65], G:[ACC: %98.83, LOSS: 140.14]\n",
      "E: 207, D:[ACC: %100.00, LOSS: 117.03], G:[ACC: %98.83, LOSS: 133.66]\n",
      "E: 208, D:[ACC: %99.41, LOSS: 131.20], G:[ACC: %100.00, LOSS: 131.28]\n",
      "E: 209, D:[ACC: %100.00, LOSS: 124.70], G:[ACC: %98.83, LOSS: 132.62]\n",
      "E: 210, D:[ACC: %100.00, LOSS: 119.51], G:[ACC: %99.61, LOSS: 126.32]\n",
      "E: 211, D:[ACC: %99.80, LOSS: 120.67], G:[ACC: %99.61, LOSS: 133.89]\n",
      "E: 212, D:[ACC: %99.80, LOSS: 117.61], G:[ACC: %99.22, LOSS: 131.59]\n",
      "E: 213, D:[ACC: %100.00, LOSS: 116.30], G:[ACC: %100.00, LOSS: 131.09]\n",
      "E: 214, D:[ACC: %100.00, LOSS: 118.15], G:[ACC: %99.22, LOSS: 128.90]\n",
      "E: 215, D:[ACC: %100.00, LOSS: 112.14], G:[ACC: %99.22, LOSS: 128.71]\n",
      "E: 216, D:[ACC: %100.00, LOSS: 118.72], G:[ACC: %98.44, LOSS: 130.49]\n",
      "E: 217, D:[ACC: %100.00, LOSS: 115.48], G:[ACC: %98.44, LOSS: 131.31]\n",
      "E: 218, D:[ACC: %100.00, LOSS: 110.73], G:[ACC: %99.22, LOSS: 129.14]\n",
      "E: 219, D:[ACC: %99.80, LOSS: 121.73], G:[ACC: %98.44, LOSS: 131.16]\n",
      "E: 220, D:[ACC: %100.00, LOSS: 118.06], G:[ACC: %100.00, LOSS: 126.28]\n",
      "E: 221, D:[ACC: %100.00, LOSS: 121.33], G:[ACC: %99.61, LOSS: 122.38]\n",
      "E: 222, D:[ACC: %100.00, LOSS: 112.42], G:[ACC: %99.61, LOSS: 130.42]\n",
      "E: 223, D:[ACC: %100.00, LOSS: 114.06], G:[ACC: %98.44, LOSS: 125.27]\n",
      "E: 224, D:[ACC: %100.00, LOSS: 119.94], G:[ACC: %100.00, LOSS: 128.71]\n",
      "E: 225, D:[ACC: %100.00, LOSS: 113.51], G:[ACC: %98.83, LOSS: 123.36]\n",
      "E: 226, D:[ACC: %100.00, LOSS: 113.74], G:[ACC: %98.44, LOSS: 125.44]\n",
      "E: 227, D:[ACC: %99.80, LOSS: 116.62], G:[ACC: %99.22, LOSS: 130.06]\n",
      "E: 228, D:[ACC: %100.00, LOSS: 116.80], G:[ACC: %99.22, LOSS: 125.76]\n",
      "E: 229, D:[ACC: %100.00, LOSS: 113.73], G:[ACC: %98.83, LOSS: 128.56]\n",
      "E: 230, D:[ACC: %100.00, LOSS: 108.27], G:[ACC: %100.00, LOSS: 130.14]\n",
      "E: 231, D:[ACC: %100.00, LOSS: 114.13], G:[ACC: %98.83, LOSS: 129.39]\n",
      "E: 232, D:[ACC: %100.00, LOSS: 109.49], G:[ACC: %98.44, LOSS: 126.61]\n",
      "E: 233, D:[ACC: %100.00, LOSS: 111.03], G:[ACC: %99.22, LOSS: 124.00]\n",
      "E: 234, D:[ACC: %100.00, LOSS: 108.14], G:[ACC: %99.22, LOSS: 128.61]\n",
      "E: 235, D:[ACC: %100.00, LOSS: 112.60], G:[ACC: %99.22, LOSS: 129.26]\n",
      "E: 236, D:[ACC: %100.00, LOSS: 109.10], G:[ACC: %99.22, LOSS: 128.18]\n",
      "E: 237, D:[ACC: %100.00, LOSS: 118.18], G:[ACC: %99.61, LOSS: 127.84]\n",
      "E: 238, D:[ACC: %100.00, LOSS: 106.03], G:[ACC: %99.22, LOSS: 132.88]\n",
      "E: 239, D:[ACC: %100.00, LOSS: 110.57], G:[ACC: %99.61, LOSS: 128.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 240, D:[ACC: %100.00, LOSS: 115.15], G:[ACC: %99.61, LOSS: 133.61]\n",
      "E: 241, D:[ACC: %100.00, LOSS: 111.49], G:[ACC: %99.61, LOSS: 131.48]\n",
      "E: 242, D:[ACC: %100.00, LOSS: 111.07], G:[ACC: %98.83, LOSS: 120.00]\n",
      "E: 243, D:[ACC: %100.00, LOSS: 106.17], G:[ACC: %100.00, LOSS: 122.74]\n",
      "E: 244, D:[ACC: %100.00, LOSS: 110.59], G:[ACC: %98.44, LOSS: 133.27]\n",
      "E: 245, D:[ACC: %99.80, LOSS: 114.58], G:[ACC: %99.61, LOSS: 123.83]\n",
      "E: 246, D:[ACC: %100.00, LOSS: 111.88], G:[ACC: %98.83, LOSS: 126.66]\n",
      "E: 247, D:[ACC: %100.00, LOSS: 108.28], G:[ACC: %98.44, LOSS: 129.74]\n",
      "E: 248, D:[ACC: %100.00, LOSS: 113.63], G:[ACC: %98.83, LOSS: 123.43]\n",
      "E: 249, D:[ACC: %99.80, LOSS: 106.05], G:[ACC: %98.44, LOSS: 128.86]\n",
      "E: 250, D:[ACC: %100.00, LOSS: 105.63], G:[ACC: %99.22, LOSS: 124.37]\n",
      "E: 251, D:[ACC: %100.00, LOSS: 110.81], G:[ACC: %98.83, LOSS: 123.47]\n",
      "E: 252, D:[ACC: %100.00, LOSS: 111.87], G:[ACC: %99.22, LOSS: 121.73]\n",
      "E: 253, D:[ACC: %100.00, LOSS: 108.78], G:[ACC: %96.88, LOSS: 127.90]\n",
      "E: 254, D:[ACC: %100.00, LOSS: 110.24], G:[ACC: %98.83, LOSS: 127.26]\n",
      "E: 255, D:[ACC: %100.00, LOSS: 110.50], G:[ACC: %76.95, LOSS: 161.04]\n",
      "E: 256, D:[ACC: %100.00, LOSS: 111.04], G:[ACC: %30.86, LOSS: 213.33]\n",
      "E: 257, D:[ACC: %100.00, LOSS: 109.19], G:[ACC: %26.56, LOSS: 198.11]\n",
      "E: 258, D:[ACC: %99.61, LOSS: 127.10], G:[ACC: %0.00, LOSS: 352.20]\n",
      "E: 259, D:[ACC: %99.41, LOSS: 112.51], G:[ACC: %0.00, LOSS: 360.17]\n",
      "E: 260, D:[ACC: %100.00, LOSS: 111.38], G:[ACC: %0.00, LOSS: 350.58]\n",
      "E: 261, D:[ACC: %100.00, LOSS: 107.82], G:[ACC: %59.77, LOSS: 188.86]\n",
      "E: 262, D:[ACC: %100.00, LOSS: 105.10], G:[ACC: %51.95, LOSS: 189.17]\n",
      "E: 263, D:[ACC: %100.00, LOSS: 112.29], G:[ACC: %96.88, LOSS: 129.08]\n",
      "E: 264, D:[ACC: %100.00, LOSS: 109.64], G:[ACC: %93.36, LOSS: 140.17]\n",
      "E: 265, D:[ACC: %100.00, LOSS: 106.81], G:[ACC: %94.92, LOSS: 136.78]\n",
      "E: 266, D:[ACC: %100.00, LOSS: 102.73], G:[ACC: %98.05, LOSS: 134.30]\n",
      "E: 267, D:[ACC: %100.00, LOSS: 102.21], G:[ACC: %96.09, LOSS: 129.72]\n",
      "E: 268, D:[ACC: %99.80, LOSS: 114.95], G:[ACC: %98.83, LOSS: 124.73]\n",
      "E: 269, D:[ACC: %100.00, LOSS: 101.97], G:[ACC: %99.22, LOSS: 125.22]\n",
      "E: 270, D:[ACC: %100.00, LOSS: 102.23], G:[ACC: %100.00, LOSS: 118.25]\n",
      "E: 271, D:[ACC: %100.00, LOSS: 99.94], G:[ACC: %100.00, LOSS: 114.52]\n",
      "E: 272, D:[ACC: %100.00, LOSS: 105.00], G:[ACC: %99.61, LOSS: 115.30]\n",
      "E: 273, D:[ACC: %100.00, LOSS: 98.76], G:[ACC: %99.61, LOSS: 108.57]\n",
      "E: 274, D:[ACC: %100.00, LOSS: 102.97], G:[ACC: %100.00, LOSS: 104.16]\n",
      "E: 275, D:[ACC: %100.00, LOSS: 108.05], G:[ACC: %100.00, LOSS: 115.99]\n",
      "E: 276, D:[ACC: %99.80, LOSS: 104.31], G:[ACC: %100.00, LOSS: 104.50]\n",
      "E: 277, D:[ACC: %100.00, LOSS: 105.06], G:[ACC: %99.61, LOSS: 105.55]\n",
      "E: 278, D:[ACC: %100.00, LOSS: 105.41], G:[ACC: %100.00, LOSS: 109.59]\n",
      "E: 279, D:[ACC: %100.00, LOSS: 104.70], G:[ACC: %99.61, LOSS: 107.32]\n",
      "E: 280, D:[ACC: %100.00, LOSS: 103.55], G:[ACC: %100.00, LOSS: 111.66]\n",
      "E: 281, D:[ACC: %100.00, LOSS: 102.46], G:[ACC: %99.22, LOSS: 106.53]\n",
      "E: 282, D:[ACC: %100.00, LOSS: 99.53], G:[ACC: %100.00, LOSS: 109.53]\n",
      "E: 283, D:[ACC: %100.00, LOSS: 101.91], G:[ACC: %99.61, LOSS: 106.94]\n",
      "E: 284, D:[ACC: %100.00, LOSS: 104.36], G:[ACC: %100.00, LOSS: 107.09]\n",
      "E: 285, D:[ACC: %100.00, LOSS: 106.69], G:[ACC: %99.61, LOSS: 102.93]\n",
      "E: 286, D:[ACC: %100.00, LOSS: 94.87], G:[ACC: %100.00, LOSS: 105.03]\n",
      "E: 287, D:[ACC: %100.00, LOSS: 95.99], G:[ACC: %100.00, LOSS: 108.97]\n",
      "E: 288, D:[ACC: %100.00, LOSS: 97.49], G:[ACC: %99.61, LOSS: 116.05]\n",
      "E: 289, D:[ACC: %100.00, LOSS: 103.84], G:[ACC: %100.00, LOSS: 110.11]\n",
      "E: 290, D:[ACC: %100.00, LOSS: 103.80], G:[ACC: %100.00, LOSS: 109.67]\n",
      "E: 291, D:[ACC: %100.00, LOSS: 97.14], G:[ACC: %99.22, LOSS: 115.26]\n",
      "E: 292, D:[ACC: %100.00, LOSS: 98.11], G:[ACC: %100.00, LOSS: 115.01]\n",
      "E: 293, D:[ACC: %99.80, LOSS: 105.67], G:[ACC: %99.61, LOSS: 110.82]\n",
      "E: 294, D:[ACC: %100.00, LOSS: 103.45], G:[ACC: %99.61, LOSS: 107.85]\n",
      "E: 295, D:[ACC: %100.00, LOSS: 98.76], G:[ACC: %100.00, LOSS: 106.22]\n",
      "E: 296, D:[ACC: %100.00, LOSS: 94.80], G:[ACC: %99.61, LOSS: 107.20]\n",
      "E: 297, D:[ACC: %99.80, LOSS: 111.04], G:[ACC: %100.00, LOSS: 103.62]\n",
      "E: 298, D:[ACC: %100.00, LOSS: 94.73], G:[ACC: %100.00, LOSS: 109.70]\n",
      "E: 299, D:[ACC: %100.00, LOSS: 94.89], G:[ACC: %100.00, LOSS: 102.63]\n",
      "E: 300, D:[ACC: %99.80, LOSS: 90.43], G:[ACC: %100.00, LOSS: 105.63]\n",
      "E: 301, D:[ACC: %100.00, LOSS: 96.10], G:[ACC: %100.00, LOSS: 106.54]\n",
      "E: 302, D:[ACC: %100.00, LOSS: 102.53], G:[ACC: %99.61, LOSS: 103.32]\n",
      "E: 303, D:[ACC: %100.00, LOSS: 97.40], G:[ACC: %100.00, LOSS: 102.81]\n",
      "E: 304, D:[ACC: %100.00, LOSS: 101.95], G:[ACC: %100.00, LOSS: 98.23]\n",
      "E: 305, D:[ACC: %100.00, LOSS: 98.04], G:[ACC: %100.00, LOSS: 101.94]\n",
      "E: 306, D:[ACC: %100.00, LOSS: 96.57], G:[ACC: %100.00, LOSS: 102.21]\n",
      "E: 307, D:[ACC: %100.00, LOSS: 95.19], G:[ACC: %99.61, LOSS: 107.35]\n",
      "E: 308, D:[ACC: %100.00, LOSS: 92.87], G:[ACC: %100.00, LOSS: 101.24]\n",
      "E: 309, D:[ACC: %100.00, LOSS: 88.49], G:[ACC: %100.00, LOSS: 100.64]\n",
      "E: 310, D:[ACC: %100.00, LOSS: 105.18], G:[ACC: %100.00, LOSS: 100.03]\n",
      "E: 311, D:[ACC: %100.00, LOSS: 92.72], G:[ACC: %100.00, LOSS: 104.42]\n",
      "E: 312, D:[ACC: %100.00, LOSS: 97.00], G:[ACC: %100.00, LOSS: 106.35]\n",
      "E: 313, D:[ACC: %100.00, LOSS: 96.55], G:[ACC: %100.00, LOSS: 100.11]\n",
      "E: 314, D:[ACC: %100.00, LOSS: 97.55], G:[ACC: %100.00, LOSS: 99.34]\n",
      "E: 315, D:[ACC: %100.00, LOSS: 99.72], G:[ACC: %100.00, LOSS: 103.49]\n",
      "E: 316, D:[ACC: %100.00, LOSS: 99.94], G:[ACC: %100.00, LOSS: 95.46]\n",
      "E: 317, D:[ACC: %100.00, LOSS: 103.97], G:[ACC: %100.00, LOSS: 101.18]\n",
      "E: 318, D:[ACC: %100.00, LOSS: 92.68], G:[ACC: %100.00, LOSS: 102.48]\n",
      "E: 319, D:[ACC: %100.00, LOSS: 95.84], G:[ACC: %100.00, LOSS: 97.79]\n",
      "E: 320, D:[ACC: %100.00, LOSS: 97.21], G:[ACC: %100.00, LOSS: 100.04]\n",
      "E: 321, D:[ACC: %100.00, LOSS: 95.73], G:[ACC: %99.61, LOSS: 100.27]\n",
      "E: 322, D:[ACC: %100.00, LOSS: 96.87], G:[ACC: %100.00, LOSS: 104.11]\n",
      "E: 323, D:[ACC: %100.00, LOSS: 93.30], G:[ACC: %100.00, LOSS: 99.29]\n",
      "E: 324, D:[ACC: %100.00, LOSS: 94.86], G:[ACC: %100.00, LOSS: 100.40]\n",
      "E: 325, D:[ACC: %100.00, LOSS: 95.35], G:[ACC: %100.00, LOSS: 99.33]\n",
      "E: 326, D:[ACC: %100.00, LOSS: 93.09], G:[ACC: %100.00, LOSS: 97.70]\n",
      "E: 327, D:[ACC: %100.00, LOSS: 85.71], G:[ACC: %100.00, LOSS: 96.46]\n",
      "E: 328, D:[ACC: %100.00, LOSS: 89.85], G:[ACC: %100.00, LOSS: 97.72]\n",
      "E: 329, D:[ACC: %100.00, LOSS: 88.39], G:[ACC: %100.00, LOSS: 92.42]\n",
      "E: 330, D:[ACC: %100.00, LOSS: 88.01], G:[ACC: %100.00, LOSS: 103.17]\n",
      "E: 331, D:[ACC: %100.00, LOSS: 98.05], G:[ACC: %100.00, LOSS: 95.96]\n",
      "E: 332, D:[ACC: %100.00, LOSS: 91.21], G:[ACC: %100.00, LOSS: 91.48]\n",
      "E: 333, D:[ACC: %100.00, LOSS: 88.55], G:[ACC: %100.00, LOSS: 93.98]\n",
      "E: 334, D:[ACC: %100.00, LOSS: 91.13], G:[ACC: %100.00, LOSS: 93.09]\n",
      "E: 335, D:[ACC: %100.00, LOSS: 93.92], G:[ACC: %99.61, LOSS: 98.14]\n",
      "E: 336, D:[ACC: %100.00, LOSS: 87.36], G:[ACC: %99.61, LOSS: 97.82]\n",
      "E: 337, D:[ACC: %100.00, LOSS: 89.03], G:[ACC: %100.00, LOSS: 97.78]\n",
      "E: 338, D:[ACC: %100.00, LOSS: 89.84], G:[ACC: %100.00, LOSS: 94.14]\n",
      "E: 339, D:[ACC: %100.00, LOSS: 85.50], G:[ACC: %100.00, LOSS: 98.99]\n",
      "E: 340, D:[ACC: %100.00, LOSS: 93.21], G:[ACC: %100.00, LOSS: 93.06]\n",
      "E: 341, D:[ACC: %100.00, LOSS: 91.61], G:[ACC: %99.61, LOSS: 90.22]\n",
      "E: 342, D:[ACC: %100.00, LOSS: 85.57], G:[ACC: %100.00, LOSS: 103.11]\n",
      "E: 343, D:[ACC: %100.00, LOSS: 94.20], G:[ACC: %100.00, LOSS: 102.45]\n",
      "E: 344, D:[ACC: %100.00, LOSS: 91.86], G:[ACC: %100.00, LOSS: 94.61]\n",
      "E: 345, D:[ACC: %100.00, LOSS: 94.13], G:[ACC: %100.00, LOSS: 88.30]\n",
      "E: 346, D:[ACC: %100.00, LOSS: 96.16], G:[ACC: %100.00, LOSS: 94.16]\n",
      "E: 347, D:[ACC: %100.00, LOSS: 89.99], G:[ACC: %100.00, LOSS: 85.33]\n",
      "E: 348, D:[ACC: %100.00, LOSS: 86.63], G:[ACC: %100.00, LOSS: 90.19]\n",
      "E: 349, D:[ACC: %100.00, LOSS: 86.36], G:[ACC: %100.00, LOSS: 85.53]\n",
      "E: 350, D:[ACC: %100.00, LOSS: 88.44], G:[ACC: %100.00, LOSS: 95.52]\n",
      "E: 351, D:[ACC: %100.00, LOSS: 89.46], G:[ACC: %100.00, LOSS: 87.16]\n",
      "E: 352, D:[ACC: %100.00, LOSS: 86.80], G:[ACC: %100.00, LOSS: 95.15]\n",
      "E: 353, D:[ACC: %100.00, LOSS: 87.65], G:[ACC: %100.00, LOSS: 90.92]\n",
      "E: 354, D:[ACC: %100.00, LOSS: 86.74], G:[ACC: %100.00, LOSS: 85.32]\n",
      "E: 355, D:[ACC: %100.00, LOSS: 84.44], G:[ACC: %100.00, LOSS: 83.83]\n",
      "E: 356, D:[ACC: %100.00, LOSS: 83.18], G:[ACC: %100.00, LOSS: 89.98]\n",
      "E: 357, D:[ACC: %100.00, LOSS: 84.45], G:[ACC: %100.00, LOSS: 95.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 358, D:[ACC: %100.00, LOSS: 86.02], G:[ACC: %100.00, LOSS: 82.25]\n",
      "E: 359, D:[ACC: %100.00, LOSS: 88.66], G:[ACC: %100.00, LOSS: 86.24]\n",
      "E: 360, D:[ACC: %100.00, LOSS: 85.96], G:[ACC: %100.00, LOSS: 89.68]\n",
      "E: 361, D:[ACC: %100.00, LOSS: 88.36], G:[ACC: %100.00, LOSS: 84.68]\n",
      "E: 362, D:[ACC: %100.00, LOSS: 83.11], G:[ACC: %100.00, LOSS: 84.76]\n",
      "E: 363, D:[ACC: %100.00, LOSS: 86.38], G:[ACC: %100.00, LOSS: 90.21]\n",
      "E: 364, D:[ACC: %100.00, LOSS: 83.96], G:[ACC: %100.00, LOSS: 86.27]\n",
      "E: 365, D:[ACC: %100.00, LOSS: 87.17], G:[ACC: %100.00, LOSS: 85.47]\n",
      "E: 366, D:[ACC: %100.00, LOSS: 87.54], G:[ACC: %100.00, LOSS: 87.16]\n",
      "E: 367, D:[ACC: %100.00, LOSS: 91.04], G:[ACC: %100.00, LOSS: 90.19]\n",
      "E: 368, D:[ACC: %100.00, LOSS: 86.83], G:[ACC: %100.00, LOSS: 87.36]\n",
      "E: 369, D:[ACC: %100.00, LOSS: 87.53], G:[ACC: %100.00, LOSS: 95.16]\n",
      "E: 370, D:[ACC: %100.00, LOSS: 87.78], G:[ACC: %100.00, LOSS: 93.21]\n",
      "E: 371, D:[ACC: %100.00, LOSS: 87.51], G:[ACC: %100.00, LOSS: 82.46]\n",
      "E: 372, D:[ACC: %99.80, LOSS: 90.33], G:[ACC: %100.00, LOSS: 84.20]\n",
      "E: 373, D:[ACC: %100.00, LOSS: 87.41], G:[ACC: %100.00, LOSS: 85.54]\n",
      "E: 374, D:[ACC: %100.00, LOSS: 93.11], G:[ACC: %100.00, LOSS: 84.91]\n",
      "E: 375, D:[ACC: %100.00, LOSS: 87.02], G:[ACC: %100.00, LOSS: 87.80]\n",
      "E: 376, D:[ACC: %100.00, LOSS: 83.36], G:[ACC: %100.00, LOSS: 84.52]\n",
      "E: 377, D:[ACC: %100.00, LOSS: 85.52], G:[ACC: %100.00, LOSS: 90.81]\n",
      "E: 378, D:[ACC: %100.00, LOSS: 85.25], G:[ACC: %100.00, LOSS: 89.28]\n",
      "E: 379, D:[ACC: %100.00, LOSS: 81.61], G:[ACC: %100.00, LOSS: 88.21]\n",
      "E: 380, D:[ACC: %100.00, LOSS: 88.53], G:[ACC: %100.00, LOSS: 89.76]\n",
      "E: 381, D:[ACC: %100.00, LOSS: 82.11], G:[ACC: %100.00, LOSS: 85.05]\n",
      "E: 382, D:[ACC: %100.00, LOSS: 83.15], G:[ACC: %100.00, LOSS: 81.41]\n",
      "E: 383, D:[ACC: %100.00, LOSS: 83.19], G:[ACC: %100.00, LOSS: 86.30]\n",
      "E: 384, D:[ACC: %100.00, LOSS: 82.84], G:[ACC: %100.00, LOSS: 83.36]\n",
      "E: 385, D:[ACC: %100.00, LOSS: 84.94], G:[ACC: %100.00, LOSS: 82.91]\n",
      "E: 386, D:[ACC: %100.00, LOSS: 78.35], G:[ACC: %100.00, LOSS: 81.07]\n",
      "E: 387, D:[ACC: %100.00, LOSS: 82.69], G:[ACC: %100.00, LOSS: 81.20]\n",
      "E: 388, D:[ACC: %100.00, LOSS: 78.36], G:[ACC: %100.00, LOSS: 81.65]\n",
      "E: 389, D:[ACC: %100.00, LOSS: 79.29], G:[ACC: %100.00, LOSS: 84.33]\n",
      "E: 390, D:[ACC: %100.00, LOSS: 77.90], G:[ACC: %100.00, LOSS: 86.41]\n",
      "E: 391, D:[ACC: %100.00, LOSS: 78.41], G:[ACC: %100.00, LOSS: 83.23]\n",
      "E: 392, D:[ACC: %100.00, LOSS: 80.90], G:[ACC: %100.00, LOSS: 81.34]\n",
      "E: 393, D:[ACC: %100.00, LOSS: 84.80], G:[ACC: %100.00, LOSS: 79.23]\n",
      "E: 394, D:[ACC: %100.00, LOSS: 87.03], G:[ACC: %100.00, LOSS: 80.18]\n",
      "E: 395, D:[ACC: %100.00, LOSS: 76.85], G:[ACC: %100.00, LOSS: 79.90]\n",
      "E: 396, D:[ACC: %100.00, LOSS: 79.27], G:[ACC: %100.00, LOSS: 82.50]\n",
      "E: 397, D:[ACC: %100.00, LOSS: 79.65], G:[ACC: %100.00, LOSS: 82.91]\n",
      "E: 398, D:[ACC: %100.00, LOSS: 83.92], G:[ACC: %100.00, LOSS: 82.30]\n",
      "E: 399, D:[ACC: %100.00, LOSS: 77.13], G:[ACC: %100.00, LOSS: 82.84]\n",
      "E: 400, D:[ACC: %100.00, LOSS: 80.40], G:[ACC: %100.00, LOSS: 83.07]\n",
      "E: 401, D:[ACC: %100.00, LOSS: 81.85], G:[ACC: %100.00, LOSS: 83.87]\n",
      "E: 402, D:[ACC: %100.00, LOSS: 76.89], G:[ACC: %100.00, LOSS: 81.59]\n",
      "E: 403, D:[ACC: %100.00, LOSS: 78.70], G:[ACC: %100.00, LOSS: 85.14]\n",
      "E: 404, D:[ACC: %100.00, LOSS: 79.60], G:[ACC: %100.00, LOSS: 80.59]\n",
      "E: 405, D:[ACC: %100.00, LOSS: 75.51], G:[ACC: %100.00, LOSS: 80.61]\n",
      "E: 406, D:[ACC: %100.00, LOSS: 84.73], G:[ACC: %100.00, LOSS: 84.66]\n",
      "E: 407, D:[ACC: %100.00, LOSS: 81.48], G:[ACC: %100.00, LOSS: 80.87]\n",
      "E: 408, D:[ACC: %100.00, LOSS: 76.33], G:[ACC: %100.00, LOSS: 78.30]\n",
      "E: 409, D:[ACC: %100.00, LOSS: 76.59], G:[ACC: %100.00, LOSS: 73.62]\n",
      "E: 410, D:[ACC: %100.00, LOSS: 79.48], G:[ACC: %100.00, LOSS: 79.44]\n",
      "E: 411, D:[ACC: %100.00, LOSS: 79.39], G:[ACC: %100.00, LOSS: 75.66]\n",
      "E: 412, D:[ACC: %100.00, LOSS: 74.37], G:[ACC: %100.00, LOSS: 78.53]\n",
      "E: 413, D:[ACC: %100.00, LOSS: 77.17], G:[ACC: %100.00, LOSS: 80.31]\n",
      "E: 414, D:[ACC: %100.00, LOSS: 79.33], G:[ACC: %100.00, LOSS: 79.39]\n",
      "E: 415, D:[ACC: %100.00, LOSS: 76.73], G:[ACC: %100.00, LOSS: 80.67]\n",
      "E: 416, D:[ACC: %100.00, LOSS: 74.23], G:[ACC: %100.00, LOSS: 74.96]\n",
      "E: 417, D:[ACC: %100.00, LOSS: 75.31], G:[ACC: %100.00, LOSS: 75.44]\n",
      "E: 418, D:[ACC: %100.00, LOSS: 75.90], G:[ACC: %100.00, LOSS: 78.38]\n",
      "E: 419, D:[ACC: %100.00, LOSS: 80.83], G:[ACC: %100.00, LOSS: 78.31]\n",
      "E: 420, D:[ACC: %100.00, LOSS: 72.48], G:[ACC: %100.00, LOSS: 77.53]\n",
      "E: 421, D:[ACC: %100.00, LOSS: 71.15], G:[ACC: %100.00, LOSS: 75.47]\n",
      "E: 422, D:[ACC: %100.00, LOSS: 77.05], G:[ACC: %100.00, LOSS: 78.15]\n",
      "E: 423, D:[ACC: %100.00, LOSS: 71.99], G:[ACC: %100.00, LOSS: 73.60]\n",
      "E: 424, D:[ACC: %100.00, LOSS: 78.07], G:[ACC: %100.00, LOSS: 77.00]\n",
      "E: 425, D:[ACC: %100.00, LOSS: 76.97], G:[ACC: %100.00, LOSS: 79.20]\n",
      "E: 426, D:[ACC: %100.00, LOSS: 81.78], G:[ACC: %100.00, LOSS: 77.90]\n",
      "E: 427, D:[ACC: %100.00, LOSS: 78.60], G:[ACC: %100.00, LOSS: 79.95]\n",
      "E: 428, D:[ACC: %99.61, LOSS: 73.54], G:[ACC: %100.00, LOSS: 81.55]\n",
      "E: 429, D:[ACC: %100.00, LOSS: 76.86], G:[ACC: %100.00, LOSS: 79.45]\n",
      "E: 430, D:[ACC: %100.00, LOSS: 79.54], G:[ACC: %100.00, LOSS: 86.88]\n",
      "E: 431, D:[ACC: %100.00, LOSS: 78.07], G:[ACC: %100.00, LOSS: 77.90]\n",
      "E: 432, D:[ACC: %100.00, LOSS: 77.93], G:[ACC: %100.00, LOSS: 77.08]\n",
      "E: 433, D:[ACC: %100.00, LOSS: 71.50], G:[ACC: %100.00, LOSS: 75.27]\n",
      "E: 434, D:[ACC: %100.00, LOSS: 69.68], G:[ACC: %100.00, LOSS: 80.90]\n",
      "E: 435, D:[ACC: %100.00, LOSS: 77.05], G:[ACC: %100.00, LOSS: 81.17]\n",
      "E: 436, D:[ACC: %100.00, LOSS: 78.22], G:[ACC: %100.00, LOSS: 77.14]\n",
      "E: 437, D:[ACC: %100.00, LOSS: 71.99], G:[ACC: %100.00, LOSS: 75.20]\n",
      "E: 438, D:[ACC: %100.00, LOSS: 73.36], G:[ACC: %100.00, LOSS: 71.04]\n",
      "E: 439, D:[ACC: %100.00, LOSS: 71.76], G:[ACC: %100.00, LOSS: 73.60]\n",
      "E: 440, D:[ACC: %100.00, LOSS: 76.16], G:[ACC: %100.00, LOSS: 77.10]\n",
      "E: 441, D:[ACC: %100.00, LOSS: 73.24], G:[ACC: %100.00, LOSS: 75.00]\n",
      "E: 442, D:[ACC: %100.00, LOSS: 76.03], G:[ACC: %100.00, LOSS: 77.37]\n",
      "E: 443, D:[ACC: %100.00, LOSS: 71.53], G:[ACC: %100.00, LOSS: 77.63]\n",
      "E: 444, D:[ACC: %100.00, LOSS: 77.76], G:[ACC: %100.00, LOSS: 76.07]\n",
      "E: 445, D:[ACC: %100.00, LOSS: 76.52], G:[ACC: %100.00, LOSS: 69.49]\n",
      "E: 446, D:[ACC: %100.00, LOSS: 75.59], G:[ACC: %100.00, LOSS: 72.88]\n",
      "E: 447, D:[ACC: %100.00, LOSS: 72.34], G:[ACC: %100.00, LOSS: 70.58]\n",
      "E: 448, D:[ACC: %100.00, LOSS: 72.39], G:[ACC: %100.00, LOSS: 69.85]\n",
      "E: 449, D:[ACC: %100.00, LOSS: 74.06], G:[ACC: %100.00, LOSS: 71.17]\n",
      "E: 450, D:[ACC: %100.00, LOSS: 73.18], G:[ACC: %100.00, LOSS: 77.44]\n",
      "E: 451, D:[ACC: %100.00, LOSS: 72.55], G:[ACC: %100.00, LOSS: 71.38]\n",
      "E: 452, D:[ACC: %100.00, LOSS: 71.78], G:[ACC: %100.00, LOSS: 72.64]\n",
      "E: 453, D:[ACC: %100.00, LOSS: 70.77], G:[ACC: %100.00, LOSS: 72.11]\n",
      "E: 454, D:[ACC: %100.00, LOSS: 71.55], G:[ACC: %100.00, LOSS: 79.25]\n",
      "E: 455, D:[ACC: %100.00, LOSS: 70.69], G:[ACC: %100.00, LOSS: 74.53]\n",
      "E: 456, D:[ACC: %100.00, LOSS: 72.80], G:[ACC: %100.00, LOSS: 74.60]\n",
      "E: 457, D:[ACC: %99.80, LOSS: 72.41], G:[ACC: %100.00, LOSS: 69.63]\n",
      "E: 458, D:[ACC: %100.00, LOSS: 73.98], G:[ACC: %100.00, LOSS: 73.11]\n",
      "E: 459, D:[ACC: %100.00, LOSS: 68.47], G:[ACC: %100.00, LOSS: 74.91]\n",
      "E: 460, D:[ACC: %100.00, LOSS: 70.25], G:[ACC: %100.00, LOSS: 74.84]\n",
      "E: 461, D:[ACC: %100.00, LOSS: 70.04], G:[ACC: %100.00, LOSS: 71.31]\n",
      "E: 462, D:[ACC: %100.00, LOSS: 72.73], G:[ACC: %100.00, LOSS: 73.38]\n",
      "E: 463, D:[ACC: %100.00, LOSS: 71.03], G:[ACC: %100.00, LOSS: 68.21]\n",
      "E: 464, D:[ACC: %100.00, LOSS: 75.27], G:[ACC: %100.00, LOSS: 70.88]\n",
      "E: 465, D:[ACC: %100.00, LOSS: 73.78], G:[ACC: %100.00, LOSS: 71.14]\n",
      "E: 466, D:[ACC: %100.00, LOSS: 71.11], G:[ACC: %100.00, LOSS: 69.32]\n",
      "E: 467, D:[ACC: %100.00, LOSS: 72.67], G:[ACC: %100.00, LOSS: 73.71]\n",
      "E: 468, D:[ACC: %100.00, LOSS: 69.39], G:[ACC: %100.00, LOSS: 70.65]\n",
      "E: 469, D:[ACC: %100.00, LOSS: 71.98], G:[ACC: %100.00, LOSS: 70.25]\n",
      "E: 470, D:[ACC: %100.00, LOSS: 68.99], G:[ACC: %100.00, LOSS: 66.38]\n",
      "E: 471, D:[ACC: %100.00, LOSS: 69.22], G:[ACC: %100.00, LOSS: 69.66]\n",
      "E: 472, D:[ACC: %100.00, LOSS: 67.02], G:[ACC: %100.00, LOSS: 69.09]\n",
      "E: 473, D:[ACC: %100.00, LOSS: 66.21], G:[ACC: %100.00, LOSS: 74.01]\n",
      "E: 474, D:[ACC: %100.00, LOSS: 70.31], G:[ACC: %100.00, LOSS: 71.73]\n",
      "E: 475, D:[ACC: %100.00, LOSS: 68.06], G:[ACC: %100.00, LOSS: 69.72]\n",
      "E: 476, D:[ACC: %100.00, LOSS: 68.09], G:[ACC: %100.00, LOSS: 71.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 477, D:[ACC: %100.00, LOSS: 64.88], G:[ACC: %100.00, LOSS: 70.63]\n",
      "E: 478, D:[ACC: %100.00, LOSS: 73.12], G:[ACC: %100.00, LOSS: 73.09]\n",
      "E: 479, D:[ACC: %100.00, LOSS: 67.68], G:[ACC: %100.00, LOSS: 73.14]\n",
      "E: 480, D:[ACC: %100.00, LOSS: 63.99], G:[ACC: %100.00, LOSS: 72.92]\n",
      "E: 481, D:[ACC: %100.00, LOSS: 70.80], G:[ACC: %0.00, LOSS: 351.43]\n",
      "E: 482, D:[ACC: %100.00, LOSS: 84.96], G:[ACC: %0.00, LOSS: 1169.45]\n",
      "E: 483, D:[ACC: %100.00, LOSS: 75.67], G:[ACC: %0.00, LOSS: 570.07]\n",
      "E: 484, D:[ACC: %79.49, LOSS: 113.54], G:[ACC: %0.00, LOSS: 1253.73]\n",
      "E: 485, D:[ACC: %100.00, LOSS: 87.26], G:[ACC: %0.00, LOSS: 701.40]\n",
      "E: 486, D:[ACC: %95.31, LOSS: 90.79], G:[ACC: %18.36, LOSS: 213.04]\n",
      "E: 487, D:[ACC: %100.00, LOSS: 80.30], G:[ACC: %40.62, LOSS: 169.26]\n",
      "E: 488, D:[ACC: %99.22, LOSS: 83.38], G:[ACC: %100.00, LOSS: 80.29]\n",
      "E: 489, D:[ACC: %100.00, LOSS: 72.16], G:[ACC: %100.00, LOSS: 84.42]\n",
      "E: 490, D:[ACC: %100.00, LOSS: 71.96], G:[ACC: %100.00, LOSS: 79.12]\n",
      "E: 491, D:[ACC: %100.00, LOSS: 72.44], G:[ACC: %99.61, LOSS: 75.05]\n",
      "E: 492, D:[ACC: %99.61, LOSS: 76.93], G:[ACC: %100.00, LOSS: 75.90]\n",
      "E: 493, D:[ACC: %99.61, LOSS: 73.92], G:[ACC: %100.00, LOSS: 75.95]\n",
      "E: 494, D:[ACC: %100.00, LOSS: 71.27], G:[ACC: %100.00, LOSS: 72.84]\n",
      "E: 495, D:[ACC: %100.00, LOSS: 69.77], G:[ACC: %100.00, LOSS: 73.48]\n",
      "E: 496, D:[ACC: %100.00, LOSS: 68.90], G:[ACC: %100.00, LOSS: 73.63]\n",
      "E: 497, D:[ACC: %100.00, LOSS: 68.26], G:[ACC: %99.61, LOSS: 74.64]\n",
      "E: 498, D:[ACC: %100.00, LOSS: 68.64], G:[ACC: %100.00, LOSS: 70.00]\n",
      "E: 499, D:[ACC: %100.00, LOSS: 69.52], G:[ACC: %100.00, LOSS: 68.94]\n",
      "E: 500, D:[ACC: %100.00, LOSS: 70.02], G:[ACC: %100.00, LOSS: 72.20]\n",
      "E: 501, D:[ACC: %100.00, LOSS: 67.74], G:[ACC: %100.00, LOSS: 71.43]\n",
      "E: 502, D:[ACC: %100.00, LOSS: 69.59], G:[ACC: %100.00, LOSS: 66.55]\n",
      "E: 503, D:[ACC: %100.00, LOSS: 68.76], G:[ACC: %100.00, LOSS: 71.49]\n",
      "E: 504, D:[ACC: %100.00, LOSS: 69.62], G:[ACC: %100.00, LOSS: 72.81]\n",
      "E: 505, D:[ACC: %100.00, LOSS: 66.69], G:[ACC: %100.00, LOSS: 67.33]\n",
      "E: 506, D:[ACC: %100.00, LOSS: 63.94], G:[ACC: %100.00, LOSS: 70.26]\n",
      "E: 507, D:[ACC: %100.00, LOSS: 68.97], G:[ACC: %100.00, LOSS: 73.65]\n",
      "E: 508, D:[ACC: %100.00, LOSS: 70.73], G:[ACC: %100.00, LOSS: 71.59]\n",
      "E: 509, D:[ACC: %100.00, LOSS: 67.79], G:[ACC: %100.00, LOSS: 67.88]\n",
      "E: 510, D:[ACC: %100.00, LOSS: 67.75], G:[ACC: %100.00, LOSS: 69.28]\n",
      "E: 511, D:[ACC: %100.00, LOSS: 74.32], G:[ACC: %100.00, LOSS: 68.28]\n",
      "E: 512, D:[ACC: %100.00, LOSS: 68.31], G:[ACC: %100.00, LOSS: 67.62]\n",
      "E: 513, D:[ACC: %100.00, LOSS: 72.13], G:[ACC: %100.00, LOSS: 72.46]\n",
      "E: 514, D:[ACC: %100.00, LOSS: 67.13], G:[ACC: %100.00, LOSS: 70.61]\n",
      "E: 515, D:[ACC: %99.80, LOSS: 68.63], G:[ACC: %100.00, LOSS: 66.06]\n",
      "E: 516, D:[ACC: %100.00, LOSS: 69.87], G:[ACC: %100.00, LOSS: 70.64]\n",
      "E: 517, D:[ACC: %100.00, LOSS: 66.10], G:[ACC: %100.00, LOSS: 68.11]\n",
      "E: 518, D:[ACC: %100.00, LOSS: 65.43], G:[ACC: %100.00, LOSS: 66.40]\n",
      "E: 519, D:[ACC: %100.00, LOSS: 66.35], G:[ACC: %100.00, LOSS: 71.71]\n",
      "E: 520, D:[ACC: %100.00, LOSS: 67.17], G:[ACC: %100.00, LOSS: 68.21]\n",
      "E: 521, D:[ACC: %100.00, LOSS: 68.31], G:[ACC: %100.00, LOSS: 65.23]\n",
      "E: 522, D:[ACC: %100.00, LOSS: 66.84], G:[ACC: %100.00, LOSS: 64.85]\n",
      "E: 523, D:[ACC: %100.00, LOSS: 69.91], G:[ACC: %100.00, LOSS: 69.97]\n",
      "E: 524, D:[ACC: %99.80, LOSS: 64.00], G:[ACC: %100.00, LOSS: 68.08]\n",
      "E: 525, D:[ACC: %100.00, LOSS: 60.58], G:[ACC: %100.00, LOSS: 67.50]\n",
      "E: 526, D:[ACC: %100.00, LOSS: 64.49], G:[ACC: %100.00, LOSS: 64.55]\n",
      "E: 527, D:[ACC: %100.00, LOSS: 67.03], G:[ACC: %100.00, LOSS: 68.68]\n",
      "E: 528, D:[ACC: %100.00, LOSS: 66.06], G:[ACC: %100.00, LOSS: 65.88]\n",
      "E: 529, D:[ACC: %100.00, LOSS: 66.58], G:[ACC: %100.00, LOSS: 65.74]\n",
      "E: 530, D:[ACC: %100.00, LOSS: 60.36], G:[ACC: %100.00, LOSS: 67.48]\n",
      "E: 531, D:[ACC: %100.00, LOSS: 62.54], G:[ACC: %100.00, LOSS: 64.14]\n",
      "E: 532, D:[ACC: %100.00, LOSS: 68.46], G:[ACC: %100.00, LOSS: 64.54]\n",
      "E: 533, D:[ACC: %100.00, LOSS: 63.24], G:[ACC: %100.00, LOSS: 64.96]\n",
      "E: 534, D:[ACC: %100.00, LOSS: 58.95], G:[ACC: %100.00, LOSS: 65.97]\n",
      "E: 535, D:[ACC: %100.00, LOSS: 65.37], G:[ACC: %100.00, LOSS: 64.63]\n",
      "E: 536, D:[ACC: %100.00, LOSS: 63.24], G:[ACC: %100.00, LOSS: 63.41]\n",
      "E: 537, D:[ACC: %100.00, LOSS: 60.97], G:[ACC: %100.00, LOSS: 66.20]\n",
      "E: 538, D:[ACC: %100.00, LOSS: 63.34], G:[ACC: %100.00, LOSS: 63.40]\n",
      "E: 539, D:[ACC: %100.00, LOSS: 60.86], G:[ACC: %100.00, LOSS: 67.96]\n",
      "E: 540, D:[ACC: %100.00, LOSS: 60.18], G:[ACC: %100.00, LOSS: 67.35]\n",
      "E: 541, D:[ACC: %100.00, LOSS: 63.83], G:[ACC: %100.00, LOSS: 66.93]\n",
      "E: 542, D:[ACC: %100.00, LOSS: 66.57], G:[ACC: %100.00, LOSS: 64.24]\n",
      "E: 543, D:[ACC: %100.00, LOSS: 63.84], G:[ACC: %100.00, LOSS: 66.63]\n",
      "E: 544, D:[ACC: %100.00, LOSS: 62.07], G:[ACC: %100.00, LOSS: 68.93]\n",
      "E: 545, D:[ACC: %100.00, LOSS: 65.36], G:[ACC: %99.61, LOSS: 67.89]\n",
      "E: 546, D:[ACC: %100.00, LOSS: 65.32], G:[ACC: %100.00, LOSS: 61.29]\n",
      "E: 547, D:[ACC: %100.00, LOSS: 59.31], G:[ACC: %100.00, LOSS: 62.72]\n",
      "E: 548, D:[ACC: %99.61, LOSS: 64.80], G:[ACC: %100.00, LOSS: 65.79]\n",
      "E: 549, D:[ACC: %100.00, LOSS: 60.78], G:[ACC: %100.00, LOSS: 64.68]\n",
      "E: 550, D:[ACC: %100.00, LOSS: 58.53], G:[ACC: %100.00, LOSS: 70.26]\n",
      "E: 551, D:[ACC: %100.00, LOSS: 59.09], G:[ACC: %100.00, LOSS: 66.68]\n",
      "E: 552, D:[ACC: %100.00, LOSS: 64.74], G:[ACC: %100.00, LOSS: 65.95]\n",
      "E: 553, D:[ACC: %100.00, LOSS: 59.75], G:[ACC: %100.00, LOSS: 59.88]\n",
      "E: 554, D:[ACC: %100.00, LOSS: 60.48], G:[ACC: %100.00, LOSS: 66.41]\n",
      "E: 555, D:[ACC: %100.00, LOSS: 61.68], G:[ACC: %100.00, LOSS: 61.59]\n",
      "E: 556, D:[ACC: %100.00, LOSS: 61.39], G:[ACC: %100.00, LOSS: 63.36]\n",
      "E: 557, D:[ACC: %100.00, LOSS: 57.15], G:[ACC: %100.00, LOSS: 61.65]\n",
      "E: 558, D:[ACC: %100.00, LOSS: 61.30], G:[ACC: %100.00, LOSS: 63.38]\n",
      "E: 559, D:[ACC: %100.00, LOSS: 59.21], G:[ACC: %100.00, LOSS: 61.98]\n",
      "E: 560, D:[ACC: %100.00, LOSS: 56.33], G:[ACC: %100.00, LOSS: 63.72]\n",
      "E: 561, D:[ACC: %100.00, LOSS: 60.26], G:[ACC: %100.00, LOSS: 62.41]\n",
      "E: 562, D:[ACC: %100.00, LOSS: 57.31], G:[ACC: %99.61, LOSS: 63.10]\n",
      "E: 563, D:[ACC: %100.00, LOSS: 57.52], G:[ACC: %100.00, LOSS: 65.34]\n",
      "E: 564, D:[ACC: %100.00, LOSS: 57.20], G:[ACC: %100.00, LOSS: 65.01]\n",
      "E: 565, D:[ACC: %100.00, LOSS: 55.83], G:[ACC: %100.00, LOSS: 66.00]\n",
      "E: 566, D:[ACC: %100.00, LOSS: 59.31], G:[ACC: %100.00, LOSS: 62.31]\n",
      "E: 567, D:[ACC: %100.00, LOSS: 63.47], G:[ACC: %100.00, LOSS: 65.35]\n",
      "E: 568, D:[ACC: %100.00, LOSS: 58.59], G:[ACC: %100.00, LOSS: 68.71]\n",
      "E: 569, D:[ACC: %100.00, LOSS: 58.68], G:[ACC: %100.00, LOSS: 64.10]\n",
      "E: 570, D:[ACC: %100.00, LOSS: 55.91], G:[ACC: %100.00, LOSS: 64.22]\n",
      "E: 571, D:[ACC: %100.00, LOSS: 59.60], G:[ACC: %100.00, LOSS: 67.07]\n",
      "E: 572, D:[ACC: %100.00, LOSS: 59.60], G:[ACC: %100.00, LOSS: 63.43]\n",
      "E: 573, D:[ACC: %99.80, LOSS: 64.12], G:[ACC: %100.00, LOSS: 64.68]\n",
      "E: 574, D:[ACC: %99.80, LOSS: 64.86], G:[ACC: %100.00, LOSS: 64.27]\n",
      "E: 575, D:[ACC: %100.00, LOSS: 57.63], G:[ACC: %100.00, LOSS: 63.75]\n",
      "E: 576, D:[ACC: %100.00, LOSS: 57.32], G:[ACC: %100.00, LOSS: 63.23]\n",
      "E: 577, D:[ACC: %100.00, LOSS: 58.33], G:[ACC: %100.00, LOSS: 62.59]\n",
      "E: 578, D:[ACC: %100.00, LOSS: 56.80], G:[ACC: %100.00, LOSS: 63.14]\n",
      "E: 579, D:[ACC: %100.00, LOSS: 64.33], G:[ACC: %100.00, LOSS: 62.61]\n",
      "E: 580, D:[ACC: %100.00, LOSS: 57.06], G:[ACC: %100.00, LOSS: 64.08]\n",
      "E: 581, D:[ACC: %100.00, LOSS: 57.49], G:[ACC: %100.00, LOSS: 60.55]\n",
      "E: 582, D:[ACC: %100.00, LOSS: 59.64], G:[ACC: %100.00, LOSS: 63.34]\n",
      "E: 583, D:[ACC: %100.00, LOSS: 56.37], G:[ACC: %100.00, LOSS: 63.21]\n",
      "E: 584, D:[ACC: %100.00, LOSS: 60.12], G:[ACC: %99.61, LOSS: 61.63]\n",
      "E: 585, D:[ACC: %100.00, LOSS: 53.88], G:[ACC: %100.00, LOSS: 66.36]\n",
      "E: 586, D:[ACC: %100.00, LOSS: 54.28], G:[ACC: %100.00, LOSS: 57.60]\n",
      "E: 587, D:[ACC: %100.00, LOSS: 56.99], G:[ACC: %100.00, LOSS: 60.18]\n",
      "E: 588, D:[ACC: %100.00, LOSS: 58.75], G:[ACC: %100.00, LOSS: 56.60]\n",
      "E: 589, D:[ACC: %100.00, LOSS: 56.70], G:[ACC: %100.00, LOSS: 60.77]\n",
      "E: 590, D:[ACC: %100.00, LOSS: 57.69], G:[ACC: %100.00, LOSS: 57.19]\n",
      "E: 591, D:[ACC: %100.00, LOSS: 54.10], G:[ACC: %100.00, LOSS: 60.79]\n",
      "E: 592, D:[ACC: %100.00, LOSS: 55.41], G:[ACC: %100.00, LOSS: 58.21]\n",
      "E: 593, D:[ACC: %100.00, LOSS: 57.55], G:[ACC: %100.00, LOSS: 61.16]\n",
      "E: 594, D:[ACC: %100.00, LOSS: 57.61], G:[ACC: %99.61, LOSS: 61.77]\n",
      "E: 595, D:[ACC: %100.00, LOSS: 59.12], G:[ACC: %100.00, LOSS: 55.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 596, D:[ACC: %100.00, LOSS: 58.74], G:[ACC: %100.00, LOSS: 62.10]\n",
      "E: 597, D:[ACC: %100.00, LOSS: 55.32], G:[ACC: %100.00, LOSS: 62.95]\n",
      "E: 598, D:[ACC: %100.00, LOSS: 53.37], G:[ACC: %100.00, LOSS: 62.94]\n",
      "E: 599, D:[ACC: %100.00, LOSS: 57.83], G:[ACC: %100.00, LOSS: 58.49]\n",
      "E: 600, D:[ACC: %100.00, LOSS: 56.98], G:[ACC: %100.00, LOSS: 57.44]\n",
      "E: 601, D:[ACC: %100.00, LOSS: 54.47], G:[ACC: %100.00, LOSS: 62.28]\n",
      "E: 602, D:[ACC: %100.00, LOSS: 55.28], G:[ACC: %100.00, LOSS: 59.22]\n",
      "E: 603, D:[ACC: %100.00, LOSS: 53.71], G:[ACC: %100.00, LOSS: 61.22]\n",
      "E: 604, D:[ACC: %100.00, LOSS: 57.09], G:[ACC: %100.00, LOSS: 59.42]\n",
      "E: 605, D:[ACC: %100.00, LOSS: 55.03], G:[ACC: %100.00, LOSS: 59.44]\n",
      "E: 606, D:[ACC: %100.00, LOSS: 56.06], G:[ACC: %100.00, LOSS: 56.21]\n",
      "E: 607, D:[ACC: %100.00, LOSS: 56.51], G:[ACC: %100.00, LOSS: 54.98]\n",
      "E: 608, D:[ACC: %99.80, LOSS: 54.36], G:[ACC: %100.00, LOSS: 59.58]\n",
      "E: 609, D:[ACC: %100.00, LOSS: 58.15], G:[ACC: %100.00, LOSS: 57.42]\n",
      "E: 610, D:[ACC: %100.00, LOSS: 52.74], G:[ACC: %100.00, LOSS: 59.18]\n",
      "E: 611, D:[ACC: %100.00, LOSS: 51.58], G:[ACC: %100.00, LOSS: 57.85]\n",
      "E: 612, D:[ACC: %100.00, LOSS: 52.12], G:[ACC: %100.00, LOSS: 59.34]\n",
      "E: 613, D:[ACC: %100.00, LOSS: 54.82], G:[ACC: %100.00, LOSS: 58.49]\n",
      "E: 614, D:[ACC: %99.80, LOSS: 54.45], G:[ACC: %100.00, LOSS: 55.05]\n",
      "E: 615, D:[ACC: %100.00, LOSS: 53.02], G:[ACC: %100.00, LOSS: 55.00]\n",
      "E: 616, D:[ACC: %100.00, LOSS: 55.96], G:[ACC: %100.00, LOSS: 52.97]\n",
      "E: 617, D:[ACC: %100.00, LOSS: 55.13], G:[ACC: %100.00, LOSS: 56.92]\n",
      "E: 618, D:[ACC: %100.00, LOSS: 52.12], G:[ACC: %100.00, LOSS: 55.31]\n",
      "E: 619, D:[ACC: %100.00, LOSS: 56.04], G:[ACC: %100.00, LOSS: 56.98]\n",
      "E: 620, D:[ACC: %100.00, LOSS: 50.85], G:[ACC: %100.00, LOSS: 52.51]\n",
      "E: 621, D:[ACC: %100.00, LOSS: 54.52], G:[ACC: %100.00, LOSS: 50.66]\n",
      "E: 622, D:[ACC: %100.00, LOSS: 53.17], G:[ACC: %100.00, LOSS: 55.90]\n",
      "E: 623, D:[ACC: %100.00, LOSS: 50.43], G:[ACC: %100.00, LOSS: 53.44]\n",
      "E: 624, D:[ACC: %100.00, LOSS: 52.40], G:[ACC: %100.00, LOSS: 56.61]\n",
      "E: 625, D:[ACC: %100.00, LOSS: 54.32], G:[ACC: %100.00, LOSS: 52.60]\n",
      "E: 626, D:[ACC: %100.00, LOSS: 55.65], G:[ACC: %100.00, LOSS: 58.46]\n",
      "E: 627, D:[ACC: %100.00, LOSS: 52.56], G:[ACC: %100.00, LOSS: 47.48]\n",
      "E: 628, D:[ACC: %100.00, LOSS: 50.07], G:[ACC: %100.00, LOSS: 56.02]\n",
      "E: 629, D:[ACC: %100.00, LOSS: 57.12], G:[ACC: %100.00, LOSS: 56.84]\n",
      "E: 630, D:[ACC: %100.00, LOSS: 53.59], G:[ACC: %100.00, LOSS: 52.98]\n",
      "E: 631, D:[ACC: %100.00, LOSS: 52.12], G:[ACC: %100.00, LOSS: 56.83]\n",
      "E: 632, D:[ACC: %100.00, LOSS: 53.63], G:[ACC: %100.00, LOSS: 59.49]\n",
      "E: 633, D:[ACC: %100.00, LOSS: 54.91], G:[ACC: %100.00, LOSS: 53.10]\n",
      "E: 634, D:[ACC: %100.00, LOSS: 55.49], G:[ACC: %100.00, LOSS: 53.24]\n",
      "E: 635, D:[ACC: %100.00, LOSS: 51.24], G:[ACC: %100.00, LOSS: 50.78]\n",
      "E: 636, D:[ACC: %100.00, LOSS: 51.47], G:[ACC: %100.00, LOSS: 49.44]\n",
      "E: 637, D:[ACC: %100.00, LOSS: 54.29], G:[ACC: %100.00, LOSS: 55.63]\n",
      "E: 638, D:[ACC: %100.00, LOSS: 52.94], G:[ACC: %100.00, LOSS: 52.97]\n",
      "E: 639, D:[ACC: %100.00, LOSS: 51.25], G:[ACC: %100.00, LOSS: 54.36]\n",
      "E: 640, D:[ACC: %100.00, LOSS: 50.49], G:[ACC: %100.00, LOSS: 52.17]\n",
      "E: 641, D:[ACC: %100.00, LOSS: 52.64], G:[ACC: %100.00, LOSS: 48.91]\n",
      "E: 642, D:[ACC: %100.00, LOSS: 53.44], G:[ACC: %100.00, LOSS: 55.24]\n",
      "E: 643, D:[ACC: %100.00, LOSS: 51.03], G:[ACC: %100.00, LOSS: 53.85]\n",
      "E: 644, D:[ACC: %100.00, LOSS: 54.13], G:[ACC: %100.00, LOSS: 54.05]\n",
      "E: 645, D:[ACC: %100.00, LOSS: 49.39], G:[ACC: %100.00, LOSS: 51.59]\n",
      "E: 646, D:[ACC: %100.00, LOSS: 48.36], G:[ACC: %100.00, LOSS: 51.48]\n",
      "E: 647, D:[ACC: %100.00, LOSS: 54.06], G:[ACC: %100.00, LOSS: 53.94]\n",
      "E: 648, D:[ACC: %100.00, LOSS: 52.18], G:[ACC: %100.00, LOSS: 49.87]\n",
      "E: 649, D:[ACC: %100.00, LOSS: 49.05], G:[ACC: %100.00, LOSS: 54.82]\n",
      "E: 650, D:[ACC: %99.80, LOSS: 51.75], G:[ACC: %100.00, LOSS: 50.87]\n",
      "E: 651, D:[ACC: %100.00, LOSS: 52.26], G:[ACC: %100.00, LOSS: 53.67]\n",
      "E: 652, D:[ACC: %100.00, LOSS: 51.38], G:[ACC: %100.00, LOSS: 52.13]\n",
      "E: 653, D:[ACC: %100.00, LOSS: 49.58], G:[ACC: %100.00, LOSS: 50.85]\n",
      "E: 654, D:[ACC: %100.00, LOSS: 49.59], G:[ACC: %100.00, LOSS: 54.05]\n",
      "E: 655, D:[ACC: %100.00, LOSS: 52.63], G:[ACC: %100.00, LOSS: 52.78]\n",
      "E: 656, D:[ACC: %100.00, LOSS: 51.95], G:[ACC: %100.00, LOSS: 52.91]\n",
      "E: 657, D:[ACC: %100.00, LOSS: 49.76], G:[ACC: %100.00, LOSS: 49.62]\n",
      "E: 658, D:[ACC: %100.00, LOSS: 50.44], G:[ACC: %100.00, LOSS: 51.92]\n",
      "E: 659, D:[ACC: %100.00, LOSS: 51.22], G:[ACC: %100.00, LOSS: 50.30]\n",
      "E: 660, D:[ACC: %100.00, LOSS: 49.95], G:[ACC: %100.00, LOSS: 52.75]\n",
      "E: 661, D:[ACC: %100.00, LOSS: 48.34], G:[ACC: %100.00, LOSS: 52.62]\n",
      "E: 662, D:[ACC: %100.00, LOSS: 50.20], G:[ACC: %100.00, LOSS: 50.83]\n",
      "E: 663, D:[ACC: %100.00, LOSS: 49.18], G:[ACC: %100.00, LOSS: 56.06]\n",
      "E: 664, D:[ACC: %100.00, LOSS: 50.49], G:[ACC: %100.00, LOSS: 50.43]\n",
      "E: 665, D:[ACC: %100.00, LOSS: 49.55], G:[ACC: %99.61, LOSS: 52.12]\n",
      "E: 666, D:[ACC: %100.00, LOSS: 50.75], G:[ACC: %100.00, LOSS: 49.79]\n",
      "E: 667, D:[ACC: %100.00, LOSS: 47.10], G:[ACC: %100.00, LOSS: 51.79]\n",
      "E: 668, D:[ACC: %100.00, LOSS: 48.64], G:[ACC: %100.00, LOSS: 46.13]\n",
      "E: 669, D:[ACC: %100.00, LOSS: 45.65], G:[ACC: %100.00, LOSS: 49.23]\n",
      "E: 670, D:[ACC: %100.00, LOSS: 47.73], G:[ACC: %99.61, LOSS: 48.70]\n",
      "E: 671, D:[ACC: %100.00, LOSS: 47.12], G:[ACC: %100.00, LOSS: 52.75]\n",
      "E: 672, D:[ACC: %100.00, LOSS: 48.94], G:[ACC: %100.00, LOSS: 51.75]\n",
      "E: 673, D:[ACC: %100.00, LOSS: 47.84], G:[ACC: %100.00, LOSS: 47.39]\n",
      "E: 674, D:[ACC: %100.00, LOSS: 47.71], G:[ACC: %100.00, LOSS: 50.46]\n",
      "E: 675, D:[ACC: %100.00, LOSS: 45.25], G:[ACC: %100.00, LOSS: 48.64]\n",
      "E: 676, D:[ACC: %100.00, LOSS: 47.86], G:[ACC: %100.00, LOSS: 48.76]\n",
      "E: 677, D:[ACC: %100.00, LOSS: 46.60], G:[ACC: %100.00, LOSS: 47.65]\n",
      "E: 678, D:[ACC: %100.00, LOSS: 47.71], G:[ACC: %99.61, LOSS: 52.32]\n",
      "E: 679, D:[ACC: %100.00, LOSS: 46.41], G:[ACC: %100.00, LOSS: 53.39]\n",
      "E: 680, D:[ACC: %100.00, LOSS: 43.31], G:[ACC: %100.00, LOSS: 51.92]\n",
      "E: 681, D:[ACC: %100.00, LOSS: 46.00], G:[ACC: %100.00, LOSS: 47.78]\n",
      "E: 682, D:[ACC: %100.00, LOSS: 47.41], G:[ACC: %100.00, LOSS: 47.25]\n",
      "E: 683, D:[ACC: %100.00, LOSS: 48.91], G:[ACC: %100.00, LOSS: 49.90]\n",
      "E: 684, D:[ACC: %100.00, LOSS: 48.28], G:[ACC: %100.00, LOSS: 55.25]\n",
      "E: 685, D:[ACC: %100.00, LOSS: 48.27], G:[ACC: %100.00, LOSS: 49.51]\n",
      "E: 686, D:[ACC: %100.00, LOSS: 47.76], G:[ACC: %100.00, LOSS: 46.66]\n",
      "E: 687, D:[ACC: %100.00, LOSS: 49.43], G:[ACC: %100.00, LOSS: 52.65]\n",
      "E: 688, D:[ACC: %100.00, LOSS: 49.62], G:[ACC: %100.00, LOSS: 53.44]\n",
      "E: 689, D:[ACC: %100.00, LOSS: 49.05], G:[ACC: %100.00, LOSS: 50.82]\n",
      "E: 690, D:[ACC: %100.00, LOSS: 47.65], G:[ACC: %100.00, LOSS: 50.08]\n",
      "E: 691, D:[ACC: %100.00, LOSS: 49.25], G:[ACC: %100.00, LOSS: 50.33]\n",
      "E: 692, D:[ACC: %100.00, LOSS: 48.16], G:[ACC: %100.00, LOSS: 49.17]\n",
      "E: 693, D:[ACC: %100.00, LOSS: 46.39], G:[ACC: %100.00, LOSS: 51.55]\n",
      "E: 694, D:[ACC: %100.00, LOSS: 43.73], G:[ACC: %100.00, LOSS: 49.11]\n",
      "E: 695, D:[ACC: %100.00, LOSS: 50.09], G:[ACC: %99.61, LOSS: 52.39]\n",
      "E: 696, D:[ACC: %100.00, LOSS: 46.31], G:[ACC: %99.61, LOSS: 52.48]\n",
      "E: 697, D:[ACC: %100.00, LOSS: 47.68], G:[ACC: %100.00, LOSS: 47.62]\n",
      "E: 698, D:[ACC: %99.61, LOSS: 45.23], G:[ACC: %100.00, LOSS: 45.74]\n",
      "E: 699, D:[ACC: %100.00, LOSS: 44.04], G:[ACC: %100.00, LOSS: 46.35]\n",
      "E: 700, D:[ACC: %100.00, LOSS: 45.41], G:[ACC: %100.00, LOSS: 48.78]\n",
      "E: 701, D:[ACC: %100.00, LOSS: 46.63], G:[ACC: %100.00, LOSS: 44.23]\n",
      "E: 702, D:[ACC: %100.00, LOSS: 45.93], G:[ACC: %100.00, LOSS: 52.68]\n",
      "E: 703, D:[ACC: %100.00, LOSS: 43.82], G:[ACC: %99.61, LOSS: 51.04]\n",
      "E: 704, D:[ACC: %100.00, LOSS: 45.14], G:[ACC: %100.00, LOSS: 46.89]\n",
      "E: 705, D:[ACC: %100.00, LOSS: 48.74], G:[ACC: %100.00, LOSS: 42.91]\n",
      "E: 706, D:[ACC: %100.00, LOSS: 46.06], G:[ACC: %100.00, LOSS: 48.14]\n",
      "E: 707, D:[ACC: %100.00, LOSS: 46.83], G:[ACC: %100.00, LOSS: 47.37]\n",
      "E: 708, D:[ACC: %100.00, LOSS: 46.98], G:[ACC: %100.00, LOSS: 47.12]\n",
      "E: 709, D:[ACC: %100.00, LOSS: 45.52], G:[ACC: %100.00, LOSS: 46.56]\n",
      "E: 710, D:[ACC: %100.00, LOSS: 45.14], G:[ACC: %100.00, LOSS: 49.53]\n",
      "E: 711, D:[ACC: %100.00, LOSS: 45.13], G:[ACC: %100.00, LOSS: 49.89]\n",
      "E: 712, D:[ACC: %100.00, LOSS: 45.38], G:[ACC: %100.00, LOSS: 48.06]\n",
      "E: 713, D:[ACC: %100.00, LOSS: 44.25], G:[ACC: %100.00, LOSS: 48.27]\n",
      "E: 714, D:[ACC: %100.00, LOSS: 45.23], G:[ACC: %100.00, LOSS: 46.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 715, D:[ACC: %100.00, LOSS: 44.58], G:[ACC: %100.00, LOSS: 45.91]\n",
      "E: 716, D:[ACC: %100.00, LOSS: 44.91], G:[ACC: %100.00, LOSS: 47.61]\n",
      "E: 717, D:[ACC: %100.00, LOSS: 44.08], G:[ACC: %100.00, LOSS: 46.77]\n",
      "E: 718, D:[ACC: %100.00, LOSS: 47.53], G:[ACC: %100.00, LOSS: 49.27]\n",
      "E: 719, D:[ACC: %100.00, LOSS: 47.19], G:[ACC: %100.00, LOSS: 45.93]\n",
      "E: 720, D:[ACC: %100.00, LOSS: 46.74], G:[ACC: %100.00, LOSS: 45.91]\n",
      "E: 721, D:[ACC: %100.00, LOSS: 44.46], G:[ACC: %100.00, LOSS: 50.26]\n",
      "E: 722, D:[ACC: %100.00, LOSS: 46.35], G:[ACC: %100.00, LOSS: 47.16]\n",
      "E: 723, D:[ACC: %100.00, LOSS: 43.37], G:[ACC: %100.00, LOSS: 48.48]\n",
      "E: 724, D:[ACC: %100.00, LOSS: 45.34], G:[ACC: %100.00, LOSS: 46.85]\n",
      "E: 725, D:[ACC: %100.00, LOSS: 45.32], G:[ACC: %100.00, LOSS: 47.13]\n",
      "E: 726, D:[ACC: %100.00, LOSS: 42.14], G:[ACC: %100.00, LOSS: 44.83]\n",
      "E: 727, D:[ACC: %100.00, LOSS: 42.76], G:[ACC: %100.00, LOSS: 45.30]\n",
      "E: 728, D:[ACC: %100.00, LOSS: 45.56], G:[ACC: %100.00, LOSS: 46.21]\n",
      "E: 729, D:[ACC: %100.00, LOSS: 44.83], G:[ACC: %100.00, LOSS: 43.79]\n",
      "E: 730, D:[ACC: %100.00, LOSS: 40.66], G:[ACC: %100.00, LOSS: 44.01]\n",
      "E: 731, D:[ACC: %100.00, LOSS: 42.73], G:[ACC: %100.00, LOSS: 44.16]\n",
      "E: 732, D:[ACC: %100.00, LOSS: 40.51], G:[ACC: %100.00, LOSS: 47.55]\n",
      "E: 733, D:[ACC: %100.00, LOSS: 43.88], G:[ACC: %100.00, LOSS: 45.79]\n",
      "E: 734, D:[ACC: %100.00, LOSS: 43.73], G:[ACC: %99.61, LOSS: 44.80]\n",
      "E: 735, D:[ACC: %100.00, LOSS: 46.23], G:[ACC: %100.00, LOSS: 44.06]\n",
      "E: 736, D:[ACC: %100.00, LOSS: 41.77], G:[ACC: %100.00, LOSS: 46.09]\n",
      "E: 737, D:[ACC: %100.00, LOSS: 44.82], G:[ACC: %100.00, LOSS: 43.99]\n",
      "E: 738, D:[ACC: %100.00, LOSS: 45.36], G:[ACC: %100.00, LOSS: 46.72]\n",
      "E: 739, D:[ACC: %100.00, LOSS: 44.59], G:[ACC: %100.00, LOSS: 44.50]\n",
      "E: 740, D:[ACC: %100.00, LOSS: 45.05], G:[ACC: %100.00, LOSS: 44.83]\n",
      "E: 741, D:[ACC: %100.00, LOSS: 44.45], G:[ACC: %100.00, LOSS: 45.79]\n",
      "E: 742, D:[ACC: %100.00, LOSS: 46.14], G:[ACC: %100.00, LOSS: 43.26]\n",
      "E: 743, D:[ACC: %100.00, LOSS: 42.83], G:[ACC: %100.00, LOSS: 44.77]\n",
      "E: 744, D:[ACC: %100.00, LOSS: 42.72], G:[ACC: %100.00, LOSS: 43.74]\n",
      "E: 745, D:[ACC: %100.00, LOSS: 41.27], G:[ACC: %100.00, LOSS: 42.61]\n",
      "E: 746, D:[ACC: %99.80, LOSS: 45.83], G:[ACC: %100.00, LOSS: 46.91]\n",
      "E: 747, D:[ACC: %100.00, LOSS: 42.18], G:[ACC: %100.00, LOSS: 42.22]\n",
      "E: 748, D:[ACC: %100.00, LOSS: 43.08], G:[ACC: %100.00, LOSS: 42.08]\n",
      "E: 749, D:[ACC: %100.00, LOSS: 39.97], G:[ACC: %100.00, LOSS: 42.15]\n",
      "E: 750, D:[ACC: %100.00, LOSS: 41.25], G:[ACC: %100.00, LOSS: 42.43]\n",
      "E: 751, D:[ACC: %100.00, LOSS: 40.15], G:[ACC: %100.00, LOSS: 44.76]\n",
      "E: 752, D:[ACC: %100.00, LOSS: 39.69], G:[ACC: %100.00, LOSS: 40.67]\n",
      "E: 753, D:[ACC: %100.00, LOSS: 40.04], G:[ACC: %100.00, LOSS: 42.36]\n",
      "E: 754, D:[ACC: %100.00, LOSS: 39.30], G:[ACC: %100.00, LOSS: 41.92]\n",
      "E: 755, D:[ACC: %100.00, LOSS: 41.80], G:[ACC: %100.00, LOSS: 41.79]\n",
      "E: 756, D:[ACC: %100.00, LOSS: 43.43], G:[ACC: %100.00, LOSS: 41.75]\n",
      "E: 757, D:[ACC: %100.00, LOSS: 42.72], G:[ACC: %100.00, LOSS: 41.50]\n",
      "E: 758, D:[ACC: %100.00, LOSS: 41.55], G:[ACC: %100.00, LOSS: 41.05]\n",
      "E: 759, D:[ACC: %100.00, LOSS: 40.71], G:[ACC: %100.00, LOSS: 39.56]\n",
      "E: 760, D:[ACC: %100.00, LOSS: 38.45], G:[ACC: %100.00, LOSS: 41.63]\n",
      "E: 761, D:[ACC: %100.00, LOSS: 38.76], G:[ACC: %100.00, LOSS: 39.66]\n",
      "E: 762, D:[ACC: %100.00, LOSS: 41.32], G:[ACC: %100.00, LOSS: 41.49]\n",
      "E: 763, D:[ACC: %100.00, LOSS: 41.03], G:[ACC: %100.00, LOSS: 40.58]\n",
      "E: 764, D:[ACC: %100.00, LOSS: 41.09], G:[ACC: %100.00, LOSS: 40.97]\n",
      "E: 765, D:[ACC: %100.00, LOSS: 39.66], G:[ACC: %100.00, LOSS: 38.52]\n",
      "E: 766, D:[ACC: %100.00, LOSS: 42.78], G:[ACC: %100.00, LOSS: 40.95]\n",
      "E: 767, D:[ACC: %100.00, LOSS: 39.47], G:[ACC: %100.00, LOSS: 39.68]\n",
      "E: 768, D:[ACC: %99.80, LOSS: 40.25], G:[ACC: %100.00, LOSS: 42.65]\n",
      "E: 769, D:[ACC: %100.00, LOSS: 40.66], G:[ACC: %100.00, LOSS: 41.75]\n",
      "E: 770, D:[ACC: %100.00, LOSS: 41.66], G:[ACC: %100.00, LOSS: 41.55]\n",
      "E: 771, D:[ACC: %100.00, LOSS: 41.59], G:[ACC: %100.00, LOSS: 41.38]\n",
      "E: 772, D:[ACC: %100.00, LOSS: 41.33], G:[ACC: %100.00, LOSS: 38.50]\n",
      "E: 773, D:[ACC: %100.00, LOSS: 39.09], G:[ACC: %100.00, LOSS: 38.80]\n",
      "E: 774, D:[ACC: %100.00, LOSS: 42.59], G:[ACC: %99.61, LOSS: 42.50]\n",
      "E: 775, D:[ACC: %100.00, LOSS: 40.99], G:[ACC: %100.00, LOSS: 43.02]\n",
      "E: 776, D:[ACC: %100.00, LOSS: 39.59], G:[ACC: %99.61, LOSS: 42.18]\n",
      "E: 777, D:[ACC: %100.00, LOSS: 40.26], G:[ACC: %100.00, LOSS: 41.20]\n",
      "E: 778, D:[ACC: %100.00, LOSS: 37.76], G:[ACC: %99.61, LOSS: 40.74]\n",
      "E: 779, D:[ACC: %100.00, LOSS: 39.17], G:[ACC: %100.00, LOSS: 41.98]\n",
      "E: 780, D:[ACC: %100.00, LOSS: 40.90], G:[ACC: %99.61, LOSS: 40.86]\n",
      "E: 781, D:[ACC: %99.61, LOSS: 47.98], G:[ACC: %100.00, LOSS: 43.11]\n",
      "E: 782, D:[ACC: %100.00, LOSS: 40.65], G:[ACC: %100.00, LOSS: 40.13]\n",
      "E: 783, D:[ACC: %100.00, LOSS: 39.55], G:[ACC: %100.00, LOSS: 43.60]\n",
      "E: 784, D:[ACC: %100.00, LOSS: 39.12], G:[ACC: %100.00, LOSS: 41.45]\n",
      "E: 785, D:[ACC: %100.00, LOSS: 41.56], G:[ACC: %99.61, LOSS: 45.63]\n",
      "E: 786, D:[ACC: %100.00, LOSS: 39.79], G:[ACC: %100.00, LOSS: 47.11]\n",
      "E: 787, D:[ACC: %100.00, LOSS: 40.90], G:[ACC: %99.61, LOSS: 42.42]\n",
      "E: 788, D:[ACC: %100.00, LOSS: 40.46], G:[ACC: %99.61, LOSS: 43.87]\n",
      "E: 789, D:[ACC: %100.00, LOSS: 40.38], G:[ACC: %99.61, LOSS: 41.66]\n",
      "E: 790, D:[ACC: %100.00, LOSS: 39.83], G:[ACC: %100.00, LOSS: 42.91]\n",
      "E: 791, D:[ACC: %100.00, LOSS: 41.77], G:[ACC: %100.00, LOSS: 43.92]\n",
      "E: 792, D:[ACC: %99.80, LOSS: 40.20], G:[ACC: %100.00, LOSS: 44.15]\n",
      "E: 793, D:[ACC: %100.00, LOSS: 38.41], G:[ACC: %100.00, LOSS: 40.14]\n",
      "E: 794, D:[ACC: %100.00, LOSS: 38.74], G:[ACC: %100.00, LOSS: 40.32]\n",
      "E: 795, D:[ACC: %100.00, LOSS: 42.24], G:[ACC: %100.00, LOSS: 40.04]\n",
      "E: 796, D:[ACC: %100.00, LOSS: 41.25], G:[ACC: %100.00, LOSS: 40.95]\n",
      "E: 797, D:[ACC: %100.00, LOSS: 39.73], G:[ACC: %100.00, LOSS: 40.22]\n",
      "E: 798, D:[ACC: %100.00, LOSS: 39.82], G:[ACC: %100.00, LOSS: 39.56]\n",
      "E: 799, D:[ACC: %100.00, LOSS: 39.32], G:[ACC: %100.00, LOSS: 41.67]\n",
      "E: 800, D:[ACC: %100.00, LOSS: 41.33], G:[ACC: %100.00, LOSS: 40.82]\n",
      "E: 801, D:[ACC: %100.00, LOSS: 40.82], G:[ACC: %100.00, LOSS: 40.36]\n",
      "E: 802, D:[ACC: %100.00, LOSS: 36.80], G:[ACC: %100.00, LOSS: 40.11]\n",
      "E: 803, D:[ACC: %100.00, LOSS: 37.11], G:[ACC: %100.00, LOSS: 42.16]\n",
      "E: 804, D:[ACC: %100.00, LOSS: 40.82], G:[ACC: %100.00, LOSS: 40.10]\n",
      "E: 805, D:[ACC: %100.00, LOSS: 39.32], G:[ACC: %100.00, LOSS: 39.76]\n",
      "E: 806, D:[ACC: %100.00, LOSS: 39.13], G:[ACC: %100.00, LOSS: 38.34]\n",
      "E: 807, D:[ACC: %100.00, LOSS: 40.31], G:[ACC: %100.00, LOSS: 37.67]\n",
      "E: 808, D:[ACC: %100.00, LOSS: 38.23], G:[ACC: %100.00, LOSS: 38.00]\n",
      "E: 809, D:[ACC: %100.00, LOSS: 36.75], G:[ACC: %99.61, LOSS: 37.31]\n",
      "E: 810, D:[ACC: %100.00, LOSS: 37.28], G:[ACC: %100.00, LOSS: 38.77]\n",
      "E: 811, D:[ACC: %100.00, LOSS: 40.02], G:[ACC: %100.00, LOSS: 38.58]\n",
      "E: 812, D:[ACC: %100.00, LOSS: 39.22], G:[ACC: %100.00, LOSS: 38.61]\n",
      "E: 813, D:[ACC: %100.00, LOSS: 37.78], G:[ACC: %100.00, LOSS: 36.38]\n",
      "E: 814, D:[ACC: %100.00, LOSS: 36.45], G:[ACC: %100.00, LOSS: 39.34]\n",
      "E: 815, D:[ACC: %100.00, LOSS: 39.42], G:[ACC: %100.00, LOSS: 37.49]\n",
      "E: 816, D:[ACC: %100.00, LOSS: 37.11], G:[ACC: %100.00, LOSS: 38.31]\n",
      "E: 817, D:[ACC: %100.00, LOSS: 38.75], G:[ACC: %100.00, LOSS: 37.63]\n",
      "E: 818, D:[ACC: %100.00, LOSS: 36.68], G:[ACC: %99.61, LOSS: 38.55]\n",
      "E: 819, D:[ACC: %100.00, LOSS: 38.05], G:[ACC: %100.00, LOSS: 38.49]\n",
      "E: 820, D:[ACC: %100.00, LOSS: 35.66], G:[ACC: %100.00, LOSS: 39.18]\n",
      "E: 821, D:[ACC: %100.00, LOSS: 37.20], G:[ACC: %100.00, LOSS: 37.95]\n",
      "E: 822, D:[ACC: %100.00, LOSS: 37.54], G:[ACC: %100.00, LOSS: 36.48]\n",
      "E: 823, D:[ACC: %100.00, LOSS: 37.17], G:[ACC: %100.00, LOSS: 41.23]\n",
      "E: 824, D:[ACC: %100.00, LOSS: 36.47], G:[ACC: %100.00, LOSS: 39.84]\n",
      "E: 825, D:[ACC: %100.00, LOSS: 38.54], G:[ACC: %100.00, LOSS: 38.66]\n",
      "E: 826, D:[ACC: %99.80, LOSS: 38.18], G:[ACC: %100.00, LOSS: 39.55]\n",
      "E: 827, D:[ACC: %100.00, LOSS: 36.61], G:[ACC: %100.00, LOSS: 40.83]\n",
      "E: 828, D:[ACC: %100.00, LOSS: 39.74], G:[ACC: %97.27, LOSS: 59.28]\n",
      "E: 829, D:[ACC: %100.00, LOSS: 43.18], G:[ACC: %0.00, LOSS: 759.10]\n",
      "E: 830, D:[ACC: %100.00, LOSS: 43.69], G:[ACC: %21.48, LOSS: 238.09]\n",
      "E: 831, D:[ACC: %88.28, LOSS: 61.30], G:[ACC: %0.00, LOSS: 1609.46]\n",
      "E: 832, D:[ACC: %100.00, LOSS: 45.96], G:[ACC: %0.00, LOSS: 1247.40]\n",
      "E: 833, D:[ACC: %94.53, LOSS: 54.30], G:[ACC: %0.00, LOSS: 798.06]\n",
      "E: 834, D:[ACC: %100.00, LOSS: 42.32], G:[ACC: %0.00, LOSS: 616.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 835, D:[ACC: %100.00, LOSS: 45.09], G:[ACC: %2.73, LOSS: 333.81]\n",
      "E: 836, D:[ACC: %100.00, LOSS: 38.84], G:[ACC: %24.61, LOSS: 194.61]\n",
      "E: 837, D:[ACC: %100.00, LOSS: 40.92], G:[ACC: %62.89, LOSS: 113.73]\n",
      "E: 838, D:[ACC: %100.00, LOSS: 40.63], G:[ACC: %80.86, LOSS: 86.19]\n",
      "E: 839, D:[ACC: %100.00, LOSS: 39.46], G:[ACC: %90.62, LOSS: 69.88]\n",
      "E: 840, D:[ACC: %100.00, LOSS: 37.91], G:[ACC: %96.88, LOSS: 61.89]\n",
      "E: 841, D:[ACC: %100.00, LOSS: 39.66], G:[ACC: %99.61, LOSS: 54.61]\n",
      "E: 842, D:[ACC: %100.00, LOSS: 38.32], G:[ACC: %97.66, LOSS: 56.95]\n",
      "E: 843, D:[ACC: %100.00, LOSS: 38.47], G:[ACC: %98.83, LOSS: 54.72]\n",
      "E: 844, D:[ACC: %100.00, LOSS: 39.13], G:[ACC: %100.00, LOSS: 50.90]\n",
      "E: 845, D:[ACC: %100.00, LOSS: 36.87], G:[ACC: %97.27, LOSS: 55.21]\n",
      "E: 846, D:[ACC: %100.00, LOSS: 37.29], G:[ACC: %99.22, LOSS: 51.58]\n",
      "E: 847, D:[ACC: %100.00, LOSS: 39.75], G:[ACC: %99.61, LOSS: 46.05]\n",
      "E: 848, D:[ACC: %100.00, LOSS: 39.31], G:[ACC: %98.83, LOSS: 48.00]\n",
      "E: 849, D:[ACC: %100.00, LOSS: 37.44], G:[ACC: %99.61, LOSS: 45.16]\n",
      "E: 850, D:[ACC: %100.00, LOSS: 35.68], G:[ACC: %99.61, LOSS: 45.01]\n",
      "E: 851, D:[ACC: %100.00, LOSS: 37.93], G:[ACC: %98.83, LOSS: 46.14]\n",
      "E: 852, D:[ACC: %100.00, LOSS: 38.74], G:[ACC: %100.00, LOSS: 44.82]\n",
      "E: 853, D:[ACC: %100.00, LOSS: 35.04], G:[ACC: %100.00, LOSS: 42.30]\n",
      "E: 854, D:[ACC: %100.00, LOSS: 36.50], G:[ACC: %100.00, LOSS: 41.86]\n",
      "E: 855, D:[ACC: %100.00, LOSS: 38.34], G:[ACC: %99.22, LOSS: 42.26]\n",
      "E: 856, D:[ACC: %100.00, LOSS: 37.59], G:[ACC: %99.61, LOSS: 41.34]\n",
      "E: 857, D:[ACC: %100.00, LOSS: 35.71], G:[ACC: %100.00, LOSS: 41.84]\n",
      "E: 858, D:[ACC: %100.00, LOSS: 35.29], G:[ACC: %99.61, LOSS: 44.30]\n",
      "E: 859, D:[ACC: %100.00, LOSS: 34.96], G:[ACC: %100.00, LOSS: 43.77]\n",
      "E: 860, D:[ACC: %99.80, LOSS: 36.80], G:[ACC: %100.00, LOSS: 44.20]\n",
      "E: 861, D:[ACC: %100.00, LOSS: 35.00], G:[ACC: %99.22, LOSS: 42.26]\n",
      "E: 862, D:[ACC: %100.00, LOSS: 38.62], G:[ACC: %99.61, LOSS: 42.72]\n",
      "E: 863, D:[ACC: %100.00, LOSS: 35.68], G:[ACC: %100.00, LOSS: 41.84]\n",
      "E: 864, D:[ACC: %100.00, LOSS: 37.02], G:[ACC: %98.83, LOSS: 45.12]\n",
      "E: 865, D:[ACC: %100.00, LOSS: 34.49], G:[ACC: %100.00, LOSS: 42.88]\n",
      "E: 866, D:[ACC: %100.00, LOSS: 33.56], G:[ACC: %100.00, LOSS: 41.08]\n",
      "E: 867, D:[ACC: %100.00, LOSS: 35.80], G:[ACC: %99.22, LOSS: 42.56]\n",
      "E: 868, D:[ACC: %100.00, LOSS: 35.63], G:[ACC: %100.00, LOSS: 41.02]\n",
      "E: 869, D:[ACC: %100.00, LOSS: 35.04], G:[ACC: %100.00, LOSS: 41.69]\n",
      "E: 870, D:[ACC: %100.00, LOSS: 33.01], G:[ACC: %99.22, LOSS: 42.73]\n",
      "E: 871, D:[ACC: %100.00, LOSS: 35.78], G:[ACC: %100.00, LOSS: 42.41]\n",
      "E: 872, D:[ACC: %100.00, LOSS: 34.22], G:[ACC: %100.00, LOSS: 41.52]\n",
      "E: 873, D:[ACC: %100.00, LOSS: 35.42], G:[ACC: %100.00, LOSS: 39.05]\n",
      "E: 874, D:[ACC: %100.00, LOSS: 34.12], G:[ACC: %100.00, LOSS: 38.81]\n",
      "E: 875, D:[ACC: %100.00, LOSS: 36.69], G:[ACC: %100.00, LOSS: 41.75]\n",
      "E: 876, D:[ACC: %100.00, LOSS: 35.27], G:[ACC: %99.61, LOSS: 39.20]\n",
      "E: 877, D:[ACC: %100.00, LOSS: 35.31], G:[ACC: %100.00, LOSS: 38.96]\n",
      "E: 878, D:[ACC: %100.00, LOSS: 35.55], G:[ACC: %100.00, LOSS: 38.85]\n",
      "E: 879, D:[ACC: %100.00, LOSS: 33.22], G:[ACC: %99.61, LOSS: 40.96]\n",
      "E: 880, D:[ACC: %100.00, LOSS: 33.14], G:[ACC: %100.00, LOSS: 40.10]\n",
      "E: 881, D:[ACC: %100.00, LOSS: 32.37], G:[ACC: %99.61, LOSS: 40.47]\n",
      "E: 882, D:[ACC: %100.00, LOSS: 33.24], G:[ACC: %99.22, LOSS: 42.01]\n",
      "E: 883, D:[ACC: %100.00, LOSS: 36.75], G:[ACC: %99.22, LOSS: 43.77]\n",
      "E: 884, D:[ACC: %100.00, LOSS: 32.28], G:[ACC: %100.00, LOSS: 42.38]\n",
      "E: 885, D:[ACC: %100.00, LOSS: 33.60], G:[ACC: %100.00, LOSS: 42.33]\n",
      "E: 886, D:[ACC: %100.00, LOSS: 34.91], G:[ACC: %99.22, LOSS: 45.99]\n",
      "E: 887, D:[ACC: %100.00, LOSS: 33.98], G:[ACC: %99.61, LOSS: 42.94]\n",
      "E: 888, D:[ACC: %100.00, LOSS: 33.46], G:[ACC: %99.22, LOSS: 43.52]\n",
      "E: 889, D:[ACC: %100.00, LOSS: 32.67], G:[ACC: %100.00, LOSS: 46.18]\n",
      "E: 890, D:[ACC: %100.00, LOSS: 37.31], G:[ACC: %99.61, LOSS: 42.00]\n",
      "E: 891, D:[ACC: %100.00, LOSS: 32.38], G:[ACC: %99.22, LOSS: 41.31]\n",
      "E: 892, D:[ACC: %100.00, LOSS: 33.90], G:[ACC: %100.00, LOSS: 40.78]\n",
      "E: 893, D:[ACC: %100.00, LOSS: 32.25], G:[ACC: %98.83, LOSS: 42.71]\n",
      "E: 894, D:[ACC: %100.00, LOSS: 35.00], G:[ACC: %99.61, LOSS: 41.79]\n",
      "E: 895, D:[ACC: %100.00, LOSS: 31.54], G:[ACC: %98.44, LOSS: 45.96]\n",
      "E: 896, D:[ACC: %100.00, LOSS: 32.24], G:[ACC: %100.00, LOSS: 42.41]\n",
      "E: 897, D:[ACC: %100.00, LOSS: 32.95], G:[ACC: %99.61, LOSS: 40.05]\n",
      "E: 898, D:[ACC: %100.00, LOSS: 33.02], G:[ACC: %100.00, LOSS: 40.26]\n",
      "E: 899, D:[ACC: %100.00, LOSS: 33.61], G:[ACC: %100.00, LOSS: 35.41]\n",
      "E: 900, D:[ACC: %100.00, LOSS: 32.91], G:[ACC: %100.00, LOSS: 38.08]\n",
      "E: 901, D:[ACC: %99.80, LOSS: 36.34], G:[ACC: %100.00, LOSS: 36.15]\n",
      "E: 902, D:[ACC: %100.00, LOSS: 32.93], G:[ACC: %100.00, LOSS: 38.06]\n",
      "E: 903, D:[ACC: %100.00, LOSS: 32.42], G:[ACC: %100.00, LOSS: 34.95]\n",
      "E: 904, D:[ACC: %100.00, LOSS: 32.43], G:[ACC: %100.00, LOSS: 35.33]\n",
      "E: 905, D:[ACC: %100.00, LOSS: 32.10], G:[ACC: %100.00, LOSS: 35.62]\n",
      "E: 906, D:[ACC: %100.00, LOSS: 31.90], G:[ACC: %99.61, LOSS: 37.83]\n",
      "E: 907, D:[ACC: %100.00, LOSS: 30.16], G:[ACC: %100.00, LOSS: 35.78]\n",
      "E: 908, D:[ACC: %100.00, LOSS: 31.41], G:[ACC: %99.61, LOSS: 35.38]\n",
      "E: 909, D:[ACC: %100.00, LOSS: 32.83], G:[ACC: %100.00, LOSS: 39.16]\n",
      "E: 910, D:[ACC: %100.00, LOSS: 32.42], G:[ACC: %100.00, LOSS: 36.43]\n",
      "E: 911, D:[ACC: %100.00, LOSS: 33.42], G:[ACC: %99.61, LOSS: 37.44]\n",
      "E: 912, D:[ACC: %100.00, LOSS: 31.64], G:[ACC: %100.00, LOSS: 36.57]\n",
      "E: 913, D:[ACC: %100.00, LOSS: 30.13], G:[ACC: %100.00, LOSS: 36.50]\n",
      "E: 914, D:[ACC: %100.00, LOSS: 30.50], G:[ACC: %99.61, LOSS: 36.67]\n",
      "E: 915, D:[ACC: %100.00, LOSS: 32.73], G:[ACC: %100.00, LOSS: 37.12]\n",
      "E: 916, D:[ACC: %100.00, LOSS: 32.20], G:[ACC: %100.00, LOSS: 36.12]\n",
      "E: 917, D:[ACC: %100.00, LOSS: 30.39], G:[ACC: %99.61, LOSS: 37.37]\n",
      "E: 918, D:[ACC: %100.00, LOSS: 32.24], G:[ACC: %100.00, LOSS: 35.66]\n",
      "E: 919, D:[ACC: %100.00, LOSS: 31.12], G:[ACC: %100.00, LOSS: 35.61]\n",
      "E: 920, D:[ACC: %100.00, LOSS: 32.71], G:[ACC: %100.00, LOSS: 37.09]\n",
      "E: 921, D:[ACC: %100.00, LOSS: 33.53], G:[ACC: %100.00, LOSS: 35.65]\n",
      "E: 922, D:[ACC: %100.00, LOSS: 30.36], G:[ACC: %99.61, LOSS: 36.44]\n",
      "E: 923, D:[ACC: %100.00, LOSS: 32.40], G:[ACC: %100.00, LOSS: 37.74]\n",
      "E: 924, D:[ACC: %100.00, LOSS: 32.77], G:[ACC: %99.61, LOSS: 37.12]\n",
      "E: 925, D:[ACC: %100.00, LOSS: 30.38], G:[ACC: %100.00, LOSS: 35.22]\n",
      "E: 926, D:[ACC: %100.00, LOSS: 31.57], G:[ACC: %100.00, LOSS: 38.07]\n",
      "E: 927, D:[ACC: %100.00, LOSS: 32.02], G:[ACC: %99.61, LOSS: 40.16]\n",
      "E: 928, D:[ACC: %100.00, LOSS: 30.67], G:[ACC: %98.83, LOSS: 45.20]\n",
      "E: 929, D:[ACC: %100.00, LOSS: 31.71], G:[ACC: %92.97, LOSS: 61.56]\n",
      "E: 930, D:[ACC: %100.00, LOSS: 30.02], G:[ACC: %73.83, LOSS: 88.87]\n",
      "E: 931, D:[ACC: %100.00, LOSS: 34.34], G:[ACC: %0.00, LOSS: 593.28]\n",
      "E: 932, D:[ACC: %100.00, LOSS: 33.37], G:[ACC: %0.00, LOSS: 553.03]\n",
      "E: 933, D:[ACC: %88.48, LOSS: 55.66], G:[ACC: %0.00, LOSS: 1639.96]\n",
      "E: 934, D:[ACC: %100.00, LOSS: 34.83], G:[ACC: %0.00, LOSS: 1648.34]\n",
      "E: 935, D:[ACC: %100.00, LOSS: 32.16], G:[ACC: %0.00, LOSS: 1649.30]\n",
      "E: 936, D:[ACC: %99.80, LOSS: 34.39], G:[ACC: %0.00, LOSS: 1642.19]\n",
      "E: 937, D:[ACC: %99.80, LOSS: 33.11], G:[ACC: %0.00, LOSS: 1645.06]\n",
      "E: 938, D:[ACC: %100.00, LOSS: 35.65], G:[ACC: %0.00, LOSS: 1632.19]\n",
      "E: 939, D:[ACC: %100.00, LOSS: 31.38], G:[ACC: %0.00, LOSS: 1340.11]\n",
      "E: 940, D:[ACC: %100.00, LOSS: 38.11], G:[ACC: %4.69, LOSS: 322.03]\n",
      "E: 941, D:[ACC: %90.43, LOSS: 54.34], G:[ACC: %0.00, LOSS: 739.28]\n",
      "E: 942, D:[ACC: %100.00, LOSS: 37.46], G:[ACC: %0.00, LOSS: 594.64]\n",
      "E: 943, D:[ACC: %100.00, LOSS: 36.45], G:[ACC: %6.25, LOSS: 211.68]\n",
      "E: 944, D:[ACC: %88.28, LOSS: 56.27], G:[ACC: %0.00, LOSS: 607.84]\n",
      "E: 945, D:[ACC: %100.00, LOSS: 34.07], G:[ACC: %0.00, LOSS: 611.55]\n",
      "E: 946, D:[ACC: %99.61, LOSS: 34.49], G:[ACC: %0.00, LOSS: 444.99]\n",
      "E: 947, D:[ACC: %100.00, LOSS: 33.60], G:[ACC: %5.47, LOSS: 227.92]\n",
      "E: 948, D:[ACC: %100.00, LOSS: 34.06], G:[ACC: %83.20, LOSS: 78.18]\n",
      "E: 949, D:[ACC: %99.80, LOSS: 33.12], G:[ACC: %92.58, LOSS: 61.62]\n",
      "E: 950, D:[ACC: %100.00, LOSS: 31.39], G:[ACC: %100.00, LOSS: 43.31]\n",
      "E: 951, D:[ACC: %100.00, LOSS: 29.97], G:[ACC: %100.00, LOSS: 39.37]\n",
      "E: 952, D:[ACC: %100.00, LOSS: 32.57], G:[ACC: %100.00, LOSS: 38.85]\n",
      "E: 953, D:[ACC: %100.00, LOSS: 30.12], G:[ACC: %100.00, LOSS: 40.52]\n",
      "E: 954, D:[ACC: %100.00, LOSS: 30.72], G:[ACC: %100.00, LOSS: 41.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 955, D:[ACC: %99.80, LOSS: 29.78], G:[ACC: %100.00, LOSS: 39.11]\n",
      "E: 956, D:[ACC: %100.00, LOSS: 30.26], G:[ACC: %100.00, LOSS: 38.25]\n",
      "E: 957, D:[ACC: %100.00, LOSS: 30.20], G:[ACC: %100.00, LOSS: 38.57]\n",
      "E: 958, D:[ACC: %100.00, LOSS: 31.62], G:[ACC: %100.00, LOSS: 39.03]\n",
      "E: 959, D:[ACC: %100.00, LOSS: 29.11], G:[ACC: %100.00, LOSS: 39.10]\n",
      "E: 960, D:[ACC: %100.00, LOSS: 28.92], G:[ACC: %100.00, LOSS: 38.47]\n",
      "E: 961, D:[ACC: %100.00, LOSS: 29.48], G:[ACC: %100.00, LOSS: 40.80]\n",
      "E: 962, D:[ACC: %100.00, LOSS: 29.36], G:[ACC: %100.00, LOSS: 39.15]\n",
      "E: 963, D:[ACC: %100.00, LOSS: 30.42], G:[ACC: %100.00, LOSS: 37.99]\n",
      "E: 964, D:[ACC: %100.00, LOSS: 29.35], G:[ACC: %100.00, LOSS: 36.34]\n",
      "E: 965, D:[ACC: %100.00, LOSS: 30.74], G:[ACC: %100.00, LOSS: 36.52]\n",
      "E: 966, D:[ACC: %100.00, LOSS: 29.05], G:[ACC: %100.00, LOSS: 37.97]\n",
      "E: 967, D:[ACC: %100.00, LOSS: 30.80], G:[ACC: %100.00, LOSS: 38.38]\n",
      "E: 968, D:[ACC: %100.00, LOSS: 29.14], G:[ACC: %100.00, LOSS: 36.68]\n",
      "E: 969, D:[ACC: %100.00, LOSS: 29.73], G:[ACC: %100.00, LOSS: 39.06]\n",
      "E: 970, D:[ACC: %100.00, LOSS: 27.46], G:[ACC: %100.00, LOSS: 36.35]\n",
      "E: 971, D:[ACC: %100.00, LOSS: 30.10], G:[ACC: %100.00, LOSS: 37.04]\n",
      "E: 972, D:[ACC: %100.00, LOSS: 27.69], G:[ACC: %100.00, LOSS: 38.38]\n",
      "E: 973, D:[ACC: %100.00, LOSS: 30.06], G:[ACC: %100.00, LOSS: 36.54]\n",
      "E: 974, D:[ACC: %100.00, LOSS: 28.78], G:[ACC: %100.00, LOSS: 36.61]\n",
      "E: 975, D:[ACC: %100.00, LOSS: 29.70], G:[ACC: %100.00, LOSS: 37.17]\n",
      "E: 976, D:[ACC: %100.00, LOSS: 29.77], G:[ACC: %100.00, LOSS: 36.16]\n",
      "E: 977, D:[ACC: %100.00, LOSS: 28.87], G:[ACC: %100.00, LOSS: 35.28]\n",
      "E: 978, D:[ACC: %100.00, LOSS: 28.98], G:[ACC: %100.00, LOSS: 35.59]\n",
      "E: 979, D:[ACC: %100.00, LOSS: 28.39], G:[ACC: %100.00, LOSS: 35.88]\n",
      "E: 980, D:[ACC: %100.00, LOSS: 29.59], G:[ACC: %100.00, LOSS: 35.93]\n",
      "E: 981, D:[ACC: %100.00, LOSS: 29.27], G:[ACC: %100.00, LOSS: 34.28]\n",
      "E: 982, D:[ACC: %100.00, LOSS: 28.31], G:[ACC: %100.00, LOSS: 35.29]\n",
      "E: 983, D:[ACC: %100.00, LOSS: 28.88], G:[ACC: %100.00, LOSS: 36.67]\n",
      "E: 984, D:[ACC: %100.00, LOSS: 28.05], G:[ACC: %100.00, LOSS: 35.23]\n",
      "E: 985, D:[ACC: %100.00, LOSS: 26.62], G:[ACC: %100.00, LOSS: 36.18]\n",
      "E: 986, D:[ACC: %100.00, LOSS: 28.23], G:[ACC: %100.00, LOSS: 35.40]\n",
      "E: 987, D:[ACC: %100.00, LOSS: 27.95], G:[ACC: %100.00, LOSS: 36.77]\n",
      "E: 988, D:[ACC: %100.00, LOSS: 28.11], G:[ACC: %100.00, LOSS: 35.09]\n",
      "E: 989, D:[ACC: %100.00, LOSS: 26.82], G:[ACC: %100.00, LOSS: 37.12]\n",
      "E: 990, D:[ACC: %100.00, LOSS: 29.17], G:[ACC: %100.00, LOSS: 34.61]\n",
      "E: 991, D:[ACC: %100.00, LOSS: 29.08], G:[ACC: %100.00, LOSS: 36.13]\n",
      "E: 992, D:[ACC: %100.00, LOSS: 26.73], G:[ACC: %100.00, LOSS: 34.78]\n",
      "E: 993, D:[ACC: %100.00, LOSS: 25.45], G:[ACC: %100.00, LOSS: 35.69]\n",
      "E: 994, D:[ACC: %100.00, LOSS: 27.58], G:[ACC: %100.00, LOSS: 37.76]\n",
      "E: 995, D:[ACC: %100.00, LOSS: 27.12], G:[ACC: %100.00, LOSS: 35.65]\n",
      "E: 996, D:[ACC: %100.00, LOSS: 26.22], G:[ACC: %100.00, LOSS: 37.88]\n",
      "E: 997, D:[ACC: %100.00, LOSS: 29.06], G:[ACC: %100.00, LOSS: 37.46]\n",
      "E: 998, D:[ACC: %100.00, LOSS: 28.46], G:[ACC: %100.00, LOSS: 36.66]\n",
      "E: 999, D:[ACC: %100.00, LOSS: 28.96], G:[ACC: %100.00, LOSS: 33.89]\n",
      "E: 1000, D:[ACC: %100.00, LOSS: 30.09], G:[ACC: %100.00, LOSS: 33.31]\n",
      "E: 1001, D:[ACC: %100.00, LOSS: 28.32], G:[ACC: %100.00, LOSS: 34.53]\n",
      "E: 1002, D:[ACC: %100.00, LOSS: 27.59], G:[ACC: %100.00, LOSS: 37.21]\n",
      "E: 1003, D:[ACC: %100.00, LOSS: 28.31], G:[ACC: %100.00, LOSS: 36.26]\n",
      "E: 1004, D:[ACC: %100.00, LOSS: 26.53], G:[ACC: %100.00, LOSS: 35.73]\n",
      "E: 1005, D:[ACC: %100.00, LOSS: 27.16], G:[ACC: %100.00, LOSS: 36.39]\n",
      "E: 1006, D:[ACC: %100.00, LOSS: 26.37], G:[ACC: %100.00, LOSS: 35.49]\n",
      "E: 1007, D:[ACC: %100.00, LOSS: 26.76], G:[ACC: %100.00, LOSS: 38.62]\n",
      "E: 1008, D:[ACC: %100.00, LOSS: 28.41], G:[ACC: %100.00, LOSS: 33.98]\n",
      "E: 1009, D:[ACC: %100.00, LOSS: 26.38], G:[ACC: %100.00, LOSS: 33.63]\n",
      "E: 1010, D:[ACC: %100.00, LOSS: 28.62], G:[ACC: %100.00, LOSS: 32.15]\n",
      "E: 1011, D:[ACC: %100.00, LOSS: 25.99], G:[ACC: %100.00, LOSS: 34.26]\n",
      "E: 1012, D:[ACC: %100.00, LOSS: 27.74], G:[ACC: %100.00, LOSS: 32.38]\n",
      "E: 1013, D:[ACC: %100.00, LOSS: 27.01], G:[ACC: %100.00, LOSS: 28.68]\n",
      "E: 1014, D:[ACC: %100.00, LOSS: 27.85], G:[ACC: %100.00, LOSS: 32.45]\n",
      "E: 1015, D:[ACC: %100.00, LOSS: 25.83], G:[ACC: %100.00, LOSS: 32.33]\n",
      "E: 1016, D:[ACC: %100.00, LOSS: 26.93], G:[ACC: %100.00, LOSS: 29.44]\n",
      "E: 1017, D:[ACC: %100.00, LOSS: 26.11], G:[ACC: %100.00, LOSS: 30.48]\n",
      "E: 1018, D:[ACC: %100.00, LOSS: 26.59], G:[ACC: %100.00, LOSS: 31.83]\n",
      "E: 1019, D:[ACC: %100.00, LOSS: 27.68], G:[ACC: %100.00, LOSS: 30.32]\n",
      "E: 1020, D:[ACC: %100.00, LOSS: 25.74], G:[ACC: %100.00, LOSS: 28.68]\n",
      "E: 1021, D:[ACC: %100.00, LOSS: 27.22], G:[ACC: %100.00, LOSS: 29.26]\n",
      "E: 1022, D:[ACC: %100.00, LOSS: 26.17], G:[ACC: %100.00, LOSS: 29.34]\n",
      "E: 1023, D:[ACC: %100.00, LOSS: 26.53], G:[ACC: %100.00, LOSS: 28.54]\n",
      "E: 1024, D:[ACC: %100.00, LOSS: 27.33], G:[ACC: %100.00, LOSS: 29.69]\n",
      "E: 1025, D:[ACC: %100.00, LOSS: 26.56], G:[ACC: %100.00, LOSS: 28.23]\n",
      "E: 1026, D:[ACC: %100.00, LOSS: 27.31], G:[ACC: %100.00, LOSS: 31.57]\n",
      "E: 1027, D:[ACC: %100.00, LOSS: 26.70], G:[ACC: %100.00, LOSS: 31.01]\n",
      "E: 1028, D:[ACC: %100.00, LOSS: 26.66], G:[ACC: %100.00, LOSS: 30.93]\n",
      "E: 1029, D:[ACC: %100.00, LOSS: 27.22], G:[ACC: %100.00, LOSS: 30.07]\n",
      "E: 1030, D:[ACC: %100.00, LOSS: 26.27], G:[ACC: %100.00, LOSS: 29.77]\n",
      "E: 1031, D:[ACC: %100.00, LOSS: 27.43], G:[ACC: %100.00, LOSS: 28.35]\n",
      "E: 1032, D:[ACC: %100.00, LOSS: 26.32], G:[ACC: %100.00, LOSS: 29.76]\n",
      "E: 1033, D:[ACC: %100.00, LOSS: 26.12], G:[ACC: %100.00, LOSS: 29.37]\n",
      "E: 1034, D:[ACC: %100.00, LOSS: 26.08], G:[ACC: %100.00, LOSS: 27.76]\n",
      "E: 1035, D:[ACC: %100.00, LOSS: 24.48], G:[ACC: %100.00, LOSS: 29.78]\n",
      "E: 1036, D:[ACC: %100.00, LOSS: 25.74], G:[ACC: %100.00, LOSS: 27.44]\n",
      "E: 1037, D:[ACC: %100.00, LOSS: 25.92], G:[ACC: %100.00, LOSS: 29.04]\n",
      "E: 1038, D:[ACC: %100.00, LOSS: 25.78], G:[ACC: %100.00, LOSS: 27.68]\n",
      "E: 1039, D:[ACC: %100.00, LOSS: 26.52], G:[ACC: %100.00, LOSS: 28.08]\n",
      "E: 1040, D:[ACC: %100.00, LOSS: 26.05], G:[ACC: %100.00, LOSS: 28.87]\n",
      "E: 1041, D:[ACC: %100.00, LOSS: 25.93], G:[ACC: %100.00, LOSS: 28.11]\n",
      "E: 1042, D:[ACC: %100.00, LOSS: 24.77], G:[ACC: %100.00, LOSS: 28.17]\n",
      "E: 1043, D:[ACC: %100.00, LOSS: 23.98], G:[ACC: %100.00, LOSS: 28.44]\n",
      "E: 1044, D:[ACC: %100.00, LOSS: 26.77], G:[ACC: %100.00, LOSS: 30.07]\n",
      "E: 1045, D:[ACC: %100.00, LOSS: 26.01], G:[ACC: %100.00, LOSS: 28.18]\n",
      "E: 1046, D:[ACC: %100.00, LOSS: 24.97], G:[ACC: %100.00, LOSS: 30.82]\n",
      "E: 1047, D:[ACC: %100.00, LOSS: 24.74], G:[ACC: %100.00, LOSS: 29.78]\n",
      "E: 1048, D:[ACC: %100.00, LOSS: 24.37], G:[ACC: %100.00, LOSS: 29.00]\n",
      "E: 1049, D:[ACC: %100.00, LOSS: 23.76], G:[ACC: %100.00, LOSS: 29.47]\n",
      "E: 1050, D:[ACC: %100.00, LOSS: 26.06], G:[ACC: %100.00, LOSS: 30.36]\n",
      "E: 1051, D:[ACC: %99.80, LOSS: 26.13], G:[ACC: %100.00, LOSS: 28.38]\n",
      "E: 1052, D:[ACC: %100.00, LOSS: 24.57], G:[ACC: %100.00, LOSS: 29.66]\n",
      "E: 1053, D:[ACC: %100.00, LOSS: 24.50], G:[ACC: %100.00, LOSS: 28.46]\n",
      "E: 1054, D:[ACC: %100.00, LOSS: 24.35], G:[ACC: %100.00, LOSS: 27.54]\n",
      "E: 1055, D:[ACC: %100.00, LOSS: 24.94], G:[ACC: %100.00, LOSS: 27.00]\n",
      "E: 1056, D:[ACC: %100.00, LOSS: 24.36], G:[ACC: %100.00, LOSS: 27.43]\n",
      "E: 1057, D:[ACC: %100.00, LOSS: 24.03], G:[ACC: %100.00, LOSS: 27.09]\n",
      "E: 1058, D:[ACC: %100.00, LOSS: 25.05], G:[ACC: %100.00, LOSS: 28.05]\n",
      "E: 1059, D:[ACC: %100.00, LOSS: 26.23], G:[ACC: %100.00, LOSS: 26.67]\n",
      "E: 1060, D:[ACC: %100.00, LOSS: 24.72], G:[ACC: %100.00, LOSS: 27.47]\n",
      "E: 1061, D:[ACC: %100.00, LOSS: 23.95], G:[ACC: %100.00, LOSS: 26.10]\n",
      "E: 1062, D:[ACC: %100.00, LOSS: 25.13], G:[ACC: %100.00, LOSS: 26.28]\n",
      "E: 1063, D:[ACC: %100.00, LOSS: 24.89], G:[ACC: %100.00, LOSS: 25.43]\n",
      "E: 1064, D:[ACC: %100.00, LOSS: 25.26], G:[ACC: %100.00, LOSS: 26.25]\n",
      "E: 1065, D:[ACC: %100.00, LOSS: 23.59], G:[ACC: %100.00, LOSS: 27.36]\n",
      "E: 1066, D:[ACC: %100.00, LOSS: 24.96], G:[ACC: %99.61, LOSS: 27.44]\n",
      "E: 1067, D:[ACC: %100.00, LOSS: 24.85], G:[ACC: %100.00, LOSS: 27.50]\n",
      "E: 1068, D:[ACC: %100.00, LOSS: 23.31], G:[ACC: %100.00, LOSS: 26.58]\n",
      "E: 1069, D:[ACC: %100.00, LOSS: 25.53], G:[ACC: %100.00, LOSS: 25.49]\n",
      "E: 1070, D:[ACC: %100.00, LOSS: 25.93], G:[ACC: %100.00, LOSS: 26.21]\n",
      "E: 1071, D:[ACC: %100.00, LOSS: 24.31], G:[ACC: %100.00, LOSS: 27.05]\n",
      "E: 1072, D:[ACC: %100.00, LOSS: 23.47], G:[ACC: %100.00, LOSS: 26.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 1073, D:[ACC: %100.00, LOSS: 23.16], G:[ACC: %100.00, LOSS: 26.04]\n",
      "E: 1074, D:[ACC: %100.00, LOSS: 24.46], G:[ACC: %100.00, LOSS: 25.76]\n",
      "E: 1075, D:[ACC: %100.00, LOSS: 24.54], G:[ACC: %100.00, LOSS: 25.83]\n",
      "E: 1076, D:[ACC: %100.00, LOSS: 23.56], G:[ACC: %100.00, LOSS: 24.50]\n",
      "E: 1077, D:[ACC: %100.00, LOSS: 24.07], G:[ACC: %100.00, LOSS: 26.21]\n",
      "E: 1078, D:[ACC: %100.00, LOSS: 25.07], G:[ACC: %100.00, LOSS: 26.21]\n",
      "E: 1079, D:[ACC: %100.00, LOSS: 24.69], G:[ACC: %100.00, LOSS: 27.35]\n",
      "E: 1080, D:[ACC: %100.00, LOSS: 23.67], G:[ACC: %100.00, LOSS: 27.89]\n",
      "E: 1081, D:[ACC: %100.00, LOSS: 26.17], G:[ACC: %100.00, LOSS: 26.04]\n",
      "E: 1082, D:[ACC: %100.00, LOSS: 23.83], G:[ACC: %100.00, LOSS: 28.88]\n",
      "E: 1083, D:[ACC: %100.00, LOSS: 23.73], G:[ACC: %100.00, LOSS: 27.82]\n",
      "E: 1084, D:[ACC: %100.00, LOSS: 24.78], G:[ACC: %100.00, LOSS: 27.70]\n",
      "E: 1085, D:[ACC: %100.00, LOSS: 24.91], G:[ACC: %100.00, LOSS: 27.22]\n",
      "E: 1086, D:[ACC: %100.00, LOSS: 22.83], G:[ACC: %100.00, LOSS: 27.24]\n",
      "E: 1087, D:[ACC: %100.00, LOSS: 22.08], G:[ACC: %100.00, LOSS: 26.37]\n",
      "E: 1088, D:[ACC: %100.00, LOSS: 23.11], G:[ACC: %100.00, LOSS: 25.92]\n",
      "E: 1089, D:[ACC: %100.00, LOSS: 24.58], G:[ACC: %100.00, LOSS: 25.31]\n",
      "E: 1090, D:[ACC: %100.00, LOSS: 23.49], G:[ACC: %100.00, LOSS: 26.61]\n",
      "E: 1091, D:[ACC: %100.00, LOSS: 23.77], G:[ACC: %100.00, LOSS: 26.07]\n",
      "E: 1092, D:[ACC: %100.00, LOSS: 23.65], G:[ACC: %100.00, LOSS: 26.98]\n",
      "E: 1093, D:[ACC: %100.00, LOSS: 23.71], G:[ACC: %100.00, LOSS: 26.15]\n",
      "E: 1094, D:[ACC: %100.00, LOSS: 23.20], G:[ACC: %100.00, LOSS: 26.53]\n",
      "E: 1095, D:[ACC: %100.00, LOSS: 22.45], G:[ACC: %100.00, LOSS: 24.89]\n",
      "E: 1096, D:[ACC: %100.00, LOSS: 23.25], G:[ACC: %100.00, LOSS: 28.07]\n",
      "E: 1097, D:[ACC: %100.00, LOSS: 22.28], G:[ACC: %100.00, LOSS: 28.88]\n",
      "E: 1098, D:[ACC: %100.00, LOSS: 23.13], G:[ACC: %100.00, LOSS: 29.69]\n",
      "E: 1099, D:[ACC: %100.00, LOSS: 25.07], G:[ACC: %100.00, LOSS: 29.77]\n",
      "E: 1100, D:[ACC: %100.00, LOSS: 22.28], G:[ACC: %100.00, LOSS: 30.20]\n",
      "E: 1101, D:[ACC: %100.00, LOSS: 22.40], G:[ACC: %59.77, LOSS: 91.25]\n",
      "E: 1102, D:[ACC: %100.00, LOSS: 24.06], G:[ACC: %0.00, LOSS: 871.12]\n",
      "E: 1103, D:[ACC: %100.00, LOSS: 24.58], G:[ACC: %0.39, LOSS: 344.31]\n",
      "E: 1104, D:[ACC: %100.00, LOSS: 31.45], G:[ACC: %0.39, LOSS: 473.60]\n",
      "E: 1105, D:[ACC: %100.00, LOSS: 25.99], G:[ACC: %68.75, LOSS: 97.77]\n",
      "E: 1106, D:[ACC: %100.00, LOSS: 23.64], G:[ACC: %85.94, LOSS: 64.68]\n",
      "E: 1107, D:[ACC: %100.00, LOSS: 26.91], G:[ACC: %97.66, LOSS: 40.55]\n",
      "E: 1108, D:[ACC: %100.00, LOSS: 27.63], G:[ACC: %98.83, LOSS: 36.72]\n",
      "E: 1109, D:[ACC: %100.00, LOSS: 24.68], G:[ACC: %99.22, LOSS: 31.08]\n",
      "E: 1110, D:[ACC: %100.00, LOSS: 24.85], G:[ACC: %100.00, LOSS: 30.37]\n",
      "E: 1111, D:[ACC: %100.00, LOSS: 23.92], G:[ACC: %99.22, LOSS: 29.52]\n",
      "E: 1112, D:[ACC: %100.00, LOSS: 23.06], G:[ACC: %100.00, LOSS: 29.71]\n",
      "E: 1113, D:[ACC: %100.00, LOSS: 22.59], G:[ACC: %99.22, LOSS: 29.58]\n",
      "E: 1114, D:[ACC: %100.00, LOSS: 24.24], G:[ACC: %100.00, LOSS: 28.78]\n",
      "E: 1115, D:[ACC: %100.00, LOSS: 24.01], G:[ACC: %100.00, LOSS: 28.74]\n",
      "E: 1116, D:[ACC: %100.00, LOSS: 23.51], G:[ACC: %100.00, LOSS: 28.18]\n",
      "E: 1117, D:[ACC: %100.00, LOSS: 24.99], G:[ACC: %99.61, LOSS: 29.70]\n",
      "E: 1118, D:[ACC: %100.00, LOSS: 23.29], G:[ACC: %100.00, LOSS: 28.29]\n",
      "E: 1119, D:[ACC: %100.00, LOSS: 22.03], G:[ACC: %100.00, LOSS: 28.47]\n",
      "E: 1120, D:[ACC: %100.00, LOSS: 22.43], G:[ACC: %99.61, LOSS: 28.17]\n",
      "E: 1121, D:[ACC: %100.00, LOSS: 23.67], G:[ACC: %100.00, LOSS: 28.02]\n",
      "E: 1122, D:[ACC: %100.00, LOSS: 22.03], G:[ACC: %100.00, LOSS: 26.74]\n",
      "E: 1123, D:[ACC: %100.00, LOSS: 22.24], G:[ACC: %99.61, LOSS: 25.52]\n",
      "E: 1124, D:[ACC: %100.00, LOSS: 21.69], G:[ACC: %100.00, LOSS: 26.72]\n",
      "E: 1125, D:[ACC: %100.00, LOSS: 21.96], G:[ACC: %100.00, LOSS: 27.28]\n",
      "E: 1126, D:[ACC: %100.00, LOSS: 24.24], G:[ACC: %100.00, LOSS: 25.92]\n",
      "E: 1127, D:[ACC: %100.00, LOSS: 23.15], G:[ACC: %100.00, LOSS: 26.65]\n",
      "E: 1128, D:[ACC: %100.00, LOSS: 21.42], G:[ACC: %100.00, LOSS: 26.87]\n",
      "E: 1129, D:[ACC: %100.00, LOSS: 22.33], G:[ACC: %100.00, LOSS: 25.26]\n",
      "E: 1130, D:[ACC: %100.00, LOSS: 22.10], G:[ACC: %100.00, LOSS: 26.46]\n",
      "E: 1131, D:[ACC: %100.00, LOSS: 21.23], G:[ACC: %100.00, LOSS: 25.38]\n",
      "E: 1132, D:[ACC: %100.00, LOSS: 21.81], G:[ACC: %100.00, LOSS: 25.93]\n",
      "E: 1133, D:[ACC: %100.00, LOSS: 23.00], G:[ACC: %100.00, LOSS: 26.52]\n",
      "E: 1134, D:[ACC: %100.00, LOSS: 23.30], G:[ACC: %100.00, LOSS: 27.14]\n",
      "E: 1135, D:[ACC: %99.80, LOSS: 22.71], G:[ACC: %100.00, LOSS: 26.03]\n",
      "E: 1136, D:[ACC: %100.00, LOSS: 22.54], G:[ACC: %100.00, LOSS: 26.41]\n",
      "E: 1137, D:[ACC: %100.00, LOSS: 22.40], G:[ACC: %100.00, LOSS: 25.81]\n",
      "E: 1138, D:[ACC: %100.00, LOSS: 22.11], G:[ACC: %100.00, LOSS: 26.33]\n",
      "E: 1139, D:[ACC: %100.00, LOSS: 21.91], G:[ACC: %100.00, LOSS: 27.50]\n",
      "E: 1140, D:[ACC: %100.00, LOSS: 22.07], G:[ACC: %100.00, LOSS: 25.49]\n",
      "E: 1141, D:[ACC: %100.00, LOSS: 20.85], G:[ACC: %100.00, LOSS: 26.19]\n",
      "E: 1142, D:[ACC: %100.00, LOSS: 22.56], G:[ACC: %100.00, LOSS: 25.65]\n",
      "E: 1143, D:[ACC: %100.00, LOSS: 22.89], G:[ACC: %100.00, LOSS: 26.04]\n",
      "E: 1144, D:[ACC: %100.00, LOSS: 22.71], G:[ACC: %100.00, LOSS: 26.25]\n",
      "E: 1145, D:[ACC: %100.00, LOSS: 23.46], G:[ACC: %100.00, LOSS: 25.76]\n",
      "E: 1146, D:[ACC: %100.00, LOSS: 20.96], G:[ACC: %100.00, LOSS: 26.64]\n",
      "E: 1147, D:[ACC: %100.00, LOSS: 22.02], G:[ACC: %100.00, LOSS: 25.75]\n",
      "E: 1148, D:[ACC: %100.00, LOSS: 21.37], G:[ACC: %100.00, LOSS: 26.72]\n",
      "E: 1149, D:[ACC: %100.00, LOSS: 22.18], G:[ACC: %100.00, LOSS: 26.31]\n",
      "E: 1150, D:[ACC: %100.00, LOSS: 22.69], G:[ACC: %100.00, LOSS: 24.33]\n",
      "E: 1151, D:[ACC: %100.00, LOSS: 21.47], G:[ACC: %100.00, LOSS: 25.52]\n",
      "E: 1152, D:[ACC: %100.00, LOSS: 20.34], G:[ACC: %100.00, LOSS: 25.25]\n",
      "E: 1153, D:[ACC: %100.00, LOSS: 19.93], G:[ACC: %100.00, LOSS: 26.22]\n",
      "E: 1154, D:[ACC: %100.00, LOSS: 20.42], G:[ACC: %100.00, LOSS: 25.74]\n",
      "E: 1155, D:[ACC: %100.00, LOSS: 21.18], G:[ACC: %100.00, LOSS: 24.48]\n",
      "E: 1156, D:[ACC: %100.00, LOSS: 21.32], G:[ACC: %100.00, LOSS: 24.93]\n",
      "E: 1157, D:[ACC: %100.00, LOSS: 21.31], G:[ACC: %100.00, LOSS: 25.42]\n",
      "E: 1158, D:[ACC: %100.00, LOSS: 21.31], G:[ACC: %100.00, LOSS: 25.14]\n",
      "E: 1159, D:[ACC: %100.00, LOSS: 20.41], G:[ACC: %100.00, LOSS: 24.56]\n",
      "E: 1160, D:[ACC: %100.00, LOSS: 20.36], G:[ACC: %99.61, LOSS: 28.04]\n",
      "E: 1161, D:[ACC: %100.00, LOSS: 19.59], G:[ACC: %100.00, LOSS: 26.04]\n",
      "E: 1162, D:[ACC: %100.00, LOSS: 21.31], G:[ACC: %100.00, LOSS: 25.20]\n",
      "E: 1163, D:[ACC: %100.00, LOSS: 21.46], G:[ACC: %100.00, LOSS: 24.84]\n",
      "E: 1164, D:[ACC: %100.00, LOSS: 20.57], G:[ACC: %100.00, LOSS: 24.80]\n",
      "E: 1165, D:[ACC: %100.00, LOSS: 20.82], G:[ACC: %100.00, LOSS: 26.80]\n",
      "E: 1166, D:[ACC: %100.00, LOSS: 20.49], G:[ACC: %100.00, LOSS: 25.96]\n",
      "E: 1167, D:[ACC: %100.00, LOSS: 21.08], G:[ACC: %100.00, LOSS: 27.19]\n",
      "E: 1168, D:[ACC: %100.00, LOSS: 21.63], G:[ACC: %100.00, LOSS: 27.57]\n",
      "E: 1169, D:[ACC: %100.00, LOSS: 20.11], G:[ACC: %100.00, LOSS: 27.94]\n",
      "E: 1170, D:[ACC: %100.00, LOSS: 20.41], G:[ACC: %100.00, LOSS: 26.31]\n",
      "E: 1171, D:[ACC: %100.00, LOSS: 22.61], G:[ACC: %100.00, LOSS: 27.95]\n",
      "E: 1172, D:[ACC: %100.00, LOSS: 22.01], G:[ACC: %100.00, LOSS: 27.07]\n",
      "E: 1173, D:[ACC: %100.00, LOSS: 20.11], G:[ACC: %99.61, LOSS: 30.72]\n",
      "E: 1174, D:[ACC: %100.00, LOSS: 20.44], G:[ACC: %100.00, LOSS: 30.56]\n",
      "E: 1175, D:[ACC: %100.00, LOSS: 21.34], G:[ACC: %100.00, LOSS: 30.64]\n",
      "E: 1176, D:[ACC: %100.00, LOSS: 20.09], G:[ACC: %100.00, LOSS: 30.32]\n",
      "E: 1177, D:[ACC: %100.00, LOSS: 20.55], G:[ACC: %100.00, LOSS: 29.61]\n",
      "E: 1178, D:[ACC: %100.00, LOSS: 20.11], G:[ACC: %100.00, LOSS: 30.51]\n",
      "E: 1179, D:[ACC: %100.00, LOSS: 20.88], G:[ACC: %100.00, LOSS: 32.79]\n",
      "E: 1180, D:[ACC: %100.00, LOSS: 19.89], G:[ACC: %100.00, LOSS: 34.03]\n",
      "E: 1181, D:[ACC: %100.00, LOSS: 21.49], G:[ACC: %100.00, LOSS: 35.71]\n",
      "E: 1182, D:[ACC: %100.00, LOSS: 19.33], G:[ACC: %100.00, LOSS: 33.54]\n",
      "E: 1183, D:[ACC: %100.00, LOSS: 19.74], G:[ACC: %100.00, LOSS: 33.61]\n",
      "E: 1184, D:[ACC: %100.00, LOSS: 20.14], G:[ACC: %100.00, LOSS: 34.56]\n",
      "E: 1185, D:[ACC: %100.00, LOSS: 19.72], G:[ACC: %100.00, LOSS: 34.62]\n",
      "E: 1186, D:[ACC: %100.00, LOSS: 18.77], G:[ACC: %99.61, LOSS: 33.97]\n",
      "E: 1187, D:[ACC: %100.00, LOSS: 19.46], G:[ACC: %99.61, LOSS: 36.07]\n",
      "E: 1188, D:[ACC: %100.00, LOSS: 19.27], G:[ACC: %99.22, LOSS: 35.82]\n",
      "E: 1189, D:[ACC: %100.00, LOSS: 20.64], G:[ACC: %99.61, LOSS: 35.66]\n",
      "E: 1190, D:[ACC: %100.00, LOSS: 20.09], G:[ACC: %99.22, LOSS: 35.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 1191, D:[ACC: %100.00, LOSS: 19.60], G:[ACC: %99.61, LOSS: 34.00]\n",
      "E: 1192, D:[ACC: %100.00, LOSS: 19.85], G:[ACC: %100.00, LOSS: 31.18]\n",
      "E: 1193, D:[ACC: %100.00, LOSS: 20.04], G:[ACC: %100.00, LOSS: 29.99]\n",
      "E: 1194, D:[ACC: %100.00, LOSS: 21.63], G:[ACC: %100.00, LOSS: 27.33]\n",
      "E: 1195, D:[ACC: %100.00, LOSS: 19.52], G:[ACC: %100.00, LOSS: 28.58]\n",
      "E: 1196, D:[ACC: %100.00, LOSS: 21.79], G:[ACC: %100.00, LOSS: 28.38]\n",
      "E: 1197, D:[ACC: %100.00, LOSS: 21.29], G:[ACC: %100.00, LOSS: 27.78]\n",
      "E: 1198, D:[ACC: %100.00, LOSS: 19.94], G:[ACC: %100.00, LOSS: 29.02]\n",
      "E: 1199, D:[ACC: %100.00, LOSS: 19.23], G:[ACC: %100.00, LOSS: 27.72]\n",
      "E: 1200, D:[ACC: %100.00, LOSS: 19.82], G:[ACC: %99.61, LOSS: 30.90]\n",
      "E: 1201, D:[ACC: %100.00, LOSS: 18.64], G:[ACC: %99.61, LOSS: 31.09]\n",
      "E: 1202, D:[ACC: %100.00, LOSS: 20.78], G:[ACC: %100.00, LOSS: 28.72]\n",
      "E: 1203, D:[ACC: %100.00, LOSS: 20.89], G:[ACC: %100.00, LOSS: 29.64]\n",
      "E: 1204, D:[ACC: %100.00, LOSS: 20.39], G:[ACC: %100.00, LOSS: 31.88]\n",
      "E: 1205, D:[ACC: %100.00, LOSS: 20.06], G:[ACC: %100.00, LOSS: 29.76]\n",
      "E: 1206, D:[ACC: %100.00, LOSS: 20.28], G:[ACC: %100.00, LOSS: 30.65]\n",
      "E: 1207, D:[ACC: %100.00, LOSS: 19.21], G:[ACC: %100.00, LOSS: 29.88]\n",
      "E: 1208, D:[ACC: %100.00, LOSS: 20.40], G:[ACC: %100.00, LOSS: 26.92]\n",
      "E: 1209, D:[ACC: %100.00, LOSS: 19.61], G:[ACC: %100.00, LOSS: 27.17]\n",
      "E: 1210, D:[ACC: %100.00, LOSS: 20.01], G:[ACC: %100.00, LOSS: 25.59]\n",
      "E: 1211, D:[ACC: %100.00, LOSS: 19.24], G:[ACC: %100.00, LOSS: 25.01]\n",
      "E: 1212, D:[ACC: %100.00, LOSS: 18.92], G:[ACC: %100.00, LOSS: 24.90]\n",
      "E: 1213, D:[ACC: %100.00, LOSS: 18.51], G:[ACC: %100.00, LOSS: 25.78]\n",
      "E: 1214, D:[ACC: %100.00, LOSS: 18.72], G:[ACC: %100.00, LOSS: 25.06]\n",
      "E: 1215, D:[ACC: %100.00, LOSS: 19.17], G:[ACC: %100.00, LOSS: 25.59]\n",
      "E: 1216, D:[ACC: %100.00, LOSS: 19.66], G:[ACC: %100.00, LOSS: 27.80]\n",
      "E: 1217, D:[ACC: %100.00, LOSS: 19.13], G:[ACC: %100.00, LOSS: 27.46]\n",
      "E: 1218, D:[ACC: %100.00, LOSS: 20.31], G:[ACC: %99.61, LOSS: 27.62]\n",
      "E: 1219, D:[ACC: %100.00, LOSS: 19.20], G:[ACC: %100.00, LOSS: 26.99]\n",
      "E: 1220, D:[ACC: %100.00, LOSS: 19.23], G:[ACC: %100.00, LOSS: 28.87]\n",
      "E: 1221, D:[ACC: %100.00, LOSS: 19.22], G:[ACC: %99.61, LOSS: 27.42]\n",
      "E: 1222, D:[ACC: %100.00, LOSS: 18.00], G:[ACC: %100.00, LOSS: 28.42]\n",
      "E: 1223, D:[ACC: %100.00, LOSS: 20.45], G:[ACC: %100.00, LOSS: 27.08]\n",
      "E: 1224, D:[ACC: %100.00, LOSS: 19.93], G:[ACC: %100.00, LOSS: 27.10]\n",
      "E: 1225, D:[ACC: %100.00, LOSS: 19.07], G:[ACC: %100.00, LOSS: 24.22]\n",
      "E: 1226, D:[ACC: %100.00, LOSS: 18.22], G:[ACC: %99.61, LOSS: 24.70]\n",
      "E: 1227, D:[ACC: %100.00, LOSS: 19.61], G:[ACC: %99.61, LOSS: 24.45]\n",
      "E: 1228, D:[ACC: %100.00, LOSS: 18.99], G:[ACC: %100.00, LOSS: 25.14]\n",
      "E: 1229, D:[ACC: %100.00, LOSS: 19.68], G:[ACC: %100.00, LOSS: 23.68]\n",
      "E: 1230, D:[ACC: %100.00, LOSS: 18.58], G:[ACC: %100.00, LOSS: 24.94]\n",
      "E: 1231, D:[ACC: %100.00, LOSS: 19.53], G:[ACC: %100.00, LOSS: 24.25]\n",
      "E: 1232, D:[ACC: %100.00, LOSS: 19.12], G:[ACC: %100.00, LOSS: 23.42]\n",
      "E: 1233, D:[ACC: %100.00, LOSS: 19.13], G:[ACC: %100.00, LOSS: 23.45]\n",
      "E: 1234, D:[ACC: %100.00, LOSS: 19.11], G:[ACC: %100.00, LOSS: 23.89]\n",
      "E: 1235, D:[ACC: %100.00, LOSS: 17.15], G:[ACC: %100.00, LOSS: 23.44]\n",
      "E: 1236, D:[ACC: %100.00, LOSS: 18.78], G:[ACC: %100.00, LOSS: 24.39]\n",
      "E: 1237, D:[ACC: %100.00, LOSS: 18.39], G:[ACC: %100.00, LOSS: 25.09]\n",
      "E: 1238, D:[ACC: %100.00, LOSS: 18.42], G:[ACC: %100.00, LOSS: 24.62]\n",
      "E: 1239, D:[ACC: %100.00, LOSS: 19.27], G:[ACC: %100.00, LOSS: 26.77]\n",
      "E: 1240, D:[ACC: %100.00, LOSS: 17.88], G:[ACC: %100.00, LOSS: 27.12]\n",
      "E: 1241, D:[ACC: %100.00, LOSS: 18.09], G:[ACC: %99.61, LOSS: 31.17]\n",
      "E: 1242, D:[ACC: %100.00, LOSS: 19.46], G:[ACC: %100.00, LOSS: 30.90]\n",
      "E: 1243, D:[ACC: %100.00, LOSS: 18.33], G:[ACC: %100.00, LOSS: 30.58]\n",
      "E: 1244, D:[ACC: %100.00, LOSS: 18.69], G:[ACC: %100.00, LOSS: 34.54]\n",
      "E: 1245, D:[ACC: %100.00, LOSS: 18.46], G:[ACC: %99.22, LOSS: 40.42]\n",
      "E: 1246, D:[ACC: %100.00, LOSS: 18.60], G:[ACC: %20.70, LOSS: 137.11]\n",
      "E: 1247, D:[ACC: %100.00, LOSS: 18.35], G:[ACC: %0.00, LOSS: 412.42]\n",
      "E: 1248, D:[ACC: %100.00, LOSS: 19.01], G:[ACC: %20.70, LOSS: 136.03]\n",
      "E: 1249, D:[ACC: %100.00, LOSS: 19.13], G:[ACC: %18.75, LOSS: 132.09]\n",
      "E: 1250, D:[ACC: %100.00, LOSS: 20.15], G:[ACC: %0.00, LOSS: 489.65]\n",
      "E: 1251, D:[ACC: %100.00, LOSS: 20.22], G:[ACC: %0.00, LOSS: 615.31]\n",
      "E: 1252, D:[ACC: %100.00, LOSS: 18.02], G:[ACC: %0.00, LOSS: 392.74]\n",
      "E: 1253, D:[ACC: %100.00, LOSS: 20.37], G:[ACC: %0.00, LOSS: 283.06]\n",
      "E: 1254, D:[ACC: %100.00, LOSS: 20.11], G:[ACC: %0.00, LOSS: 182.61]\n",
      "E: 1255, D:[ACC: %100.00, LOSS: 19.33], G:[ACC: %0.00, LOSS: 135.21]\n",
      "E: 1256, D:[ACC: %100.00, LOSS: 18.79], G:[ACC: %0.00, LOSS: 146.28]\n",
      "E: 1257, D:[ACC: %100.00, LOSS: 18.17], G:[ACC: %0.39, LOSS: 125.98]\n",
      "E: 1258, D:[ACC: %100.00, LOSS: 19.24], G:[ACC: %0.00, LOSS: 144.53]\n",
      "E: 1259, D:[ACC: %100.00, LOSS: 19.19], G:[ACC: %0.00, LOSS: 150.33]\n",
      "E: 1260, D:[ACC: %100.00, LOSS: 18.24], G:[ACC: %0.00, LOSS: 160.85]\n",
      "E: 1261, D:[ACC: %100.00, LOSS: 18.42], G:[ACC: %0.00, LOSS: 168.40]\n",
      "E: 1262, D:[ACC: %100.00, LOSS: 17.73], G:[ACC: %0.00, LOSS: 160.21]\n",
      "E: 1263, D:[ACC: %100.00, LOSS: 19.08], G:[ACC: %0.00, LOSS: 144.78]\n",
      "E: 1264, D:[ACC: %100.00, LOSS: 18.82], G:[ACC: %0.00, LOSS: 140.09]\n",
      "E: 1265, D:[ACC: %100.00, LOSS: 18.37], G:[ACC: %0.00, LOSS: 144.89]\n",
      "E: 1266, D:[ACC: %100.00, LOSS: 19.00], G:[ACC: %0.00, LOSS: 129.60]\n",
      "E: 1267, D:[ACC: %100.00, LOSS: 17.94], G:[ACC: %0.00, LOSS: 128.66]\n",
      "E: 1268, D:[ACC: %100.00, LOSS: 18.48], G:[ACC: %0.00, LOSS: 170.72]\n",
      "E: 1269, D:[ACC: %100.00, LOSS: 18.18], G:[ACC: %0.00, LOSS: 218.60]\n",
      "E: 1270, D:[ACC: %100.00, LOSS: 19.46], G:[ACC: %0.00, LOSS: 325.55]\n",
      "E: 1271, D:[ACC: %98.24, LOSS: 30.37], G:[ACC: %0.00, LOSS: 1171.05]\n",
      "E: 1272, D:[ACC: %100.00, LOSS: 27.67], G:[ACC: %0.00, LOSS: 1390.23]\n",
      "E: 1273, D:[ACC: %100.00, LOSS: 24.29], G:[ACC: %0.00, LOSS: 750.16]\n",
      "E: 1274, D:[ACC: %100.00, LOSS: 20.42], G:[ACC: %9.77, LOSS: 164.38]\n",
      "E: 1275, D:[ACC: %89.45, LOSS: 42.44], G:[ACC: %0.00, LOSS: 670.54]\n",
      "E: 1276, D:[ACC: %100.00, LOSS: 41.12], G:[ACC: %0.00, LOSS: 673.37]\n",
      "E: 1277, D:[ACC: %100.00, LOSS: 25.97], G:[ACC: %0.00, LOSS: 817.04]\n",
      "E: 1278, D:[ACC: %100.00, LOSS: 22.43], G:[ACC: %98.83, LOSS: 37.26]\n",
      "E: 1279, D:[ACC: %94.92, LOSS: 36.33], G:[ACC: %0.00, LOSS: 498.91]\n",
      "E: 1280, D:[ACC: %100.00, LOSS: 21.56], G:[ACC: %0.00, LOSS: 762.28]\n",
      "E: 1281, D:[ACC: %100.00, LOSS: 21.99], G:[ACC: %0.00, LOSS: 440.60]\n",
      "E: 1282, D:[ACC: %100.00, LOSS: 24.48], G:[ACC: %54.30, LOSS: 134.09]\n",
      "E: 1283, D:[ACC: %100.00, LOSS: 20.20], G:[ACC: %2.34, LOSS: 278.80]\n",
      "E: 1284, D:[ACC: %100.00, LOSS: 22.82], G:[ACC: %15.62, LOSS: 150.80]\n",
      "E: 1285, D:[ACC: %100.00, LOSS: 20.02], G:[ACC: %1.56, LOSS: 224.88]\n",
      "E: 1286, D:[ACC: %100.00, LOSS: 18.42], G:[ACC: %0.39, LOSS: 239.75]\n",
      "E: 1287, D:[ACC: %100.00, LOSS: 19.01], G:[ACC: %0.00, LOSS: 181.46]\n",
      "E: 1288, D:[ACC: %100.00, LOSS: 18.53], G:[ACC: %1.17, LOSS: 162.70]\n",
      "E: 1289, D:[ACC: %100.00, LOSS: 18.94], G:[ACC: %7.81, LOSS: 131.74]\n",
      "E: 1290, D:[ACC: %100.00, LOSS: 18.93], G:[ACC: %37.50, LOSS: 96.12]\n",
      "E: 1291, D:[ACC: %100.00, LOSS: 18.64], G:[ACC: %65.62, LOSS: 83.28]\n",
      "E: 1292, D:[ACC: %99.61, LOSS: 24.22], G:[ACC: %87.89, LOSS: 55.10]\n",
      "E: 1293, D:[ACC: %100.00, LOSS: 18.91], G:[ACC: %90.23, LOSS: 58.46]\n",
      "E: 1294, D:[ACC: %100.00, LOSS: 19.40], G:[ACC: %89.84, LOSS: 63.47]\n",
      "E: 1295, D:[ACC: %100.00, LOSS: 19.27], G:[ACC: %87.89, LOSS: 67.85]\n",
      "E: 1296, D:[ACC: %99.80, LOSS: 19.43], G:[ACC: %92.97, LOSS: 65.46]\n",
      "E: 1297, D:[ACC: %100.00, LOSS: 17.34], G:[ACC: %93.75, LOSS: 64.14]\n",
      "E: 1298, D:[ACC: %100.00, LOSS: 17.99], G:[ACC: %94.14, LOSS: 65.07]\n",
      "E: 1299, D:[ACC: %100.00, LOSS: 18.03], G:[ACC: %92.97, LOSS: 67.14]\n",
      "E: 1300, D:[ACC: %100.00, LOSS: 18.04], G:[ACC: %93.36, LOSS: 60.23]\n",
      "E: 1301, D:[ACC: %100.00, LOSS: 16.63], G:[ACC: %96.09, LOSS: 44.35]\n",
      "E: 1302, D:[ACC: %100.00, LOSS: 16.81], G:[ACC: %91.02, LOSS: 46.94]\n",
      "E: 1303, D:[ACC: %100.00, LOSS: 19.41], G:[ACC: %95.70, LOSS: 38.58]\n",
      "E: 1304, D:[ACC: %100.00, LOSS: 17.57], G:[ACC: %98.83, LOSS: 37.73]\n",
      "E: 1305, D:[ACC: %100.00, LOSS: 18.48], G:[ACC: %98.44, LOSS: 32.93]\n",
      "E: 1306, D:[ACC: %100.00, LOSS: 17.72], G:[ACC: %98.83, LOSS: 32.07]\n",
      "E: 1307, D:[ACC: %100.00, LOSS: 17.70], G:[ACC: %98.83, LOSS: 32.13]\n",
      "E: 1308, D:[ACC: %100.00, LOSS: 18.03], G:[ACC: %99.61, LOSS: 30.81]\n",
      "E: 1309, D:[ACC: %100.00, LOSS: 17.68], G:[ACC: %99.22, LOSS: 31.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 1310, D:[ACC: %100.00, LOSS: 17.80], G:[ACC: %99.61, LOSS: 31.29]\n",
      "E: 1311, D:[ACC: %100.00, LOSS: 16.85], G:[ACC: %99.22, LOSS: 32.62]\n",
      "E: 1312, D:[ACC: %100.00, LOSS: 17.34], G:[ACC: %98.44, LOSS: 31.17]\n",
      "E: 1313, D:[ACC: %99.80, LOSS: 18.85], G:[ACC: %98.83, LOSS: 32.41]\n",
      "E: 1314, D:[ACC: %100.00, LOSS: 17.41], G:[ACC: %99.22, LOSS: 31.36]\n",
      "E: 1315, D:[ACC: %100.00, LOSS: 16.84], G:[ACC: %99.22, LOSS: 30.32]\n",
      "E: 1316, D:[ACC: %100.00, LOSS: 17.57], G:[ACC: %100.00, LOSS: 30.18]\n",
      "E: 1317, D:[ACC: %100.00, LOSS: 17.43], G:[ACC: %99.22, LOSS: 31.71]\n",
      "E: 1318, D:[ACC: %100.00, LOSS: 16.34], G:[ACC: %99.61, LOSS: 30.64]\n",
      "E: 1319, D:[ACC: %100.00, LOSS: 17.28], G:[ACC: %99.61, LOSS: 31.80]\n",
      "E: 1320, D:[ACC: %100.00, LOSS: 16.79], G:[ACC: %99.22, LOSS: 31.74]\n",
      "E: 1321, D:[ACC: %100.00, LOSS: 16.47], G:[ACC: %98.83, LOSS: 33.06]\n",
      "E: 1322, D:[ACC: %100.00, LOSS: 16.16], G:[ACC: %98.44, LOSS: 32.43]\n",
      "E: 1323, D:[ACC: %100.00, LOSS: 16.08], G:[ACC: %98.83, LOSS: 31.37]\n",
      "E: 1324, D:[ACC: %100.00, LOSS: 17.07], G:[ACC: %99.22, LOSS: 31.24]\n",
      "E: 1325, D:[ACC: %100.00, LOSS: 16.95], G:[ACC: %99.61, LOSS: 31.18]\n",
      "E: 1326, D:[ACC: %100.00, LOSS: 15.74], G:[ACC: %99.61, LOSS: 30.14]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8000\n",
    "STEPS = 1  # 60000 // BATCH_SIZE\n",
    "\n",
    "\n",
    "train_loss_g = []\n",
    "train_loss_d = []\n",
    "\n",
    "train_acc_g = []\n",
    "train_acc_d = []\n",
    "\n",
    "\n",
    "disc_itr = load_batch()\n",
    "gen_itr = load_noise()\n",
    "\n",
    "\n",
    "# fixed test sample\n",
    "noise = np.random.normal(size=(age_len, noise_len,) )\n",
    "conditions = np.identity(age_len)\n",
    "\n",
    "\n",
    "# epochs\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "        \n",
    "        c2, x, z_fake, y_fake, z_real, y_real = next(disc_itr)\n",
    "    \n",
    "        # train\n",
    "        ## ['loss', 'model_1_loss', 'model_1_loss', 'model_1_acc', 'model_1_acc_1']\n",
    "        loss_1, _, _, acc_1, _ = discriminator.train_on_batch([c2, z_real], [y_real, c2])\n",
    "        loss_2, _, _, acc_2, _ = discriminator.train_on_batch([c2, z_fake], [y_fake, c2])\n",
    "\n",
    "        batch_loss = 0.5 * (loss_1 + loss_2)\n",
    "        batch_acc = 0.5 * (acc_1 + acc_2)\n",
    "\n",
    "        loss.append(batch_loss)\n",
    "        acc.append(batch_acc)\n",
    "\n",
    "    train_loss_d.append(np.mean(np.array(loss)))\n",
    "    train_acc_d.append(np.mean(np.array(acc)))\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "\n",
    "      c2, x, y_true = next(gen_itr)\n",
    "\n",
    "      # train\n",
    "      ## ['loss', 'model_1_loss', 'model_1_loss', 'model_1_acc', 'model_1_acc_1']\n",
    "      loss_1, _, _, acc_1, _ = gan.train_on_batch([c2, x], [y_true, c2])\n",
    "\n",
    "      loss.append(loss_1)\n",
    "      acc.append(acc_1)\n",
    "\n",
    "    train_loss_g.append(np.mean(np.array(loss)))\n",
    "    train_acc_g.append(np.mean(np.array(acc)))\n",
    "\n",
    "\n",
    "    print(\"E: {}, D:[ACC: %{:.2f}, LOSS: {:.2f}], G:[ACC: %{:.2f}, LOSS: {:.2f}]\".format(\n",
    "          e,\n",
    "          train_acc_d[-1] * 100,\n",
    "          train_loss_d[-1] * 100,\n",
    "          train_acc_g[-1] * 100,\n",
    "          train_loss_g[-1] * 100\n",
    "      ))\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        ## visualize results\n",
    "        synthesized = generator.predict([conditions, noise])\n",
    "        visualizeGAN(e, None, synthesized, conditions)\n",
    "        \n",
    "        ## save model\n",
    "        pth = os.path.join(models_path, 'gan.h5')\n",
    "        gan.save(pth)\n",
    "\n",
    "        pth = os.path.join(models_path, 'generator-{}-{}-{}.h5'.format(e, train_loss_g[-1], train_acc_g[-1]))\n",
    "        generator.save(pth)\n",
    "\n",
    "        pth = os.path.join(models_path, 'discriminator.h5')\n",
    "        discriminator.save(pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnlcqEI8i9uH"
   },
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LzhKde-CaDu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "plt.plot(train_loss_g, label=\"Generator Loss\");\n",
    "plt.plot(train_loss_d, label=\"Discriminator Loss\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9iCkvcai9uS"
   },
   "source": [
    "## Plot Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YhSUa3fROSg"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "plt.plot(train_acc_g, label=\"Generator Accuracy\");\n",
    "plt.plot(train_acc_d, label=\"Discriminator Accuracy\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1koOVNb_i9vD"
   },
   "outputs": [],
   "source": [
    "generator.save('./gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save('./disc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.save('./gan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize results\n",
    "synthesized = generator.predict([conditions, noise])\n",
    "visualizeGAN(8000, None, synthesized, conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST Test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
