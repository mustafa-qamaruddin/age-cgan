{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "root_path = './'\n",
    "\n",
    "models_path = os.path.join(root_path, 'saved_models_new')\n",
    "if not os.path.exists(models_path):\n",
    "    os.mkdir(models_path)\n",
    "\n",
    "\n",
    "tgt_pth = os.path.join(root_path, 'visualize_dcgan-v20')\n",
    "\n",
    "if not os.path.exists(tgt_pth):\n",
    "    os.mkdir(tgt_pth)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "img_rows, img_cols, img_depth = (32, 32, 3)\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = x_train / 127.5 - 1\n",
    "\n",
    "def rescaleFn(img):\n",
    "    return img / 127.5 - 1\n",
    "\n",
    "\n",
    "datagen=ImageDataGenerator()\n",
    "\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "train_generator = datagen.flow(x_train, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (32, 32, 3)\n",
    "img_len = np.prod(img_shape)\n",
    "latent_dim = img_len\n",
    "noise_len = 100\n",
    "input_dim = noise_len\n",
    "ngf = 64  # generator\n",
    "ndf = 16  # discriminator\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    return keras.Sequential([\n",
    "        # conv block 1\n",
    "        keras.layers.Conv2D(\n",
    "            filters=ndf,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=2,\n",
    "            input_shape=img_shape,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        \n",
    "        # conv block 2\n",
    "        keras.layers.Conv2D(\n",
    "            filters=ndf * 2,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=2,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        \n",
    "        # conv block 3\n",
    "        keras.layers.Conv2D(\n",
    "            filters=ndf * 4,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=2,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.leaky_relu),\n",
    "        \n",
    "        # conv block 4\n",
    "        keras.layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=2,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        \n",
    "        #output\n",
    "        keras.layers.Activation(tf.nn.sigmoid),\n",
    "        \n",
    "        #flatten\n",
    "        keras.layers.Flatten()\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    \n",
    "    return keras.Sequential([\n",
    "        # reshape 1d to 3d\n",
    "        keras.layers.Reshape((10, 10, 1), input_shape=(input_dim,)),\n",
    "        \n",
    "        # transpose conv block 1\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 8,\n",
    "            kernel_size=(5, 5),\n",
    "            strides=2,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 2\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 4,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 3\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 8,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 4\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 16,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 5\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=ngf * 32,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(tf.nn.relu),\n",
    "        \n",
    "        # transpose conv block 6\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=3,\n",
    "            kernel_size=(2, 2),\n",
    "            strides=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=keras.initializers.he_normal()\n",
    "        ),\n",
    "        \n",
    "        # output\n",
    "        keras.layers.Activation(tf.nn.tanh)\n",
    "    ])\n",
    "\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "generator = build_generator()\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLR = 0.0002  # generator\n",
    "DLR = 0.0002  # discriminator\n",
    "LR = 0.0002\n",
    "\n",
    "\n",
    "optim = keras.optimizers.Adam(LR, beta_1=0.5)\n",
    "\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=optim,\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# noise\n",
    "z = keras.layers.Input(shape=(noise_len,))\n",
    "\n",
    "# freeze discriminator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# output\n",
    "x = generator(z)\n",
    "out = discriminator(x)\n",
    "\n",
    "# GAN\n",
    "gan = keras.models.Model(inputs=z, outputs=out)\n",
    "\n",
    "gan.compile(\n",
    "    optimizer=optim,\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeGAN(e, z_real, z_fake):\n",
    "\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(20, 18))\n",
    "\n",
    "    r_real = 0\n",
    "    r_fake = 0\n",
    "    for row, axe in enumerate(axes):\n",
    "        for col, cell in enumerate(axe):\n",
    "            if row % 2 == 0:\n",
    "                cell.imshow(\n",
    "                    0.5 * z_real[r_real * 5 + col] + 0.5\n",
    "                )\n",
    "            else:\n",
    "                cell.imshow(\n",
    "                    0.5 * z_fake[r_fake * 5 + col] + 0.5\n",
    "                )\n",
    "\n",
    "            cell.axis(\"off\")\n",
    "\n",
    "        if row % 2 == 0:\n",
    "            r_real += 1\n",
    "        else:\n",
    "            r_fake += 1\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(os.path.join(tgt_pth, '{}.jpg'.format(str(e).zfill(3))))\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10251\n",
    "STEPS = 1\n",
    "\n",
    "\n",
    "train_loss_g = []\n",
    "train_loss_d = []\n",
    "\n",
    "train_acc_g = []\n",
    "train_acc_d = []\n",
    "\n",
    "\n",
    "# to be visualized\n",
    "vis_noise = np.random.normal(size=(10, noise_len,))\n",
    "\n",
    "# fake and real label ( Label Smoothing & Label Flipping )\n",
    "y_fake = np.zeros(shape=(BATCH_SIZE,))\n",
    "\n",
    "\n",
    "# epochs\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "        \n",
    "        z = np.random.normal(loc=0.0, scale=0.2, size=(BATCH_SIZE, noise_len))\n",
    "        x_fake = generator.predict(z)\n",
    "        x_real = next(train_generator)\n",
    "        \n",
    "        if x_real.shape[0] < BATCH_SIZE:\n",
    "            x_real = next(train_generator)\n",
    "\n",
    "        # add noise\n",
    "        x_real = x_real + np.random.normal(loc=0, scale=0.01, size=x_real.shape)\n",
    "        x_fake = x_fake + np.random.normal(loc=0, scale=0.01, size=x_fake.shape)\n",
    "        y_real = np.random.normal(loc=0.95, scale=0.05, size=x_fake.shape)\n",
    "        \n",
    "        # train\n",
    "        loss_2, acc_2 = discriminator.train_on_batch(x_real, y_real)\n",
    "        loss_1, acc_1 = discriminator.train_on_batch(x_fake, y_fake)\n",
    "\n",
    "        batch_loss = 0.5 * (loss_1 + loss_2)\n",
    "        batch_acc = 0.5 * (acc_1 + acc_2)\n",
    "\n",
    "        loss.append(batch_loss)\n",
    "        acc.append(batch_acc)\n",
    "\n",
    "    train_loss_d.append(np.mean(np.array(loss)))\n",
    "    train_acc_d.append(np.mean(np.array(acc)))\n",
    "\n",
    "    #batches\n",
    "    loss = []\n",
    "    acc = []\n",
    "\n",
    "    for p in range(STEPS):\n",
    "\n",
    "        z = np.random.normal(loc=0.0, scale=0.2, size=(BATCH_SIZE, noise_len))\n",
    "        y_real = np.random.normal(loc=0.95, scale=0.05, size=x_fake.shape)\n",
    "        \n",
    "        # train\n",
    "        loss_1, acc_1 = gan.train_on_batch(z, y_real)\n",
    "        \n",
    "        loss.append(loss_1)\n",
    "        acc.append(acc_1)\n",
    "\n",
    "    train_loss_g.append(np.mean(np.array(loss)))\n",
    "    train_acc_g.append(np.mean(np.array(acc)))\n",
    "\n",
    "\n",
    "    print(\"E: {}, [D ACC: %{:.2f}], [D LOSS: {:.2f}], [G ACC: %{:.2f}], [G LOSS: {:.2f}]\".format(\n",
    "          e,\n",
    "          train_acc_d[-1] * 100,\n",
    "          train_loss_d[-1] * 100,\n",
    "          train_acc_g[-1] * 100,\n",
    "          train_loss_g[-1] * 100\n",
    "      ))\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        ## visualize results\n",
    "        viz_fake = generator.predict(vis_noise)\n",
    "        x_real = next(train_generator)\n",
    "        visualizeGAN(e, x_real, viz_fake)\n",
    "        \n",
    "        ## save model\n",
    "        pth = os.path.join(models_path, 'gan.h5')\n",
    "        gan.save(pth)\n",
    "\n",
    "        pth = os.path.join(models_path, 'generator-{}-{}-{}.h5'.format(e, train_loss_g[-1], train_acc_g[-1]))\n",
    "        generator.save(pth)\n",
    "\n",
    "        pth = os.path.join(models_path, 'discriminator.h5')\n",
    "        discriminator.save(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "plt.plot(train_loss_g, label=\"Generator Loss\");\n",
    "plt.plot(train_loss_d, label=\"Discriminator Loss\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "plt.plot(train_acc_g, label=\"Generator Accuracy\");\n",
    "plt.plot(train_acc_d, label=\"Discriminator Accuracy\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
